"""
–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ContextAwareModel v2.0

–ö–ª—é—á–µ–≤—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
- LightGBM –≤–º–µ—Å—Ç–æ sklearn GradientBoosting (–±—ã—Å—Ç—Ä–µ–µ, —Ç–æ—á–Ω–µ–µ)
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —á–µ—Ä–µ–∑ Optuna
- –£–º–Ω—ã–π –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤—ã–±—Ä–æ—Å–æ–≤ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ–º NaN
- –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
- Cross-validation –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
- Feature importance –∞–Ω–∞–ª–∏–∑
- –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (RMSE –≤–º–µ—Å—Ç–æ MSE)
"""

import os
import warnings
from typing import Dict, List, Optional, Tuple, Any

import joblib
import numpy as np
import pandas as pd

from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import (
    roc_auc_score,
    average_precision_score,
    mean_squared_error,
    mean_absolute_error,
    precision_recall_curve,
    f1_score,
)
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å LightGBM, –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî fallback –Ω–∞ sklearn
try:
    import lightgbm as lgb

    HAS_LIGHTGBM = True
except (ImportError, OSError):
    HAS_LIGHTGBM = False
    from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Optuna –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
try:
    import optuna
    from optuna.samplers import TPESampler

    HAS_OPTUNA = True
    optuna.logging.set_verbosity(optuna.logging.WARNING)
except ImportError:
    HAS_OPTUNA = False

warnings.filterwarnings("ignore", category=UserWarning)


class ContextAwareModel:
    """
    –ú–æ–¥–µ–ª—å –¥–ª—è —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –∏–∑ snapshot_builder1:
    - will_purchase_next_7d (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
    - days_to_next_purchase (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)
    - next_purchase_amount (—Ä–µ–≥—Ä–µ—Å—Å–∏—è)

    –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å:
    - LightGBM (–∏–ª–∏ fallback –Ω–∞ sklearn)
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–¥–±–æ—Ä–æ–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    - –ö–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
    - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–æ—Ä–æ–≥–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    """

    def __init__(
            self,
            random_state: int = 42,
            use_optuna: bool = True,
            optuna_trials: int = 50,
            calibrate_proba: bool = True,
            verbose: bool = True,
    ):
        self.random_state = random_state
        self.use_optuna = use_optuna and HAS_OPTUNA
        self.optuna_trials = optuna_trials
        self.calibrate_proba = calibrate_proba
        self.verbose = verbose

        # –ú–æ–¥–µ–ª–∏
        self.clf = None
        self.clf_calibrated = None
        self.reg_days = None
        self.reg_amount = None

        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        self.feature_columns_: Optional[pd.Index] = None
        self.label_encoders_: Dict[str, LabelEncoder] = {}
        self.numeric_medians_: Dict[str, float] = {}
        self.numeric_stds_: Dict[str, float] = {}

        # –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        self.optimal_threshold_: float = 0.5

        # Feature importance
        self.feature_importance_: Optional[pd.DataFrame] = None

        # –õ—É—á—à–∏–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.best_params_clf_: Optional[Dict] = None
        self.best_params_reg_days_: Optional[Dict] = None
        self.best_params_reg_amount_: Optional[Dict] = None

    def _log(self, msg: str):
        if self.verbose:
            print(msg)

    # ========= –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ =========

    def _split_features_targets(
            self, df: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.Series, pd.Series, pd.Series]:
        """–†–∞–∑–¥–µ–ª—è–µ—Ç DataFrame –Ω–∞ —Ñ–∏—á–∏ –∏ —Ç–∞—Ä–≥–µ—Ç—ã."""
        target_cols = [
            "will_purchase_next_7d",
            "days_to_next_purchase",
            "next_purchase_amount",
        ]

        missing = [c for c in target_cols if c not in df.columns]
        if missing:
            raise ValueError(f"Missing target columns in data: {missing}")

        y_clf = df["will_purchase_next_7d"].astype(int)
        y_days = df["days_to_next_purchase"].astype(float)
        y_amount = df["next_purchase_amount"].astype(float)

        # –ò—Å–∫–ª—é—á–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏–∑ —Ñ–∏—á–µ–π
        drop_cols = set(target_cols) | {
            "snapshot_date",
            "user_id",
            "last_ts",
            "index",
        }

        X = df.drop(columns=[c for c in drop_cols if c in df.columns])

        return X, y_clf, y_days, y_amount

    def _detect_outliers_iqr(self, series: pd.Series, factor: float = 3.0) -> pd.Series:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º IQR."""
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        lower = Q1 - factor * IQR
        upper = Q3 + factor * IQR
        return series.clip(lower, upper)

    def _prepare_features_fit(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ñ–∏—á–µ–π –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏:
        - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
        - –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö, –º–æ–¥–æ–π –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
        - Label encoding –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (LightGBM —É–º–µ–µ—Ç —Å –Ω–∏–º–∏ —Ä–∞–±–æ—Ç–∞—Ç—å)
        """
        X = X.copy()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫
        numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in numeric_cols:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ–¥–∏–∞–Ω—É –¥–ª—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è NaN
            median_val = X[col].median()
            self.numeric_medians_[col] = median_val if pd.notna(median_val) else 0.0

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º std –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            std_val = X[col].std()
            self.numeric_stds_[col] = std_val if pd.notna(std_val) and std_val > 0 else 1.0

            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –º–µ–¥–∏–∞–Ω–æ–π
            X[col] = X[col].fillna(self.numeric_medians_[col])

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—ã–±—Ä–æ—Å—ã
            X[col] = self._detect_outliers_iqr(X[col])

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in categorical_cols:
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
            X[col] = X[col].fillna("__MISSING__")

            # Label encoding
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
            self.label_encoders_[col] = le

        self.feature_columns_ = X.columns

        return X

    def _prepare_features_infer(self, X: pd.DataFrame) -> pd.DataFrame:
        """–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ñ–∏—á–µ–π –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏."""
        if self.feature_columns_ is None:
            raise ValueError("Model is not fitted: feature_columns_ is None")

        X = X.copy()

        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        drop_cols = {"snapshot_date", "user_id", "last_ts", "index"}
        X = X.drop(columns=[c for c in drop_cols if c in X.columns])

        # –ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_cols = [c for c in X.columns if c in self.numeric_medians_]
        for col in numeric_cols:
            X[col] = X[col].fillna(self.numeric_medians_.get(col, 0.0))
            X[col] = self._detect_outliers_iqr(X[col])

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        for col, le in self.label_encoders_.items():
            if col in X.columns:
                X[col] = X[col].fillna("__MISSING__")
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                X[col] = X[col].astype(str).apply(
                    lambda x: le.transform([x])[0] if x in le.classes_ else -1
                )

        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –∫–æ–ª–æ–Ω–æ–∫
        for col in self.feature_columns_:
            if col not in X.columns:
                X[col] = 0

        X = X[self.feature_columns_]

        return X

    # ========= –ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í =========

    def _get_lgb_params(self, trial: "optuna.Trial", task: str = "classification") -> Dict:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã LightGBM –¥–ª—è Optuna trial."""
        params = {
            "n_estimators": trial.suggest_int("n_estimators", 100, 500),
            "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.2, log=True),
            "max_depth": trial.suggest_int("max_depth", 3, 10),
            "num_leaves": trial.suggest_int("num_leaves", 20, 150),
            "min_child_samples": trial.suggest_int("min_child_samples", 10, 100),
            "subsample": trial.suggest_float("subsample", 0.6, 1.0),
            "colsample_bytree": trial.suggest_float("colsample_bytree", 0.6, 1.0),
            "reg_alpha": trial.suggest_float("reg_alpha", 1e-8, 10.0, log=True),
            "reg_lambda": trial.suggest_float("reg_lambda", 1e-8, 10.0, log=True),
            "random_state": self.random_state,
            "verbosity": -1,
            "n_jobs": -1,
        }

        if task == "classification":
            params["objective"] = "binary"
            params["metric"] = "auc"
        else:
            params["objective"] = "regression"
            params["metric"] = "rmse"

        return params

    def _optimize_classifier(self, X: np.ndarray, y: np.ndarray) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ —á–µ—Ä–µ–∑ Optuna."""

        def objective(trial):
            params = self._get_lgb_params(trial, "classification")

            cv_scores = []
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=self.random_state)

            for train_idx, val_idx in skf.split(X, y):
                X_train_cv, X_val_cv = X[train_idx], X[val_idx]
                y_train_cv, y_val_cv = y[train_idx], y[val_idx]

                model = lgb.LGBMClassifier(**params)
                model.fit(
                    X_train_cv, y_train_cv,
                    eval_set=[(X_val_cv, y_val_cv)],
                )

                proba = model.predict_proba(X_val_cv)[:, 1]
                score = roc_auc_score(y_val_cv, proba)
                cv_scores.append(score)

            return np.mean(cv_scores)

        study = optuna.create_study(
            direction="maximize",
            sampler=TPESampler(seed=self.random_state)
        )
        study.optimize(objective, n_trials=self.optuna_trials, show_progress_bar=self.verbose)

        return self._get_lgb_params(study.best_trial, "classification")

    def _optimize_regressor(self, X: np.ndarray, y: np.ndarray, metric: str = "rmse") -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞ —á–µ—Ä–µ–∑ Optuna."""

        def objective(trial):
            params = self._get_lgb_params(trial, "regression")

            cv_scores = []
            from sklearn.model_selection import KFold
            kf = KFold(n_splits=3, shuffle=True, random_state=self.random_state)

            for train_idx, val_idx in kf.split(X):
                X_train_cv, X_val_cv = X[train_idx], X[val_idx]
                y_train_cv, y_val_cv = y[train_idx], y[val_idx]

                model = lgb.LGBMRegressor(**params)
                model.fit(
                    X_train_cv, y_train_cv,
                    eval_set=[(X_val_cv, y_val_cv)],
                )

                preds = model.predict(X_val_cv)
                score = np.sqrt(mean_squared_error(y_val_cv, preds))
                cv_scores.append(score)

            return np.mean(cv_scores)

        study = optuna.create_study(
            direction="minimize",
            sampler=TPESampler(seed=self.random_state)
        )
        study.optimize(objective, n_trials=self.optuna_trials, show_progress_bar=self.verbose)

        return self._get_lgb_params(study.best_trial, "regression")

    def _find_optimal_threshold(self, y_true: np.ndarray, proba: np.ndarray) -> float:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø–æ F1-score."""
        precision, recall, thresholds = precision_recall_curve(y_true, proba)

        # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        f1_scores = np.where(
            (precision + recall) > 0,
            2 * (precision * recall) / (precision + recall),
            0
        )

        # thresholds –Ω–∞ 1 —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ä–æ—á–µ, —á–µ–º precision/recall
        if len(thresholds) > 0:
            best_idx = np.argmax(f1_scores[:-1])
            return float(thresholds[best_idx])
        return 0.5

    # ========= –û–ë–£–ß–ï–ù–ò–ï =========

    def fit(
            self,
            train_df: pd.DataFrame,
            val_df: Optional[pd.DataFrame] = None,
    ) -> Dict[str, float]:
        """
        –û–±—É—á–∞–µ—Ç —Ç—Ä–∏ –º–æ–¥–µ–ª–∏:
        - –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä will_purchase_next_7d
        - —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä days_to_next_purchase
        - —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä next_purchase_amount

        –ï—Å–ª–∏ val_df –∑–∞–¥–∞–Ω, —Å—Ä–∞–∑—É —Å—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏.
        """
        self._log("üîß Preparing features...")

        # 1. –†–∞–∑–¥–µ–ª—è–µ–º —Ñ–∏—á–∏ –∏ —Ç–∞—Ä–≥–µ—Ç—ã
        X_train_raw, y_clf_train, y_days_train, y_amount_train = self._split_features_targets(train_df)

        # 2. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ —Ñ–∏—á–µ–π
        X_train = self._prepare_features_fit(X_train_raw)
        X_train_np = X_train.values
        y_clf_np = y_clf_train.values

        self._log(f"üìä Training data: {X_train.shape[0]} samples, {X_train.shape[1]} features")
        self._log(f"üìä Positive class ratio: {y_clf_train.mean():.2%}")

        # 3. –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
        if HAS_LIGHTGBM:
            self._log("üöÄ Using LightGBM")

            # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
            if self.use_optuna:
                self._log(f"üîç Optimizing classifier hyperparameters ({self.optuna_trials} trials)...")
                self.best_params_clf_ = self._optimize_classifier(X_train_np, y_clf_np)
            else:
                self.best_params_clf_ = {
                    "n_estimators": 300,
                    "learning_rate": 0.05,
                    "max_depth": 6,
                    "num_leaves": 63,
                    "min_child_samples": 20,
                    "subsample": 0.8,
                    "colsample_bytree": 0.8,
                    "reg_alpha": 0.1,
                    "reg_lambda": 0.1,
                    "random_state": self.random_state,
                    "verbosity": -1,
                    "n_jobs": -1,
                }

            self.clf = lgb.LGBMClassifier(**self.best_params_clf_)
        else:
            self._log("‚ö†Ô∏è LightGBM not found, using sklearn GradientBoosting (slower)")
            self.clf = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=5,
                subsample=0.8,
                random_state=self.random_state,
            )

        # 4. –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        self._log("üéØ Training classifier...")
        self.clf.fit(X_train_np, y_clf_np)

        # 5. –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        if self.calibrate_proba:
            self._log("üìê Calibrating probabilities...")
            self.clf_calibrated = CalibratedClassifierCV(
                self.clf, method="isotonic", cv=3
            )
            self.clf_calibrated.fit(X_train_np, y_clf_np)

        # 6. –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
        proba_train = self._get_proba(X_train_np)
        self.optimal_threshold_ = self._find_optimal_threshold(y_clf_np, proba_train)
        self._log(f"üìä Optimal classification threshold: {self.optimal_threshold_:.3f}")

        # 7. –û–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ (—Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö)
        mask_pos = y_clf_train == 1
        n_positive = mask_pos.sum()
        self._log(f"üìà Training regressors on {n_positive} positive samples...")

        if n_positive >= 10:  # –ú–∏–Ω–∏–º—É–º 10 –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
            X_pos = X_train_np[mask_pos]
            y_days_pos = y_days_train[mask_pos].values
            y_amount_pos = y_amount_train[mask_pos].values

            if HAS_LIGHTGBM:
                # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤
                if self.use_optuna and n_positive >= 30:
                    self._log("üîç Optimizing days regressor...")
                    self.best_params_reg_days_ = self._optimize_regressor(X_pos, y_days_pos)
                    self._log("üîç Optimizing amount regressor...")
                    self.best_params_reg_amount_ = self._optimize_regressor(X_pos, y_amount_pos)
                else:
                    base_reg_params = {
                        "n_estimators": 200,
                        "learning_rate": 0.05,
                        "max_depth": 5,
                        "num_leaves": 31,
                        "min_child_samples": 10,
                        "subsample": 0.8,
                        "colsample_bytree": 0.8,
                        "random_state": self.random_state,
                        "verbosity": -1,
                        "n_jobs": -1,
                    }
                    self.best_params_reg_days_ = base_reg_params.copy()
                    self.best_params_reg_amount_ = base_reg_params.copy()

                self.reg_days = lgb.LGBMRegressor(**self.best_params_reg_days_)
                self.reg_amount = lgb.LGBMRegressor(**self.best_params_reg_amount_)
            else:
                self.reg_days = GradientBoostingRegressor(
                    n_estimators=200,
                    learning_rate=0.05,
                    max_depth=5,
                    subsample=0.8,
                    random_state=self.random_state,
                )
                self.reg_amount = GradientBoostingRegressor(
                    n_estimators=200,
                    learning_rate=0.05,
                    max_depth=5,
                    subsample=0.8,
                    random_state=self.random_state,
                )

            self.reg_days.fit(X_pos, y_days_pos)
            self.reg_amount.fit(X_pos, y_amount_pos)
        else:
            self._log("‚ö†Ô∏è Too few positive samples, training regressors on all data")
            if HAS_LIGHTGBM:
                self.reg_days = lgb.LGBMRegressor(random_state=self.random_state, verbosity=-1)
                self.reg_amount = lgb.LGBMRegressor(random_state=self.random_state, verbosity=-1)
            else:
                self.reg_days = GradientBoostingRegressor(random_state=self.random_state)
                self.reg_amount = GradientBoostingRegressor(random_state=self.random_state)

            self.reg_days.fit(X_train_np, y_days_train.values)
            self.reg_amount.fit(X_train_np, y_amount_train.values)

        # 8. Feature importance
        self._compute_feature_importance(X_train)

        # 9. –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        metrics = {}
        if val_df is not None and not val_df.empty:
            self._log("üìä Evaluating on validation set...")
            metrics = self.evaluate(val_df)

        self._log("‚úÖ Training complete!")
        return metrics

    def _get_proba(self, X: np.ndarray) -> np.ndarray:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å —É—á–µ—Ç–æ–º –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏."""
        if self.calibrate_proba and self.clf_calibrated is not None:
            return self.clf_calibrated.predict_proba(X)[:, 1]
        return self.clf.predict_proba(X)[:, 1]

    def _compute_feature_importance(self, X_train: pd.DataFrame):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        if HAS_LIGHTGBM and hasattr(self.clf, "feature_importances_"):
            importance = self.clf.feature_importances_
            self.feature_importance_ = pd.DataFrame({
                "feature": X_train.columns,
                "importance": importance
            }).sort_values("importance", ascending=False)

    # ========= –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï =========

    def predict(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame —Å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏:
        - purchase_proba
        - will_purchase_pred (0/1)
        - days_to_next_pred
        - next_purchase_amount_pred
        """
        if any(m is None for m in [self.clf, self.reg_days, self.reg_amount]):
            raise ValueError("Model is not fitted. Call fit() first.")

        # –£–±–∏—Ä–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã, –µ—Å–ª–∏ –µ—Å—Ç—å
        X_raw = df.copy()
        for col in ["will_purchase_next_7d", "days_to_next_purchase", "next_purchase_amount"]:
            if col in X_raw.columns:
                X_raw = X_raw.drop(columns=[col])

        # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        X = self._prepare_features_infer(X_raw)
        X_np = X.values

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (—Å –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏)
        proba = self._get_proba(X_np)
        will_purchase_pred = (proba >= self.optimal_threshold_).astype(int)

        # –†–µ–≥—Ä–µ—Å—Å–∏—è
        days_pred = self.reg_days.predict(X_np)
        amount_pred = self.reg_amount.predict(X_np)

        # –ú–∞—Å–∫–∏—Ä—É–µ–º —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Ö, —É –∫–æ–≥–æ –º–æ–¥–µ–ª—å —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ –ø–æ–∫—É–ø–∫–∏ –Ω–µ –±—É–¥–µ—Ç
        days_pred = np.where(will_purchase_pred == 1, days_pred, 999.0)
        amount_pred = np.where(will_purchase_pred == 1, amount_pred, 0.0)

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–∞–∑—É–º–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
        days_pred = np.clip(days_pred, 0, 999)
        amount_pred = np.clip(amount_pred, 0, None)

        result = pd.DataFrame(
            {
                "purchase_proba": proba,
                "will_purchase_pred": will_purchase_pred,
                "days_to_next_pred": days_pred,
                "next_purchase_amount_pred": amount_pred,
            },
            index=df.index,
        )

        return result

    # ========= –û–¶–ï–ù–ö–ê =========

    def evaluate(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        –°—á–∏—Ç–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–º DataFrame —Å —Ç–∞—Ä–≥–µ—Ç–∞–º–∏:
        - AUC-ROC, PR-AUC, F1 –¥–ª—è will_purchase_next_7d
        - RMSE / MAE –¥–ª—è –¥–Ω–µ–π –¥–æ –ø–æ–∫—É–ø–∫–∏ (—Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–µ—Ö, –≥–¥–µ –±—ã–ª–∞ –ø–æ–∫—É–ø–∫–∞)
        - RMSE / MAE –¥–ª—è —Å—É–º–º—ã –ø–æ–∫—É–ø–∫–∏ (—Ç–æ–ª—å–∫–æ –Ω–∞ —Ç–µ—Ö, –≥–¥–µ –±—ã–ª–∞ –ø–æ–∫—É–ø–∫–∞)
        """
        X_raw, y_clf, y_days, y_amount = self._split_features_targets(df)
        X = self._prepare_features_infer(X_raw)
        X_np = X.values

        proba = self._get_proba(X_np)
        preds = (proba >= self.optimal_threshold_).astype(int)

        metrics = {}

        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        if len(np.unique(y_clf)) > 1:
            metrics["auc_roc"] = roc_auc_score(y_clf, proba)
            metrics["auc_pr"] = average_precision_score(y_clf, proba)
            metrics["f1"] = f1_score(y_clf, preds)
            metrics["threshold"] = self.optimal_threshold_
        else:
            metrics["auc_roc"] = float("nan")
            metrics["auc_pr"] = float("nan")
            metrics["f1"] = float("nan")
            metrics["threshold"] = self.optimal_threshold_

        # –†–µ–≥—Ä–µ—Å—Å–∏–∏ ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö
        mask_pos = y_clf == 1
        if mask_pos.sum() > 0:
            days_pred = self.reg_days.predict(X_np[mask_pos])
            amount_pred = self.reg_amount.predict(X_np[mask_pos])

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: —Ç–µ–ø–µ—Ä—å —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ RMSE, –∞ –Ω–µ MSE
            metrics["rmse_days"] = np.sqrt(mean_squared_error(y_days[mask_pos], days_pred))
            metrics["mae_days"] = mean_absolute_error(y_days[mask_pos], days_pred)

            metrics["rmse_amount"] = np.sqrt(mean_squared_error(y_amount[mask_pos], amount_pred))
            metrics["mae_amount"] = mean_absolute_error(y_amount[mask_pos], amount_pred)
        else:
            metrics["rmse_days"] = float("nan")
            metrics["mae_days"] = float("nan")
            metrics["rmse_amount"] = float("nan")
            metrics["mae_amount"] = float("nan")

        return metrics

    def get_feature_importance(self, top_n: int = 20) -> Optional[pd.DataFrame]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ø-N –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤."""
        if self.feature_importance_ is not None:
            return self.feature_importance_.head(top_n)
        return None

    # ========= –°–ï–†–ò–ê–õ–ò–ó–ê–¶–ò–Ø =========

    def save(self, path: str):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        state = {
            "random_state": self.random_state,
            "use_optuna": self.use_optuna,
            "optuna_trials": self.optuna_trials,
            "calibrate_proba": self.calibrate_proba,
            "clf": self.clf,
            "clf_calibrated": self.clf_calibrated,
            "reg_days": self.reg_days,
            "reg_amount": self.reg_amount,
            "feature_columns_": self.feature_columns_,
            "label_encoders_": self.label_encoders_,
            "numeric_medians_": self.numeric_medians_,
            "numeric_stds_": self.numeric_stds_,
            "optimal_threshold_": self.optimal_threshold_,
            "feature_importance_": self.feature_importance_,
            "best_params_clf_": self.best_params_clf_,
            "best_params_reg_days_": self.best_params_reg_days_,
            "best_params_reg_amount_": self.best_params_reg_amount_,
        }
        joblib.dump(state, path)

    @classmethod
    def load(cls, path: str) -> "ContextAwareModel":
        state = joblib.load(path)
        model = cls(
            random_state=state.get("random_state", 42),
            use_optuna=state.get("use_optuna", False),  # –ü—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –Ω–µ –Ω—É–∂–µ–Ω Optuna
            calibrate_proba=state.get("calibrate_proba", True),
        )
        model.clf = state["clf"]
        model.clf_calibrated = state.get("clf_calibrated")
        model.reg_days = state["reg_days"]
        model.reg_amount = state["reg_amount"]
        model.feature_columns_ = state["feature_columns_"]
        model.label_encoders_ = state.get("label_encoders_", {})
        model.numeric_medians_ = state.get("numeric_medians_", {})
        model.numeric_stds_ = state.get("numeric_stds_", {})
        model.optimal_threshold_ = state.get("optimal_threshold_", 0.5)
        model.feature_importance_ = state.get("feature_importance_")
        model.best_params_clf_ = state.get("best_params_clf_")
        model.best_params_reg_days_ = state.get("best_params_reg_days_")
        model.best_params_reg_amount_ = state.get("best_params_reg_amount_")
        return model