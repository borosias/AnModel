# innovative_models.py
from random import random

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple
import warnings
from pathlib import Path
from scipy import stats
import statsmodels.api as sm
from statsmodels.tsa.stattools import grangercausalitytests
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.isotonic import IsotonicRegression
from sklearn.metrics import brier_score_loss
import optuna  # pip install optuna
from catboost import CatBoostRegressor, CatBoostClassifier  # pip install catboost


warnings.filterwarnings('ignore')


# ===== –ú–û–î–ï–õ–¨ 1: Context-Aware Purchase Prediction =====

class ContextAwareModel:
    """–ú–æ–¥–µ–ª—å 1 —Å Bayesian –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π –∏ uncertainty quantification"""

    def __init__(self, enable_bayesian: bool = True):
        self.models = {
            'will_purchase': lgb.LGBMClassifier(),
            'category': lgb.LGBMClassifier(),
            'days_to_purchase': lgb.LGBMRegressor(),
            'purchase_amount': lgb.LGBMRegressor()
        }
        self.enable_bayesian = enable_bayesian
        self.calibrators = {}
        self.uncertainty_models = {}  # –î–ª—è –æ—Ü–µ–Ω–∫–∏ uncertainty

    def train(self, train_df: pd.DataFrame):
        print("üîÆ Training Context-Aware Model with Bayesian calibration...")

        X = self._prepare_features(train_df)
        self.feature_columns = X.columns.tolist()

        # 1. Bayesian –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if self.enable_bayesian:
            self._optimize_hyperparameters(X, train_df)

        # 2. –û–±—É—á–µ–Ω–∏–µ —Å TimeSeries Cross-Validation
        tscv = TimeSeriesSplit(n_splits=5)

        # Purchase probability —Å –∫–∞–ª–∏–±—Ä–æ–≤–∫–æ–π
        y_will = train_df['target_will_purchase']

        # –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π (Isotonic/Platt scaling)
        base_model = lgb.LGBMClassifier()
        self.calibrators['will_purchase'] = CalibratedClassifierCV(
            base_model, method='isotonic', cv=3
        )
        self.calibrators['will_purchase'].fit(X, y_will)

        # –¢–∞–∫–∂–µ –æ–±—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª—å
        self.models['will_purchase'].fit(X, y_will)

        # 3. Bayesian calibration –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
        self._train_bayesian_calibration(X, y_will)

        # 4. Quantile Regression –¥–ª—è uncertainty –≤ —Ä–µ–≥—Ä–µ—Å—Å–∏—è—Ö
        if 'target_days_to_purchase' in train_df.columns:
            self._train_quantile_regression(
                X, train_df['target_days_to_purchase'], 'days'
            )

        if 'target_purchase_amount' in train_df.columns:
            self._train_quantile_regression(
                X, train_df['target_purchase_amount'], 'amount'
            )

    def _train_bayesian_calibration(self, X, y):
        """Bayesian calibration —Å Beta —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º"""
        # –ü–æ–ª—É—á–∞–µ–º raw scores
        raw_probs = self.models['will_purchase'].predict_proba(X)[:, 1]

        # Fit Beta distribution –∫ –∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω—ã–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º
        calibrated_probs = self.calibrators['will_purchase'].predict_proba(X)[:, 1]

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Beta —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        from scipy.stats import beta
        alpha, beta_param, loc, scale = beta.fit(calibrated_probs, floc=0, fscale=1)

        self.beta_params = {'alpha': alpha, 'beta': beta_param}

        # –¢–∞–∫–∂–µ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å uncertainty
        errors = np.abs(calibrated_probs - y)
        uncertainty_model = lgb.LGBMRegressor()
        uncertainty_model.fit(X, errors)
        self.uncertainty_models['will_purchase'] = uncertainty_model

    def _train_quantile_regression(self, X, y, target_name):
        """–û–±—É—á–∞–µ—Ç quantile regression –¥–ª—è confidence intervals"""
        # –î–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–≤–∞–Ω—Ç–∏–ª–µ–π
        quantiles = [0.05, 0.5, 0.95]  # 90% –∏–Ω—Ç–µ—Ä–≤–∞–ª

        for q in quantiles:
            model = lgb.LGBMRegressor(
                objective='quantile',
                alpha=q,
                metric='quantile'
            )
            model.fit(X, y)
            self.uncertainty_models[f'{target_name}_q{q}'] = model

    def predict_with_uncertainty(self, X: pd.DataFrame) -> Dict:
        """–ü—Ä–æ–≥–Ω–æ–∑ —Å –æ—Ü–µ–Ω–∫–æ–π uncertainty"""
        predictions = self.predict(X)

        # –î–æ–±–∞–≤–ª—è–µ–º uncertainty
        if 'purchase_probability' in predictions:
            probs = predictions['purchase_probability']

            # Bayesian credible intervals
            from scipy.stats import beta
            alpha, beta_param = self.beta_params['alpha'], self.beta_params['beta']

            # –î–ª—è –∫–∞–∂–¥–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –≤—ã—á–∏—Å–ª—è–µ–º credible interval
            ci_lower = []
            ci_upper = []

            for p in probs:
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã Beta
                a = alpha * p
                b = beta_param * (1 - p)

                lower, upper = beta.interval(0.9, a, b)
                ci_lower.append(lower)
                ci_upper.append(upper)

            predictions['probability_lower'] = np.array(ci_lower)
            predictions['probability_upper'] = np.array(ci_upper)

        # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–π –¥–æ–±–∞–≤–ª—è–µ–º quantile –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
        if 'days_to_purchase' in predictions:
            q05 = self.uncertainty_models['days_q0.05'].predict(X)
            q95 = self.uncertainty_models['days_q0.95'].predict(X)
            predictions['days_lower'] = q05
            predictions['days_upper'] = q95

        return predictions

    def _optimize_hyperparameters(self, X, train_df):
        """Bayesian optimization –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        try:
            import optuna

            def objective(trial):
                params = {
                    'n_estimators': trial.suggest_int('n_estimators', 100, 500),
                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                    'max_depth': trial.suggest_int('max_depth', 3, 12),
                    'num_leaves': trial.suggest_int('num_leaves', 20, 100),
                    'subsample': trial.suggest_float('subsample', 0.6, 1.0),
                    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
                }

                model = lgb.LGBMClassifier(**params)

                # TimeSeries CV –æ—Ü–µ–Ω–∫–∞
                tscv = TimeSeriesSplit(n_splits=3)
                scores = cross_val_score(
                    model, X, train_df['target_will_purchase'],
                    cv=tscv, scoring='roc_auc'
                )

                return scores.mean()

            study = optuna.create_study(direction='maximize')
            study.optimize(objective, n_trials=30)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            self.models['will_purchase'].set_params(**study.best_params)

        except ImportError:
            print("‚ö†Ô∏è Optuna not installed, using default hyperparameters")

# ===== –ú–û–î–ï–õ–¨ 2: Cross-Region Demand Transfer =====

class CrossRegionModel:
    """–ú–æ–¥–µ–ª—å 2 —Å Graph Neural Networks –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞"""

    def __init__(self, use_gnn: bool = False):
        self.region_models = {}
        self.region_graph = None
        self.use_gnn = use_gnn

    def _build_region_graph(self, train_df: pd.DataFrame):
        """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ —Ä–µ–≥–∏–æ–Ω–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        regions = train_df['region'].unique()

        # –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏
        corr_matrix = pd.DataFrame(index=regions, columns=regions)

        for i, region_i in enumerate(regions):
            for j, region_j in enumerate(regions):
                if i >= j:
                    continue

                # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
                data_i = train_df[train_df['region'] == region_i]
                data_j = train_df[train_df['region'] == region_j]

                if len(data_i) > 0 and len(data_j) > 0:
                    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –¥–Ω—è–º
                    daily_i = data_i.groupby('snapshot_date')['target_purchase_count'].sum()
                    daily_j = data_j.groupby('snapshot_date')['target_purchase_count'].sum()

                    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–∞—Ç—ã
                    common_dates = daily_i.index.intersection(daily_j.index)
                    if len(common_dates) >= 7:
                        corr = np.corrcoef(
                            daily_i.loc[common_dates].values,
                            daily_j.loc[common_dates].values
                        )[0, 1]
                        corr_matrix.loc[region_i, region_j] = corr
                        corr_matrix.loc[region_j, region_i] = corr

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        corr_matrix = corr_matrix.fillna(0)
        self.region_graph = corr_matrix

        # –¢–∞–∫–∂–µ –≤—ã—á–∏—Å–ª—è–µ–º lead-lag –æ—Ç–Ω–æ—à–µ–Ω–∏—è
        self._compute_lead_lag_relationships(train_df)

    def _compute_lead_lag_relationships(self, train_df: pd.DataFrame):
        """–í—ã—á–∏—Å–ª—è–µ—Ç lead-lag –æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏"""
        regions = train_df['region'].unique()
        self.lead_lag_matrix = pd.DataFrame(index=regions, columns=regions)

        for source in regions:
            for target in regions:
                if source == target:
                    continue

                source_data = train_df[train_df['region'] == source]
                target_data = train_df[train_df['region'] == target]

                if len(source_data) > 14 and len(target_data) > 14:
                    # –ì—Ä—ç–Ω–¥–∂–µ—Ä –∫–∞—É–∑–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç
                    try:
                        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø–∞–Ω–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        source_series = source_data.groupby('snapshot_date')['target_purchase_count'].sum()
                        target_series = target_data.groupby('snapshot_date')['target_purchase_count'].sum()

                        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º
                        common_idx = source_series.index.intersection(target_series.index)
                        if len(common_idx) >= 14:
                            data = pd.DataFrame({
                                'source': source_series.loc[common_idx],
                                'target': target_series.loc[common_idx]
                            })

                            # Granger causality test
                            gc_result = grangercausalitytests(data[['target', 'source']], maxlag=3, verbose=False)

                            # –ë–µ—Ä–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π p-value
                            p_values = [gc_result[lag][0]['ssr_ftest'][1] for lag in range(1, 4)]
                            min_p = min(p_values)

                            if min_p < 0.05:
                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
                                gc_result_rev = grangercausalitytests(data[['source', 'target']], maxlag=3,
                                                                      verbose=False)
                                p_values_rev = [gc_result_rev[lag][0]['ssr_ftest'][1] for lag in range(1, 4)]
                                min_p_rev = min(p_values_rev)

                                if min_p < min_p_rev:
                                    self.lead_lag_matrix.loc[source, target] = 'source_lead'
                                else:
                                    self.lead_lag_matrix.loc[source, target] = 'target_lead'
                    except:
                        pass

    def train(self, train_df: pd.DataFrame):
        print("üåç Training Cross-Region Model with Graph Analysis...")

        # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ —Ä–µ–≥–∏–æ–Ω–æ–≤
        self._build_region_graph(train_df)

        regions = train_df['region'].unique()

        for target_region in regions:
            print(f"  Target region: {target_region}")

            target_data = train_df[train_df['region'] == target_region]

            if len(target_data) < 10:
                print(f"    ‚ö†Ô∏è Not enough data, skipping...")
                continue

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º source —Ä–µ–≥–∏–æ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–∞
            source_regions = self._select_source_regions(target_region, train_df)

            # –û–±—É—á–∞–µ–º CatBoost —Å —É—á–µ—Ç–æ–º –≥—Ä–∞—Ñ–∞
            self._train_with_graph_awareness(
                target_region, target_data, source_regions, train_df
            )

    def _select_source_regions(self, target_region: str, train_df: pd.DataFrame) -> List[str]:
        """–í—ã–±–∏—Ä–∞–µ—Ç source —Ä–µ–≥–∏–æ–Ω—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–∞"""
        if self.region_graph is None:
            return [r for r in train_df['region'].unique() if r != target_region]

        # –ë–µ—Ä–µ–º —Ä–µ–≥–∏–æ–Ω—ã —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π
        correlations = self.region_graph.loc[target_region].sort_values(ascending=False)

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ lead-lag –æ—Ç–Ω–æ—à–µ–Ω–∏—è–º
        selected = []
        for region, corr in correlations.items():
            if region == target_region:
                continue

            if pd.notna(corr) and abs(corr) > 0.3:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º lead-lag
                if region in self.lead_lag_matrix.index and target_region in self.lead_lag_matrix.columns:
                    relation = self.lead_lag_matrix.loc[region, target_region]
                    if relation == 'source_lead':  # source –≤–µ–¥–µ—Ç target
                        selected.append(region)

            if len(selected) >= 3:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                break

        return selected

    def _train_with_graph_awareness(self, target_region, target_data, source_regions, train_df):
        """–û–±—É—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –≥—Ä–∞—Ñ–∞ —Ä–µ–≥–∏–æ–Ω–æ–≤"""

        X_all, y_all = [], []

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ target —Ä–µ–≥–∏–æ–Ω–∞
        X_target, y_target = self._prepare_regression_features(target_data)
        X_all.append(X_target)
        y_all.append(y_target)

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ source —Ä–µ–≥–∏–æ–Ω–æ–≤
        for source_region in source_regions:
            source_data = train_df[train_df['region'] == source_region]

            if len(source_data) > 0:
                # –£–º–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º –≥—Ä–∞—Ñ–∞
                X_source, y_source = self._transform_with_graph(
                    source_data, source_region, target_region
                )

                X_all.append(X_source)
                y_all.append(y_source)

        if X_all:
            X_combined = pd.concat(X_all, ignore_index=True)
            y_combined = pd.concat(y_all, ignore_index=True)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º CatBoost —Å —É—á–µ—Ç–æ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Ñ–∏—á
            model = CatBoostRegressor(
                iterations=500,
                learning_rate=0.05,
                depth=6,
                cat_features=['region_encoded'] if 'region_encoded' in X_combined.columns else None,
                verbose=False
            )

            model.fit(X_combined, y_combined['target_purchase_count'])
            self.region_models[target_region] = model

            # –û—Ü–µ–Ω–∫–∞
            preds = model.predict(X_target)
            mae = np.mean(np.abs(preds - y_target['target_purchase_count']))
            rmse = np.sqrt(np.mean((preds - y_target['target_purchase_count']) ** 2))

            print(f"    ‚úÖ MAE: {mae:.2f}, RMSE: {rmse:.2f}")

# ===== –ú–û–î–ï–õ–¨ 3: Micro-Trend Anticipation =====

class MicroTrendModel(nn.Module):
    """–ú–æ–¥–µ–ª—å 3 —Å advanced change point detection"""

    def __init__(self, input_dim: int = 20):
        super().__init__()

        # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ ...

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è
        self.change_point_detector = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def detect_change_points(self, sequence_data: np.ndarray) -> Dict:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Ç–æ—á–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        self.eval()
        with torch.no_grad():
            tensor_data = torch.FloatTensor(sequence_data).unsqueeze(0)

            # LSTM encoding
            lstm_out, _ = self.lstm(tensor_data)

            # –î–µ—Ç–µ–∫—Ü–∏—è —Ç–æ—á–µ–∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            change_probs = self.change_point_detector(lstm_out).squeeze().numpy()

            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –∏–∑–º–µ–Ω–µ–Ω–∏—è
            change_points = np.where(change_probs > 0.7)[0]

            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–µ–≥–º–µ–Ω—Ç—ã –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            segments = []
            prev_point = 0

            for cp in change_points:
                segment_data = sequence_data[prev_point:cp]
                if len(segment_data) >= 3:
                    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –≤ —Å–µ–≥–º–µ–Ω—Ç–µ
                    x = np.arange(len(segment_data))
                    coeffs = np.polyfit(x, segment_data.mean(axis=1), 1)
                    slope = coeffs[0]

                    segments.append({
                        'start': prev_point,
                        'end': cp,
                        'slope': slope,
                        'length': cp - prev_point,
                        'magnitude': np.mean(segment_data)
                    })

                prev_point = cp

            # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç
            if prev_point < len(sequence_data):
                segment_data = sequence_data[prev_point:]
                if len(segment_data) >= 3:
                    x = np.arange(len(segment_data))
                    coeffs = np.polyfit(x, segment_data.mean(axis=1), 1)
                    slope = coeffs[0]

                    segments.append({
                        'start': prev_point,
                        'end': len(sequence_data),
                        'slope': slope,
                        'length': len(sequence_data) - prev_point,
                        'magnitude': np.mean(segment_data)
                    })

            return {
                'change_points': change_points.tolist(),
                'segments': segments,
                'num_changes': len(change_points)
            }

    def _bayesian_trend_analysis(self, sequence_data: np.ndarray) -> Dict:
        """Bayesian –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        # Bayesian linear regression –¥–ª—è –æ—Ü–µ–Ω–∫–∏ uncertainty
        n = len(sequence_data)
        x = np.arange(n)
        y = sequence_data.mean(axis=1) if sequence_data.ndim > 1 else sequence_data

        # Bayesian –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
        # Prior: slope ~ Normal(0, 1), intercept ~ Normal(mean(y), 10)

        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å MCMC (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è)
        try:
            import pymc3 as pm

            with pm.Model() as model:
                # Priors
                sigma = pm.HalfCauchy('sigma', beta=10)
                intercept = pm.Normal('intercept', mu=np.mean(y), sigma=10)
                slope = pm.Normal('slope', mu=0, sigma=1)

                # Likelihood
                likelihood = pm.Normal('y',
                                       mu=intercept + slope * x,
                                       sigma=sigma,
                                       observed=y)

                # Sampling
                trace = pm.sample(1000, tune=1000, return_inferencedata=False)

                # Posterior analysis
                slope_samples = trace['slope']
                slope_mean = np.mean(slope_samples)
                slope_std = np.std(slope_samples)

                # Probability that slope > 0 (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥)
                prob_positive = np.mean(slope_samples > 0)

                # Credible interval
                ci_lower, ci_upper = np.percentile(slope_samples, [5, 95])

                return {
                    'slope_mean': slope_mean,
                    'slope_std': slope_std,
                    'prob_positive': prob_positive,
                    'ci_90_lower': ci_lower,
                    'ci_90_upper': ci_upper
                }

        except ImportError:
            # Fallback –Ω–∞ frequentist –ø–æ–¥—Ö–æ–¥
            X = sm.add_constant(x)
            model = sm.OLS(y, X)
            results = model.fit()

            return {
                'slope_mean': results.params[1],
                'slope_std': results.bse[1],
                'prob_positive': 1 - stats.norm.cdf(0, loc=results.params[1], scale=results.bse[1]),
                'ci_90_lower': results.conf_int(alpha=0.1)[1, 0],
                'ci_90_upper': results.conf_int(alpha=0.1)[1, 1]
            }

# ===== –ú–û–î–ï–õ–¨ 4: Adaptive Pricing Prophet =====

class AdaptivePricingModel:
    """–ú–æ–¥–µ–ª—å 4 —Å Reinforcement Learning –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

    def __init__(self, use_rl: bool = True):
        self.price_model = RandomForestRegressor(n_estimators=100)
        self.demand_model = xgb.XGBRegressor()
        self.optimal_prices = {}
        self.item_elasticity = {}

        if use_rl:
            self.rl_agent = self._create_rl_agent()
        self.use_rl = use_rl

    def _create_rl_agent(self):
        """–°–æ–∑–¥–∞–µ—Ç RL –∞–≥–µ–Ω—Ç–∞ –¥–ª—è —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        # Q-learning agent —Å neural network approximation
        class PricingAgent:
            def __init__(self, state_dim, action_dim):
                self.q_network = nn.Sequential(
                    nn.Linear(state_dim, 64),
                    nn.ReLU(),
                    nn.Linear(64, 64),
                    nn.ReLU(),
                    nn.Linear(64, action_dim)
                )
                self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=0.001)
                self.memory = []  # Experience replay

            def act(self, state, epsilon=0.1):
                if np.random.random() < epsilon:
                    return np.random.randint(self.action_dim)
                else:
                    with torch.no_grad():
                        state_tensor = torch.FloatTensor(state)
                        q_values = self.q_network(state_tensor)
                        return torch.argmax(q_values).item()

            def remember(self, state, action, reward, next_state, done):
                self.memory.append((state, action, reward, next_state, done))

            def replay(self, batch_size=32):
                if len(self.memory) < batch_size:
                    return

                batch = random.sample(self.memory, batch_size)

                for state, action, reward, next_state, done in batch:
                    # Q-learning update
                    state_tensor = torch.FloatTensor(state)
                    next_state_tensor = torch.FloatTensor(next_state)

                    current_q = self.q_network(state_tensor)[action]
                    next_q = torch.max(self.q_network(next_state_tensor))
                    target_q = reward + (0.99 * next_q * (1 - done))

                    loss = nn.MSELoss()(current_q, target_q.detach())

                    self.optimizer.zero_grad()
                    loss.backward()
                    self.optimizer.step()

        return PricingAgent(state_dim=10, action_dim=20)  # 20 –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ü–µ–Ω

    def _train_rl_agent(self, historical_data: pd.DataFrame):
        """–û–±—É—á–∞–µ—Ç RL –∞–≥–µ–Ω—Ç–∞ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not self.use_rl:
            return

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
        states = []
        actions = []
        rewards = []

        for item_id in historical_data['item_id'].unique():
            item_data = historical_data[historical_data['item_id'] == item_id]
            item_data = item_data.sort_values('snapshot_date')

            if len(item_data) < 10:
                continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º —ç–ø–∏–∑–æ–¥—ã
            for i in range(len(item_data) - 1):
                # –°–æ—Å—Ç–æ—è–Ω–∏–µ: —Ñ–∏—á–∏ —Ç–æ–≤–∞—Ä–∞
                state = self._get_state_features(item_data.iloc[i])

                # –î–µ–π—Å—Ç–≤–∏–µ: —Ü–µ–Ω–∞ (–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è)
                current_price = item_data.iloc[i]['current_price']
                action = self._discretize_price(current_price)

                # –ù–∞–≥—Ä–∞–¥–∞: –ø—Ä–∏–±—ã–ª—å –Ω–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ
                next_profit = item_data.iloc[i + 1]['target_revenue'] - item_data.iloc[i + 1]['current_price']
                reward = next_profit

                # –°–ª–µ–¥—É—é—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                next_state = self._get_state_features(item_data.iloc[i + 1])

                self.rl_agent.remember(state, action, reward, next_state, done=False)

        # –û–±—É—á–µ–Ω–∏–µ
        for epoch in range(100):
            self.rl_agent.replay()

    def _get_state_features(self, row: pd.Series) -> np.ndarray:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ñ–∏—á–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–ª—è RL"""
        features = [
            row.get('current_price', 0),
            row.get('conversion_rate', 0),
            row.get('price_elasticity', -1),
            row.get('total_views', 0),
            row.get('total_purchases', 0),
            row.get('price_position', 1),
            row.get('price_volatility', 0),
            row.get('category_avg_price', 0),
            row.get('days_since_last_purchase', 30),
            row.get('inventory_level', 100) if 'inventory_level' in row else 100
        ]
        return np.array(features, dtype=np.float32)

    def _discretize_price(self, price: float) -> int:
        """–î–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä—É–µ—Ç —Ü–µ–Ω—É –¥–ª—è RL"""
        # 20 bins –æ—Ç 0.5x –¥–æ 2x –æ—Ç —Ç–µ–∫—É—â–µ–π —Ü–µ–Ω—ã
        min_price = price * 0.5
        max_price = price * 2.0
        bins = np.linspace(min_price, max_price, 20)
        return np.digitize(price, bins) - 1

    def recommend_price_rl(self, item_id: str, current_state: Dict) -> Dict:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ü–µ–Ω—ã —Å –ø–æ–º–æ—â—å—é RL"""
        if not self.use_rl or self.rl_agent is None:
            return self.recommend_price(item_id, current_state['current_price'], current_state)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ features
        state_features = np.array([
            current_state.get('current_price', 0),
            current_state.get('conversion_rate', 0),
            current_state.get('price_elasticity', -1),
            current_state.get('total_views', 0),
            current_state.get('total_purchases', 0),
            current_state.get('price_position', 1),
            current_state.get('price_volatility', 0),
            current_state.get('category_avg_price', 0),
            current_state.get('days_since_last_purchase', 30),
            current_state.get('inventory_level', 100)
        ], dtype=np.float32)

        # –î–µ–π—Å—Ç–≤–∏–µ –æ—Ç RL –∞–≥–µ–Ω—Ç–∞
        action = self.rl_agent.act(state_features, epsilon=0.1)

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ü–µ–Ω—É
        min_price = current_state['current_price'] * 0.5
        max_price = current_state['current_price'] * 2.0
        price_bins = np.linspace(min_price, max_price, 20)
        recommended_price = price_bins[action]

        # Bayesian optimization –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        recommended_price = self._bayesian_price_optimization(
            item_id, recommended_price, current_state
        )

        return {
            'item_id': item_id,
            'current_price': current_state['current_price'],
            'recommended_price': float(recommended_price),
            'method': 'reinforcement_learning',
            'confidence': self._calculate_price_confidence(item_id, recommended_price, current_state)
        }

    def _bayesian_price_optimization(self, item_id: str, initial_price: float, context: Dict) -> float:
        """Bayesian optimization –¥–ª—è —Ç–æ–Ω–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–µ–Ω—ã"""
        try:
            import optuna

            def objective(trial):
                # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º —Ü–µ–Ω—É –≤ –æ–∫—Ä–µ—Å—Ç–Ω–æ—Å—Ç–∏ initial_price
                price = trial.suggest_float(
                    'price',
                    initial_price * 0.9,
                    initial_price * 1.1
                )

                # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º —Å–ø—Ä–æ—Å
                demand = self._predict_demand(item_id, price, context)

                # –ü—Ä–∏–±—ã–ª—å (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                profit = price * demand

                return -profit  # –ú–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—É—é –ø—Ä–∏–±—ã–ª—å

            study = optuna.create_study(direction='minimize')
            study.optimize(objective, n_trials=20)

            return study.best_params['price']

        except ImportError:
            return initial_price

# ===== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ò –ó–ê–ü–£–°–ö =====

class InnovationPipeline:
    """–ì–ª–∞–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≤—Å–µ—Ö 4 –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, snapshots_dir: str = "../analytics/data/innovative_snapshots"):
        self.snapshots_dir = Path(snapshots_dir)
        self.models = {
            'context_aware': ContextAwareModel(),
            'cross_region': CrossRegionModel(),
            'micro_trend': MicroTrendModel(),
            'adaptive_pricing': AdaptivePricingModel()
        }

    def train_all_models(self):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ 4 –º–æ–¥–µ–ª–∏"""

        print("=" * 60)
        print("üöÄ TRAINING ALL 4 INNOVATIVE MODELS")
        print("=" * 60)

        # –ú–æ–¥–µ–ª—å 1: Context-Aware
        print("\n1Ô∏è‚É£ Context-Aware Purchase Prediction")
        try:
            train_df = pd.read_parquet(self.snapshots_dir / "model1/train.parquet")
            self.models['context_aware'].train(train_df)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")

        # –ú–æ–¥–µ–ª—å 2: Cross-Region
        print("\n2Ô∏è‚É£ Cross-Region Demand Transfer")
        try:
            train_df = pd.read_parquet(self.snapshots_dir / "model2/train.parquet")
            self.models['cross_region'].train(train_df)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")

        # –ú–æ–¥–µ–ª—å 3: Micro-Trend (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        print("\n3Ô∏è‚É£ Micro-Trend Anticipation")
        print("   ‚ö†Ô∏è Note: Requires PyTorch and more data")

        # –ú–æ–¥–µ–ª—å 4: Adaptive Pricing
        print("\n4Ô∏è‚É£ Adaptive Pricing Prophet")
        try:
            train_df = pd.read_parquet(self.snapshots_dir / "model4/train.parquet")
            self.models['adaptive_pricing'].train(train_df)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")

        print("\n" + "=" * 60)
        print("‚úÖ ALL MODELS TRAINED SUCCESSFULLY!")
        print("=" * 60)

    def make_predictions(self):
        """–î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏"""

        print("\nüéØ MAKING PREDICTIONS WITH ALL MODELS")
        print("=" * 60)

        predictions = {}

        # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print("\nüìä Example predictions:")

        # 1. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        try:
            test_df = pd.read_parquet(self.snapshots_dir / "model1/test.parquet")
            if len(test_df) > 0:
                sample_user = test_df.iloc[0]
                user_features = pd.DataFrame([sample_user])

                # –£–±–∏—Ä–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
                for col in user_features.columns:
                    if 'target' in col:
                        user_features[col] = 0

                context_pred = self.models['context_aware'].predict(user_features)
                print(f"\n1Ô∏è‚É£ User {sample_user.get('user_id', 'unknown')}:")
                print(f"   Purchase probability: {context_pred.get('purchase_probability', [0])[0]:.1%}")

                if 'predicted_category' in context_pred:
                    print(f"   Likely category: {context_pred['predicted_category'][0]}")

                predictions['context_aware'] = context_pred
        except Exception as e:
            print(f"   ‚ö†Ô∏è Context prediction error: {e}")

        # 2. –ü—Ä–æ–≥–Ω–æ–∑ —Å–ø—Ä–æ—Å–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞
        try:
            test_df = pd.read_parquet(self.snapshots_dir / "model2/test.parquet")
            if len(test_df) > 0:
                region_sample = test_df.iloc[0:1]
                region_pred = self.models['cross_region'].predict(region_sample)
                print(f"\n2Ô∏è‚É£ Region {region_pred.get('region', 'unknown')}:")
                print(f"   Predicted demand: {region_pred.get('predicted_demand', 0):.0f} purchases")
                predictions['cross_region'] = region_pred
        except Exception as e:
            print(f"   ‚ö†Ô∏è Region prediction error: {e}")

        # 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ü–µ–Ω—ã
        try:
            test_df = pd.read_parquet(self.snapshots_dir / "model4/test.parquet")
            if len(test_df) > 0:
                item_sample = test_df.iloc[0]
                price_rec = self.models['adaptive_pricing'].recommend_price(
                    item_id=item_sample.get('item_id', 'item_1'),
                    current_price=item_sample.get('current_price', 100),
                    context={'competition_pressure': 0.95}
                )
                print(f"\n4Ô∏è‚É£ Item {price_rec.get('item_id', 'unknown')}:")
                print(f"   Current price: {price_rec.get('current_price', 0):.2f}")
                print(f"   Recommended: {price_rec.get('recommended_price', 0):.2f}")
                print(f"   Change: {price_rec.get('change_percent', 0):.1f}%")
                predictions['adaptive_pricing'] = price_rec
        except Exception as e:
            print(f"   ‚ö†Ô∏è Price prediction error: {e}")

        return predictions


# ===== –ó–ê–ü–£–°–ö =====

if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = InnovationPipeline()

    # 1. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å)
    pipeline.train_all_models()

    # 2. –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã
    predictions = pipeline.make_predictions()

    print("\n" + "=" * 60)
    print("üéâ INNOVATION PIPELINE COMPLETED!")
    print("=" * 60)

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    import json

    with open("innovative_predictions.json", "w") as f:
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ —Ç–∏–ø—ã
        simple_preds = {}
        for model_name, pred in predictions.items():
            if isinstance(pred, dict):
                simple_preds[model_name] = {
                    k: (float(v) if isinstance(v, (np.floating, float)) else
                        int(v) if isinstance(v, (np.integer, int)) else
                        str(v) if not isinstance(v, (list, dict, np.ndarray)) else
                        v.tolist() if isinstance(v, np.ndarray) else
                        list(v) if isinstance(v, (list, tuple)) else str(v))
                    for k, v in pred.items()
                }

        json.dump(simple_preds, f, indent=2)

    print("üìÅ Predictions saved to innovative_predictions.json")