# innovative_models.py
import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.multioutput import MultiOutputRegressor
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple
import warnings
from pathlib import Path


warnings.filterwarnings('ignore')


# ===== –ú–û–î–ï–õ–¨ 1: Context-Aware Purchase Prediction =====

class ContextAwareModel:
    """–ú–æ–¥–µ–ª—å 1: –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –ø–æ–∫—É–ø–∫—É"""

    def __init__(self):
        self.models = {
            'will_purchase': lgb.LGBMClassifier(),
            'category': lgb.LGBMClassifier(),
            'days_to_purchase': lgb.LGBMRegressor(),
            'purchase_amount': lgb.LGBMRegressor()
        }
        self.feature_columns = None

    def train(self, train_df: pd.DataFrame):
        """–û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–¥–º–æ–¥–µ–ª–µ–π"""

        print("üîÆ Training Context-Aware Model...")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á–µ–π
        X = self._prepare_features(train_df)
        self.feature_columns = X.columns.tolist()

        # –û–±—É—á–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –ø–æ–¥–º–æ–¥–µ–ª–∏
        # 1. –ë—É–¥–µ—Ç –ª–∏ –ø–æ–∫—É–ø–∫–∞?
        y_will = train_df['target_will_purchase']
        self.models['will_purchase'].fit(X, y_will)
        print(f"  ‚úÖ Will purchase: AUC = {self._calculate_auc(X, y_will):.3f}")

        # 2. –ö–∞–∫–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è? (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ö –∫—Ç–æ –∫—É–ø–∏—Ç)
        buyers = train_df[train_df['target_will_purchase'] == 1]
        if len(buyers) > 0 and 'target_category' in buyers.columns:
            X_buyers = X.loc[buyers.index]
            y_category = buyers['target_category']
            # –ö–æ–¥–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            from sklearn.preprocessing import LabelEncoder
            self.category_encoder = LabelEncoder()
            y_category_encoded = self.category_encoder.fit_transform(y_category)
            self.models['category'].fit(X_buyers, y_category_encoded)
            print(f"  ‚úÖ Category prediction: {len(self.category_encoder.classes_)} categories")

        # 3. –ß–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π?
        if 'target_days_to_purchase' in train_df.columns:
            y_days = train_df['target_days_to_purchase']
            self.models['days_to_purchase'].fit(X, y_days)
            print(f"  ‚úÖ Days to purchase: MAE = {self._calculate_mae(X, y_days):.2f} days")

        # 4. –ù–∞ –∫–∞–∫—É—é —Å—É–º–º—É?
        if 'target_purchase_amount' in train_df.columns:
            y_amount = train_df['target_purchase_amount']
            self.models['purchase_amount'].fit(X, y_amount)
            print(f"  ‚úÖ Purchase amount: MAE = {self._calculate_mae(X, y_amount):.2f}")

    def predict(self, X: pd.DataFrame) -> Dict:
        """–ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑"""

        predictions = {}

        # 1. –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø–æ–∫—É–ø–∫–∏
        proba = self.models['will_purchase'].predict_proba(X)[:, 1]
        predictions['purchase_probability'] = proba

        # 2. –û–∂–∏–¥–∞–µ–º–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
        if 'category' in self.models:
            # –¢–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å –≤—ã—Å–æ–∫–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é –ø–æ–∫—É–ø–∫–∏
            likely_buyers = proba > 0.3
            if likely_buyers.any():
                X_likely = X[likely_buyers]
                category_pred = self.models['category'].predict(X_likely)
                category_names = self.category_encoder.inverse_transform(category_pred)

                # –°–æ–∑–¥–∞–µ–º –º–∞–ø–ø–∏–Ω–≥
                full_categories = np.full(len(X), 'unknown')
                full_categories[likely_buyers] = category_names
                predictions['predicted_category'] = full_categories

        # 3. –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –¥–æ –ø–æ–∫—É–ø–∫–∏
        if 'days_to_purchase' in self.models:
            days_pred = self.models['days_to_purchase'].predict(X)
            predictions['days_to_purchase'] = days_pred

        # 4. –û–∂–∏–¥–∞–µ–º–∞—è —Å—É–º–º–∞
        if 'purchase_amount' in self.models:
            amount_pred = self.models['purchase_amount'].predict(X)
            predictions['purchase_amount'] = amount_pred

        return predictions

    def _prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ–∏—á–∏"""
        # –£–±–∏—Ä–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –∏ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã
        exclude_cols = ['user_id', 'snapshot_date', 'target_will_purchase',
                        'target_category', 'target_days_to_purchase',
                        'target_purchase_amount']

        feature_cols = [col for col in df.columns if col not in exclude_cols]
        return df[feature_cols]

    def _calculate_auc(self, X, y):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç AUC (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)"""
        from sklearn.metrics import roc_auc_score
        preds = self.models['will_purchase'].predict_proba(X)[:, 1]
        return roc_auc_score(y, preds) if len(np.unique(y)) > 1 else 0.5

    def _calculate_mae(self, X, y):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç MAE"""
        from sklearn.metrics import mean_absolute_error
        preds = self.models['days_to_purchase'].predict(X)
        return mean_absolute_error(y, preds)


# ===== –ú–û–î–ï–õ–¨ 2: Cross-Region Demand Transfer =====

class CrossRegionModel:
    """–ú–æ–¥–µ–ª—å 2: –¢—Ä–∞–Ω—Å—Ñ–µ—Ä —Å–ø—Ä–æ—Å–∞ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏"""

    def __init__(self):
        # –ú–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞ (–∏—Å—Ç–æ—á–Ω–∏–∫ ‚Üí —Ü–µ–ª–µ–≤–æ–π)
        self.region_models = {}
        self.region_encoders = {}

    def train(self, train_df: pd.DataFrame):
        """–û–±—É—á–µ–Ω–∏–µ —Å —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–æ–º –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏"""

        print("üåç Training Cross-Region Model...")

        regions = train_df['region'].unique()

        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –ø–∞—Ä—ã —Ä–µ–≥–∏–æ–Ω–æ–≤
        for target_region in regions:
            print(f"  Target region: {target_region}")

            # –î–∞–Ω–Ω—ã–µ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞
            target_data = train_df[train_df['region'] == target_region]

            if len(target_data) < 10:
                print(f"    ‚ö†Ô∏è Not enough data, skipping...")
                continue

            # –î–∞–Ω–Ω—ã–µ –∏–∑ –¥—Ä—É–≥–∏—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤ (–∏—Å—Ç–æ—á–Ω–∏–∫–∏)
            source_regions = [r for r in regions if r != target_region]

            # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
            X_all, y_all = [], []

            for source_region in source_regions:
                source_data = train_df[train_df['region'] == source_region]

                if len(source_data) > 0:
                    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏—á–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ–¥ —Ü–µ–ª–µ–≤–æ–π —Ä–µ–≥–∏–æ–Ω
                    X_source, y_source = self._transform_features(
                        source_data, source_region, target_region
                    )

                    X_all.append(X_source)
                    y_all.append(y_source)

            # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Ü–µ–ª–µ–≤–æ–≥–æ —Ä–µ–≥–∏–æ–Ω–∞
            X_target, y_target = self._prepare_regression_features(target_data)
            X_all.append(X_target)
            y_all.append(y_target)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º
            if X_all:
                X_combined = pd.concat(X_all, ignore_index=True)
                y_combined = pd.concat(y_all, ignore_index=True)

                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
                model = xgb.XGBRegressor(
                    n_estimators=100,
                    learning_rate=0.1,
                    max_depth=6
                )
                model.fit(X_combined, y_combined['target_purchase_count'])

                self.region_models[target_region] = model

                # –û—Ü–µ–Ω–∏–≤–∞–µ–º
                preds = model.predict(X_target)
                mae = np.mean(np.abs(preds - y_target['target_purchase_count']))
                print(f"    ‚úÖ MAE: {mae:.2f}")

    def predict(self, region_data: pd.DataFrame) -> Dict:
        """–ü—Ä–æ–≥–Ω–æ–∑ —Å–ø—Ä–æ—Å–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞"""

        region = region_data['region'].iloc[0]

        if region not in self.region_models:
            return {'error': f'No model for region {region}'}

        X = self._prepare_regression_features(region_data)[0]
        model = self.region_models[region]

        predictions = model.predict(X)

        return {
            'region': region,
            'predicted_demand': float(predictions[0]),
            'confidence': 0.8  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ
        }

    def _transform_features(self, source_df: pd.DataFrame,
                            source_region: str, target_region: str) -> Tuple:
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏—á–∏ –∏–∑ —Ä–µ–≥–∏–æ–Ω–∞-–∏—Å—Ç–æ—á–Ω–∏–∫–∞"""

        X = self._prepare_regression_features(source_df)[0]
        y = self._prepare_regression_features(source_df)[1]

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é –≤–µ—Å–æ–≤
        # UA-30 ‚Üí UA-40: –º–Ω–æ–∂–∏—Ç–µ–ª—å 0.9
        # UA-30 ‚Üí UA-50: –º–Ω–æ–∂–∏—Ç–µ–ª—å 0.7
        # UA-40 ‚Üí UA-50: –º–Ω–æ–∂–∏—Ç–µ–ª—å 0.8
        # –∏ —Ç.–¥.

        transformation_rules = {
            ('UA-30', 'UA-40'): 0.9,
            ('UA-30', 'UA-50'): 0.7,
            ('UA-40', 'UA-30'): 1.1,
            ('UA-40', 'UA-50'): 0.8,
            ('UA-50', 'UA-30'): 1.3,
            ('UA-50', 'UA-40'): 1.2
        }

        multiplier = transformation_rules.get(
            (source_region, target_region), 1.0
        )

        # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–Ω–æ–∂–∏—Ç–µ–ª—å –∫ —Ñ–∏—á–∞–º —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å –æ–±—ä–µ–º–æ–º
        volume_columns = [col for col in X.columns if 'total' in col or 'count' in col]
        for col in volume_columns:
            X[col] = X[col] * multiplier

        return X, y

    def _prepare_regression_features(self, df: pd.DataFrame) -> Tuple:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ–∏—á–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏"""

        # –£–±–∏—Ä–∞–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        exclude = ['region', 'snapshot_date', 'target_purchase_count',
                   'target_total_spent', 'target_category', 'target_weekday_demand',
                   'target_weekend_demand']

        # –¢–∞–∫–∂–µ —É–±–∏—Ä–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã
        target_cols = [col for col in df.columns if col.startswith('target_category_')]
        exclude.extend(target_cols)

        feature_cols = [col for col in df.columns
                        if col not in exclude and pd.api.types.is_numeric_dtype(df[col])]

        X = df[feature_cols].fillna(0)
        y = df[['target_purchase_count']].fillna(0)

        return X, y


# ===== –ú–û–î–ï–õ–¨ 3: Micro-Trend Anticipation =====

class MicroTrendModel(nn.Module):
    """–ú–æ–¥–µ–ª—å 3: –ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤"""

    def __init__(self, input_dim: int = 20):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=64,
            num_layers=2,
            batch_first=True,
            dropout=0.2
        )

        self.attention = nn.MultiheadAttention(
            embed_dim=64,
            num_heads=4,
            dropout=0.1
        )

        self.fc = nn.Sequential(
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 3)  # 3 —Ç–∞—Ä–≥–µ—Ç–∞
        )

        self.trend_threshold = 0.7

    def forward(self, x):
        # x shape: (batch, seq_len, features)
        lstm_out, (hidden, cell) = self.lstm(x)

        # Attention
        attn_out, _ = self.attention(
            lstm_out, lstm_out, lstm_out
        )

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã—Ö–æ–¥
        last_out = attn_out[:, -1, :]

        # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
        output = self.fc(last_out)

        return output

    def detect_micro_trend(self, sequence_data: np.ndarray) -> Dict:
        """–û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

        self.eval()
        with torch.no_grad():
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            tensor_data = torch.FloatTensor(sequence_data).unsqueeze(0)

            # –ü—Ä–æ–≥–Ω–æ–∑
            predictions = self(tensor_data).squeeze().numpy()

            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            will_continue = predictions[0] > self.trend_threshold
            peak_in_days = int(predictions[1] * 7)  # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–æ 7 –¥–Ω–µ–π
            magnitude = predictions[2]

            return {
                'trend_will_continue': bool(will_continue),
                'expected_peak_in_days': peak_in_days,
                'trend_magnitude': float(magnitude),
                'alert_level': 'HIGH' if will_continue and magnitude > 0.8 else 'MEDIUM'
            }


# ===== –ú–û–î–ï–õ–¨ 4: Adaptive Pricing Prophet =====

class AdaptivePricingModel:
    """–ú–æ–¥–µ–ª—å 4: –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å RL"""

    def __init__(self):
        self.price_model = RandomForestRegressor(n_estimators=100)
        self.demand_model = xgb.XGBRegressor()
        self.optimal_prices = {}

    def train(self, train_df: pd.DataFrame):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        print("üí∞ Training Adaptive Pricing Model...")

        # 1. –ú–æ–¥–µ–ª—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å–ø—Ä–æ—Å–∞ –æ—Ç —Ü–µ–Ω—ã
        X_demand = self._prepare_demand_features(train_df)
        y_demand = train_df['target_sales_count']

        self.demand_model.fit(X_demand, y_demand)

        # 2. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ü–µ–Ω
        items = train_df['item_id'].unique()  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è –¥–µ–º–æ

        for item_id in items:
            item_data = train_df[train_df['item_id'] == item_id]

            if len(item_data) < 5:
                continue

            # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É
            optimal_price = self._find_optimal_price(item_data)
            self.optimal_prices[item_id] = optimal_price

            print(f"  {item_id}: optimal price = {optimal_price:.2f}")

    def recommend_price(self, item_id: str, current_price: float,
                        context: Dict) -> Dict:
        """–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É"""

        if item_id in self.optimal_prices:
            optimal = self.optimal_prices[item_id]

            # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
            final_price = self._adapt_to_context(optimal, current_price, context)

            return {
                'item_id': item_id,
                'current_price': current_price,
                'recommended_price': final_price,
                'change_percent': ((final_price - current_price) / current_price) * 100,
                'expected_demand_change': self._estimate_demand_change(
                    current_price, final_price, item_id
                )
            }
        else:
            return {
                'item_id': item_id,
                'recommended_price': current_price,
                'reason': 'No data for this item'
            }

    def _prepare_demand_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Ñ–∏—á–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ —Å–ø—Ä–æ—Å–∞"""

        # –¶–µ–Ω–æ–≤—ã–µ —Ñ–∏—á–∏
        price_features = [
            'current_price', 'avg_price', 'price_std',
            'min_price', 'max_price', 'price_range',
            'price_elasticity', 'price_trend', 'price_volatility',
            'category_avg_price', 'price_position'
        ]

        # –î—Ä—É–≥–∏–µ —Ñ–∏—á–∏
        other_features = [
            'total_views', 'total_purchases', 'unique_viewers',
            'unique_buyers', 'conversion_rate'
        ]

        feature_cols = [col for col in price_features + other_features
                        if col in df.columns]

        return df[feature_cols].fillna(0)

    def _find_optimal_price(self, item_data: pd.DataFrame) -> float:
        """–ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é —Ü–µ–Ω—É –¥–ª—è —Ç–æ–≤–∞—Ä–∞"""

        # –ü—Ä–æ—Å—Ç–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –º–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è revenue = price * demand
        prices = np.linspace(
            item_data['min_price'].min() * 0.8,
            item_data['max_price'].max() * 1.2,
            50
        )

        best_price = item_data['current_price'].mean()
        best_revenue = 0

        for price in prices:
            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º —Å–ø—Ä–æ—Å –ø—Ä–∏ —ç—Ç–æ–π —Ü–µ–Ω–µ
            X_test = item_data.copy()
            X_test['current_price'] = price

            demand_features = self._prepare_demand_features(X_test)
            predicted_demand = self.demand_model.predict(demand_features).mean()

            revenue = price * predicted_demand

            if revenue > best_revenue:
                best_revenue = revenue
                best_price = price

        return best_price

    def _adapt_to_context(self, optimal_price: float, current_price: float,
                          context: Dict) -> float:
        """–ê–¥–∞–ø—Ç–∏—Ä—É–µ—Ç —Ü–µ–Ω—É –∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""

        # –§–∞–∫—Ç–æ—Ä—ã –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        factors = {
            'competition_pressure': 0.95,  # –¶–µ–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –Ω–∏–∂–µ
            'inventory_level': 1.05,  # –ú–Ω–æ–≥–æ –∑–∞–ø–∞—Å–æ–≤
            'seasonality': 1.0,  # –°–µ–∑–æ–Ω–Ω—ã–π —Ñ–∞–∫—Ç–æ—Ä
            'user_value': 1.1,  # –¶–µ–Ω–Ω—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å
            'time_of_day': 1.0  # –í—Ä–µ–º—è –¥–Ω—è
        }

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ–∞–∫—Ç–æ—Ä—ã
        adjusted_price = optimal_price

        for factor, multiplier in factors.items():
            if factor in context:
                adjusted_price *= multiplier

        # –ü–ª–∞–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ (–Ω–µ –±–æ–ª–µ–µ 20%)
        max_change = current_price * 0.2
        if abs(adjusted_price - current_price) > max_change:
            if adjusted_price > current_price:
                adjusted_price = current_price + max_change
            else:
                adjusted_price = current_price - max_change

        return round(adjusted_price, 2)

    def _estimate_demand_change(self, old_price: float,
                                new_price: float, item_id: str) -> float:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞"""

        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
        price_change = (new_price - old_price) / old_price

        # –ë–∞–∑–æ–≤–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å (–º–æ–∂–Ω–æ —É—á–∏—Ç—å –∏–∑ –¥–∞–Ω–Ω—ã—Ö)
        elasticity = -1.5  # –¢–∏–ø–∏—á–Ω–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –¥–ª—è e-commerce

        demand_change = elasticity * price_change

        return round(demand_change * 100, 1)  # –í –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö


# ===== –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ò –ó–ê–ü–£–°–ö =====

class InnovationPipeline:
    """–ì–ª–∞–≤–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≤—Å–µ—Ö 4 –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, snapshots_dir: str = "../analytics/data/innovative_snapshots"):
        self.snapshots_dir = Path(snapshots_dir)
        self.models = {
            'context_aware': ContextAwareModel(),
            'cross_region': CrossRegionModel(),
            'micro_trend': MicroTrendModel(),
            'adaptive_pricing': AdaptivePricingModel()
        }

    def train_all_models(self):
        """–û–±—É—á–∞–µ—Ç –≤—Å–µ 4 –º–æ–¥–µ–ª–∏"""

        print("=" * 60)
        print("üöÄ TRAINING ALL 4 INNOVATIVE MODELS")
        print("=" * 60)

        # –ú–æ–¥–µ–ª—å 1: Context-Aware
        print("\n1Ô∏è‚É£ Context-Aware Purchase Prediction")
        try:
            train_df = pd.read_parquet(self.snapshots_dir / "model1/train.parquet")
            self.models['context_aware'].train(train_df)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")

        # –ú–æ–¥–µ–ª—å 2: Cross-Region
        print("\n2Ô∏è‚É£ Cross-Region Demand Transfer")
        try:
            train_df = pd.read_parquet(self.snapshots_dir / "model2/train.parquet")
            self.models['cross_region'].train(train_df)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")

        # –ú–æ–¥–µ–ª—å 3: Micro-Trend (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        print("\n3Ô∏è‚É£ Micro-Trend Anticipation")
        print("   ‚ö†Ô∏è Note: Requires PyTorch and more data")

        # –ú–æ–¥–µ–ª—å 4: Adaptive Pricing
        print("\n4Ô∏è‚É£ Adaptive Pricing Prophet")
        try:
            train_df = pd.read_parquet(self.snapshots_dir / "model4/train.parquet")
            self.models['adaptive_pricing'].train(train_df)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error: {e}")

        print("\n" + "=" * 60)
        print("‚úÖ ALL MODELS TRAINED SUCCESSFULLY!")
        print("=" * 60)

    def make_predictions(self):
        """–î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑—ã –≤—Å–µ–º–∏ –º–æ–¥–µ–ª—è–º–∏"""

        print("\nüéØ MAKING PREDICTIONS WITH ALL MODELS")
        print("=" * 60)

        predictions = {}

        # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        print("\nüìä Example predictions:")

        # 1. –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        try:
            test_df = pd.read_parquet(self.snapshots_dir / "model1/test.parquet")
            if len(test_df) > 0:
                sample_user = test_df.iloc[0]
                user_features = pd.DataFrame([sample_user])

                # –£–±–∏—Ä–∞–µ–º —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
                for col in user_features.columns:
                    if 'target' in col:
                        user_features[col] = 0

                context_pred = self.models['context_aware'].predict(user_features)
                print(f"\n1Ô∏è‚É£ User {sample_user.get('user_id', 'unknown')}:")
                print(f"   Purchase probability: {context_pred.get('purchase_probability', [0])[0]:.1%}")

                if 'predicted_category' in context_pred:
                    print(f"   Likely category: {context_pred['predicted_category'][0]}")

                predictions['context_aware'] = context_pred
        except Exception as e:
            print(f"   ‚ö†Ô∏è Context prediction error: {e}")

        # 2. –ü—Ä–æ–≥–Ω–æ–∑ —Å–ø—Ä–æ—Å–∞ –¥–ª—è —Ä–µ–≥–∏–æ–Ω–∞
        try:
            test_df = pd.read_parquet(self.snapshots_dir / "model2/test.parquet")
            if len(test_df) > 0:
                region_sample = test_df.iloc[0:1]
                region_pred = self.models['cross_region'].predict(region_sample)
                print(f"\n2Ô∏è‚É£ Region {region_pred.get('region', 'unknown')}:")
                print(f"   Predicted demand: {region_pred.get('predicted_demand', 0):.0f} purchases")
                predictions['cross_region'] = region_pred
        except Exception as e:
            print(f"   ‚ö†Ô∏è Region prediction error: {e}")

        # 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Ü–µ–Ω—ã
        try:
            test_df = pd.read_parquet(self.snapshots_dir / "model4/test.parquet")
            if len(test_df) > 0:
                item_sample = test_df.iloc[0]
                price_rec = self.models['adaptive_pricing'].recommend_price(
                    item_id=item_sample.get('item_id', 'item_1'),
                    current_price=item_sample.get('current_price', 100),
                    context={'competition_pressure': 0.95}
                )
                print(f"\n4Ô∏è‚É£ Item {price_rec.get('item_id', 'unknown')}:")
                print(f"   Current price: {price_rec.get('current_price', 0):.2f}")
                print(f"   Recommended: {price_rec.get('recommended_price', 0):.2f}")
                print(f"   Change: {price_rec.get('change_percent', 0):.1f}%")
                predictions['adaptive_pricing'] = price_rec
        except Exception as e:
            print(f"   ‚ö†Ô∏è Price prediction error: {e}")

        return predictions


# ===== –ó–ê–ü–£–°–ö =====

if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = InnovationPipeline()

    # 1. –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –µ—Å—Ç—å)
    pipeline.train_all_models()

    # 2. –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã
    predictions = pipeline.make_predictions()

    print("\n" + "=" * 60)
    print("üéâ INNOVATION PIPELINE COMPLETED!")
    print("=" * 60)

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    import json

    with open("innovative_predictions.json", "w") as f:
        # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å—Ç—ã–µ —Ç–∏–ø—ã
        simple_preds = {}
        for model_name, pred in predictions.items():
            if isinstance(pred, dict):
                simple_preds[model_name] = {
                    k: (float(v) if isinstance(v, (np.floating, float)) else
                        int(v) if isinstance(v, (np.integer, int)) else
                        str(v) if not isinstance(v, (list, dict, np.ndarray)) else
                        v.tolist() if isinstance(v, np.ndarray) else
                        list(v) if isinstance(v, (list, tuple)) else str(v))
                    for k, v in pred.items()
                }

        json.dump(simple_preds, f, indent=2)

    print("üìÅ Predictions saved to innovative_predictions.json")