# train_snapshot_model1.py
import os
import pandas as pd

from src.models import ContextAwareModel

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SNAPSHOT_DIR = os.path.join(BASE_DIR, "..", "..", "analytics", "data", "snapshots", "model1")
MODEL_PATH = os.path.join(BASE_DIR, "..", "production_models", "context_aware_model1.pkl")

def load_dataset(name: str) -> pd.DataFrame:
    path = os.path.join(SNAPSHOT_DIR, f"{name}.parquet")
    if not os.path.exists(path):
        raise FileNotFoundError(path)
    return pd.read_parquet(path)


def main():
    print("ğŸ“‚ Loading datasets...")
    train_df = load_dataset("train")
    val_df = load_dataset("val")
    test_df = load_dataset("test")

    print(f"Train: {len(train_df):,} rows")
    print(f"Val:   {len(val_df):,} rows")
    print(f"Test:  {len(test_df):,} rows")

    # ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ
    model = ContextAwareModel()

    print("\nğŸš€ Training model...")
    val_metrics = model.fit(train_df, val_df=val_df)

    print("\nğŸ“Š Validation metrics:")
    for k, v in val_metrics.items():
        print(f"  {k}: {v:.4f}" if isinstance(v, (int, float)) else f"  {k}: {v}")

    # ĞÑ†ĞµĞ½ĞºĞ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğµ
    print("\nğŸ§ª Test metrics:")
    test_metrics = model.evaluate(test_df)
    for k, v in test_metrics.items():
        print(f"  {k}: {v:.4f}" if isinstance(v, (int, float)) else f"  {k}: {v}")

    print("\nğŸ“ˆ Top 10 important features:")
    fi = model.get_feature_importance(top_n=10)
    if fi is not None:
        for _, row in fi.iterrows():
            print(f"  {row['feature']}: {row['importance']:.1f}")

    # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    print(f"\nğŸ’¾ Saving model to {MODEL_PATH}")
    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
    model.save(MODEL_PATH)

    # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
    print("\nğŸ”® Example predictions on first 5 test rows:")
    preds = model.predict(test_df.head(5))
    print(preds)


if __name__ == "__main__":
    main()
