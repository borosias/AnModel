# innovative_snapshot_builder.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Dict, List, Tuple
import warnings

warnings.filterwarnings('ignore')


class InnovativeSnapshotBuilder:
    """–°–æ–∑–¥–∞–µ—Ç —Å–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è 4 –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, parquet_dir: str = "../analytics/data/parquet"):
        self.parquet_dir = Path(parquet_dir)
        self.google_trends = self._load_google_trends()

    def _load_google_trends(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º Google Trends –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)"""
        trends_file = Path("trends_data/trends_master.parquet")
        if trends_file.exists():
            return pd.read_parquet(trends_file)
        return pd.DataFrame()

    def build_all_snapshots(self,
                            train_end: str,
                            val_end: str,
                            test_end: str,
                            window_back_days: int = 90,  # –£–≤–µ–ª–∏—á–∏–ª–∏ –¥–ª—è –ª—É—á—à–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                            window_forward_days: int = 14):  # 14 –¥–Ω–µ–π –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤

        print("üöÄ Building innovative snapshots for 4 models...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–±—ã—Ç–∏—è
        events = self._load_and_prepare_events()

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞—Ç—ã —Å–Ω–∞–ø—à–æ—Ç–æ–≤ (–µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤)
        snapshot_dates = self._generate_snapshot_dates(events, window_back_days, window_forward_days)

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test
        train_dates, val_dates, test_dates = self._split_dates(
            snapshot_dates, train_end, val_end, test_end
        )

        print(f"üìä Total snapshots: {len(snapshot_dates)}")
        print(f"  Train: {len(train_dates)}, Val: {len(val_dates)}, Test: {len(test_dates)}")

        # –°—Ç—Ä–æ–∏–º —Å–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        snapshots = {}

        # –ú–æ–¥–µ–ª—å 1: Context-Aware Purchase Prediction
        snapshots['model1'] = self._build_model1_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        # –ú–æ–¥–µ–ª—å 2: Cross-Region Demand Transfer
        snapshots['model2'] = self._build_model2_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        # –ú–æ–¥–µ–ª—å 3: Micro-Trend Anticipation
        snapshots['model3'] = self._build_model3_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        # –ú–æ–¥–µ–ª—å 4: Adaptive Pricing
        snapshots['model4'] = self._build_model4_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        return snapshots

    def _load_and_prepare_events(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –≥–æ—Ç–æ–≤–∏–º —Å–æ–±—ã—Ç–∏—è"""
        parquet_files = list(self.parquet_dir.glob("events_part_*.parquet"))

        if not parquet_files:
            raise ValueError("No parquet files found!")

        print(f"üìÇ Loading {len(parquet_files)} parquet files...")

        # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤—ã—Ö 5 —Ñ–∞–π–ª–æ–≤ (–¥–ª—è –¥–µ–º–æ)
        dfs = []
        for file in parquet_files:
            df = pd.read_parquet(file)
            df['ts'] = pd.to_datetime(df['ts'])

            # –ü–∞—Ä—Å–∏–º properties
            df = self._parse_properties(df)

            dfs.append(df)

        events = pd.concat(dfs, ignore_index=True).sort_values('ts')

        print(f"‚úÖ Loaded {len(events):,} events from {events['ts'].min()} to {events['ts'].max()}")

        return events

    def _parse_properties(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–∞—Ä—Å–∏–º JSON —Å–≤–æ–π—Å—Ç–≤–∞"""
        import json

        def parse_json(x):
            if isinstance(x, str):
                try:
                    return json.loads(x.replace("'", '"'))
                except:
                    return {}
            return x if isinstance(x, dict) else {}

        df['properties_dict'] = df['properties'].apply(parse_json)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ –ø–æ–ª—è
        df['search_query'] = df['properties_dict'].apply(lambda x: x.get('search_query', ''))
        df['category'] = df['properties_dict'].apply(lambda x: x.get('category', ''))
        df['device'] = df['properties_dict'].apply(lambda x: x.get('device', 'desktop'))
        df['price_from_props'] = df['properties_dict'].apply(lambda x: float(x.get('price', 0)))

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ü–µ–Ω—É
        df['price'] = df.apply(
            lambda row: row['price'] if pd.notna(row['price']) else row['price_from_props'],
            axis=1
        )

        return df

    def _generate_snapshot_dates(self, events: pd.DataFrame,
                                 window_back: int, window_forward: int) -> List[datetime]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞—Ç—ã —Å–Ω–∞–ø—à–æ—Ç–æ–≤ (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤)"""
        min_date = events['ts'].min().floor('D') + timedelta(days=window_back)
        max_date = events['ts'].max().ceil('D') - timedelta(days=window_forward)

        return pd.date_range(start=min_date, end=max_date, freq='1D')

    def _split_dates(self, dates: List[datetime],
                     train_end: str, val_end: str, test_end: str) -> Tuple:
        """–†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç—ã –Ω–∞ train/val/test"""
        train_end_dt = pd.to_datetime(train_end)
        val_end_dt = pd.to_datetime(val_end)
        test_end_dt = pd.to_datetime(test_end)

        train = [d for d in dates if d <= train_end_dt]
        val = [d for d in dates if train_end_dt < d <= val_end_dt]
        test = [d for d in dates if val_end_dt < d <= test_end_dt]

        return train, val, test

    # ===== –ú–û–î–ï–õ–¨ 1: Context-Aware Purchase Prediction =====

    def _build_model1_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ö–û–ù–¢–ï–ö–°–¢–ù–û–ô –ø–æ–∫—É–ø–∫–∏"""

        print("\nüîÆ Building Model 1: Context-Aware Purchase Prediction...")

        datasets = {}

        for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
            snapshots = []

            for snapshot_date in dates:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è –¥–µ–º–æ
                # –û–∫–Ω–æ —Ñ–∏—á
                feature_start = snapshot_date - timedelta(days=window_back)
                feature_end = snapshot_date

                # –û–∫–Ω–æ —Ç–∞—Ä–≥–µ—Ç–æ–≤
                target_start = snapshot_date
                target_end = snapshot_date + timedelta(days=window_forward)

                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è
                feature_events = events[
                    (events['ts'] >= feature_start) &
                    (events['ts'] < feature_end)
                    ]

                target_events = events[
                    (events['ts'] >= target_start) &
                    (events['ts'] < target_end)
                    ]

                # –°—Ç—Ä–æ–∏–º —Ñ–∏—á–∏
                features = self._build_model1_features(feature_events, snapshot_date)

                # –°—Ç—Ä–æ–∏–º –ú–£–õ–¨–¢–ò–ó–ê–î–ê–ß–ù–´–ï —Ç–∞—Ä–≥–µ—Ç—ã
                targets = self._build_model1_targets(target_events, features.index, snapshot_date)

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                snapshot_df = features.join(targets, how='left').fillna(0)
                snapshot_df['snapshot_date'] = snapshot_date

                snapshots.append(snapshot_df.reset_index())

            if snapshots:
                datasets[dataset_name] = pd.concat(snapshots, ignore_index=True)
                print(f"  {dataset_name}: {len(datasets[dataset_name]):,} samples")
            else:
                datasets[dataset_name] = pd.DataFrame()

        return datasets

    def _build_model1_features(self, events: pd.DataFrame, snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è Model 1 —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

        if events.empty:
            return pd.DataFrame()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_features = events.groupby('user_id').agg({
            'event_id': 'count',
            'ts': ['max', 'min', 'nunique'],
            'event_type': lambda x: (x == 'purchase').sum(),
            'price': 'sum'
        })

        # Flatten columns
        user_features.columns = ['total_events', 'last_event', 'first_event',
                                 'active_days', 'total_purchases', 'total_spent']

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        user_features['days_since_first'] = (snapshot_date - user_features['first_event']).dt.days
        user_features['days_since_last'] = (snapshot_date - user_features['last_event']).dt.days
        user_features['events_per_day'] = user_features['total_events'] / (user_features['days_since_first'] + 1)

        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º—è
        user_features['snapshot_month'] = snapshot_date.month
        user_features['snapshot_day_of_week'] = snapshot_date.weekday()
        user_features['snapshot_hour'] = snapshot_date.hour

        # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        # 1. –°–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        sessions = events.groupby(['user_id', 'session_id']).agg({
            'ts': ['min', 'max', 'count'],
            'event_type': lambda x: list(x)
        })

        sessions.columns = ['session_start', 'session_end', 'session_events', 'session_event_types']

        user_sessions = sessions.groupby('user_id').agg({
            'session_events': ['mean', 'std', 'count'],
            'session_start': 'min',
            'session_end': 'max'
        })

        user_sessions.columns = ['avg_session_events', 'std_session_events', 'session_count',
                                 'first_session', 'last_session']

        user_sessions['avg_session_duration'] = (
                                                        user_sessions['last_session'] - user_sessions['first_session']
                                                ).dt.total_seconds() / 3600 / (user_sessions['session_count'] + 1)

        # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
        if 'category' in events.columns:
            category_counts = events.groupby(['user_id', 'category']).size().unstack(fill_value=0)
            category_counts = category_counts.add_prefix('category_')
            user_features = user_features.join(category_counts, how='left')

        # 3. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—É—Ç—Ä–æ/–¥–µ–Ω—å/–≤–µ—á–µ—Ä)
        events['hour'] = events['ts'].dt.hour
        time_patterns = pd.crosstab(events['user_id'], pd.cut(events['hour'],
                                                              bins=[0, 8, 16, 24],
                                                              labels=['night', 'day', 'evening']))
        time_patterns = time_patterns.add_prefix('activity_')

        # 4. –¶–µ–Ω–æ–≤–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        purchases = events[events['event_type'] == 'purchase']
        if not purchases.empty:
            price_sensitivity = purchases.groupby('user_id')['price'].agg(['mean', 'std', 'min', 'max'])
            price_sensitivity.columns = ['avg_purchase_price', 'price_std', 'min_price', 'max_price']
            price_sensitivity['price_range'] = price_sensitivity['max_price'] - price_sensitivity['min_price']
            user_features = user_features.join(price_sensitivity, how='left')

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∏—á–∏
        user_features = user_features.join(user_sessions, how='left')
        user_features = user_features.join(time_patterns, how='left')

        # –î–æ–±–∞–≤–ª—è–µ–º Google Trends –µ—Å–ª–∏ –µ—Å—Ç—å
        if not self.google_trends.empty:
            trends_features = self._add_google_trends_features(snapshot_date)
            # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ –≤—Å–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
            for col in trends_features.columns:
                user_features[col] = trends_features[col].iloc[0] if len(trends_features) > 0 else 0

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        user_features = user_features.fillna(0)

        return user_features

    def _build_model1_targets(self, target_events: pd.DataFrame,
                              user_index: pd.Index, snapshot_date: datetime) -> pd.DataFrame:
        """–ú—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è Model 1"""

        targets = pd.DataFrame(index=user_index)

        # 1. –ë–∏–Ω–∞—Ä–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: –∫—É–ø–∏—Ç –ª–∏ –≤–æ–æ–±—â–µ
        targets['target_will_purchase'] = 0

        # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–∫—É–ø–∫–∏ (–µ—Å–ª–∏ –∫—É–ø–∏—Ç)
        targets['target_category'] = ''

        # 3. –í—Ä–µ–º—è –¥–æ –ø–æ–∫—É–ø–∫–∏ (–≤ –¥–Ω—è—Ö)
        targets['target_days_to_purchase'] = 999

        # 4. –°—É–º–º–∞ –ø–æ–∫—É–ø–∫–∏
        targets['target_purchase_amount'] = 0

        if target_events.empty:
            return targets

        purchases = target_events[target_events['event_type'] == 'purchase']

        if not purchases.empty:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
            for user_id in purchases['user_id'].unique():
                user_purchases = purchases[purchases['user_id'] == user_id]

                targets.loc[user_id, 'target_will_purchase'] = 1

                # –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                if 'category' in user_purchases.columns:
                    top_category = user_purchases['category'].mode()
                    if not top_category.empty:
                        targets.loc[user_id, 'target_category'] = top_category.iloc[0]

                # –í—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–π –ø–æ–∫—É–ø–∫–∏
                first_purchase_time = user_purchases['ts'].min()
                days_to_purchase = (first_purchase_time - snapshot_date).days
                targets.loc[user_id, 'target_days_to_purchase'] = max(0, days_to_purchase)

                # –°—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫
                targets.loc[user_id, 'target_purchase_amount'] = user_purchases['price'].sum()

        return targets

    def _add_google_trends_features(self, snapshot_date: datetime) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ Google Trends"""
        if self.google_trends.empty:
            return pd.DataFrame()

        # –ë–µ—Ä–µ–º —Ç—Ä–µ–Ω–¥—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
        start_date = snapshot_date - timedelta(days=7)

        recent_trends = self.google_trends[
            (self.google_trends['date'] >= start_date.date()) &
            (self.google_trends['date'] <= snapshot_date.date())
            ]

        if recent_trends.empty:
            return pd.DataFrame()

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
        trends_features = {}

        for query in recent_trends['query'].unique():
            query_trends = recent_trends[recent_trends['query'] == query]

            # –°—Ä–µ–¥–Ω—è—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
            trends_features[f'trend_{query}_avg'] = query_trends['popularity'].mean()

            # –¢—Ä–µ–Ω–¥ (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ)
            if len(query_trends) > 1:
                first = query_trends.iloc[0]['popularity']
                last = query_trends.iloc[-1]['popularity']
                trends_features[f'trend_{query}_growth'] = (last - first) / (first + 1)
            else:
                trends_features[f'trend_{query}_growth'] = 0

        return pd.DataFrame([trends_features])

    # ===== –ú–û–î–ï–õ–¨ 2: Cross-Region Demand Transfer =====

    def _build_model2_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ —Å–ø—Ä–æ—Å–∞ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏"""

        print("\nüåç Building Model 2: Cross-Region Demand Transfer...")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        regions = events['region'].unique()
        print(f"  Regions: {regions}")

        datasets = {'train': [], 'val': [], 'test': []}

        for region in regions:
            region_events = events[events['region'] == region]

            for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
                region_snapshots = []

                for snapshot_date in dates:  # –û–≥—Ä–∞–Ω–∏—á–∏–º
                    feature_start = snapshot_date - timedelta(days=window_back)
                    feature_end = snapshot_date
                    target_start = snapshot_date
                    target_end = snapshot_date + timedelta(days=window_forward)

                    feature_events = region_events[
                        (region_events['ts'] >= feature_start) &
                        (region_events['ts'] < feature_end)
                        ]

                    target_events = region_events[
                        (region_events['ts'] >= target_start) &
                        (region_events['ts'] < target_end)
                        ]

                    # –§–∏—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–∞
                    features = self._build_model2_features(feature_events, snapshot_date, region)
                    targets = self._build_model2_targets(target_events, snapshot_date)

                    snapshot_df = pd.concat([features, targets], axis=1)
                    snapshot_df['snapshot_date'] = snapshot_date
                    snapshot_df['region'] = region

                    region_snapshots.append(snapshot_df.reset_index(drop=True))

                if region_snapshots:
                    region_df = pd.concat(region_snapshots, ignore_index=True)
                    datasets[dataset_name].append(region_df)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã
        result = {}
        for dataset_name in ['train', 'val', 'test']:
            if datasets[dataset_name]:
                result[dataset_name] = pd.concat(datasets[dataset_name], ignore_index=True)
                print(f"  {dataset_name}: {len(result[dataset_name]):,} region-snapshots")
            else:
                result[dataset_name] = pd.DataFrame()

        return result

    def _build_model2_features(self, events: pd.DataFrame,
                               snapshot_date: datetime, region: str) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è Model 2 (—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ)"""

        features = {}

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥–∏–æ–Ω–∞
        features['region_total_events'] = len(events)
        features['region_unique_users'] = events['user_id'].nunique()
        features['region_purchase_count'] = (events['event_type'] == 'purchase').sum()
        features['region_total_spent'] = events[events['event_type'] == 'purchase']['price'].sum()

        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if len(events) > 0:
            events['hour'] = events['ts'].dt.hour
            morning_events = ((events['hour'] >= 6) & (events['hour'] < 12)).sum()
            evening_events = ((events['hour'] >= 18) & (events['hour'] < 24)).sum()
            features['region_morning_ratio'] = morning_events / len(events)
            features['region_evening_ratio'] = evening_events / len(events)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        if 'category' in events.columns:
            top_categories = events['category'].value_counts().head(3)
            for i, (cat, count) in enumerate(top_categories.items()):
                features[f'region_top_category_{i + 1}'] = cat
                features[f'region_top_category_{i + 1}_count'] = count

        # –¶–µ–Ω–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        purchase_events = events[events['event_type'] == 'purchase']
        if len(purchase_events) > 0:
            features['region_avg_price'] = purchase_events['price'].mean()
            features['region_price_std'] = purchase_events['price'].std()

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        features['snapshot_month'] = snapshot_date.month
        features['snapshot_week'] = snapshot_date.isocalendar().week
        features['is_weekend'] = snapshot_date.weekday() >= 5

        # –°–æ—Å–µ–¥–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω—ã (–¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞)
        if region == 'UA-30':
            features['neighbor_UA40_proximity'] = 1.0
            features['neighbor_UA50_proximity'] = 0.7
        elif region == 'UA-40':
            features['neighbor_UA30_proximity'] = 1.0
            features['neighbor_UA50_proximity'] = 0.8
        else:  # UA-50
            features['neighbor_UA30_proximity'] = 0.7
            features['neighbor_UA40_proximity'] = 0.8

        return pd.DataFrame([features])

    def _build_model2_targets(self, target_events: pd.DataFrame,
                              snapshot_date: datetime) -> pd.DataFrame:
        """–¢–∞—Ä–≥–µ—Ç—ã –¥–ª—è Model 2 (—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø—Ä–æ—Å)"""

        targets = {}

        # –û–±—â–∏–π —Å–ø—Ä–æ—Å
        purchases = target_events[target_events['event_type'] == 'purchase']
        targets['target_purchase_count'] = len(purchases)
        targets['target_total_spent'] = purchases['price'].sum() if not purchases.empty else 0

        # –°–ø—Ä–æ—Å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (top 3)
        if 'category' in purchases.columns and not purchases.empty:
            top_categories = purchases['category'].value_counts().head(3)
            for i, (cat, count) in enumerate(top_categories.items()):
                targets[f'target_category_{cat}_demand'] = count

        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞
        if not purchases.empty:
            purchases['day_of_week'] = purchases['ts'].dt.dayofweek
            weekday_demand = purchases[purchases['day_of_week'] < 5]['price'].sum()
            weekend_demand = purchases[purchases['day_of_week'] >= 5]['price'].sum()
            targets['target_weekday_demand'] = weekday_demand
            targets['target_weekend_demand'] = weekend_demand

        return pd.DataFrame([targets])

    # ===== –ú–û–î–ï–õ–¨ 3: Micro-Trend Anticipation =====

    def _build_model3_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤"""

        print("\nüìà Building Model 3: Micro-Trend Anticipation...")

        # –§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞—Ö
        trend_events = events[events['event_type'].isin(['search', 'product_view'])]

        datasets = {}

        for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
            snapshots = []

            for snapshot_date in dates:  # –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ —Ç–æ—á–µ–∫ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤
                # –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤ (7 –¥–Ω–µ–π –Ω–∞–∑–∞–¥)
                feature_start = snapshot_date - timedelta(days=7)
                feature_end = snapshot_date

                # –ë—É–¥—É—â–∏–π —Ç—Ä–µ–Ω–¥ (—Å–ª–µ–¥—É—é—â–∏–µ 3-7 –¥–Ω–µ–π)
                target_start = snapshot_date
                target_end = snapshot_date + timedelta(days=7)

                feature_events = trend_events[
                    (trend_events['ts'] >= feature_start) &
                    (trend_events['ts'] < feature_end)
                    ]

                target_events = trend_events[
                    (trend_events['ts'] >= target_start) &
                    (trend_events['ts'] < target_end)
                    ]

                # –§–∏—á–∏ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤/—Ç–æ–≤–∞—Ä–æ–≤
                features = self._build_model3_features(feature_events, snapshot_date)
                targets = self._build_model3_targets(target_events, features.index)

                if not features.empty and not targets.empty:
                    snapshot_df = features.join(targets, how='left').fillna(0)
                    snapshot_df['snapshot_date'] = snapshot_date
                    snapshots.append(snapshot_df.reset_index())

            if snapshots:
                datasets[dataset_name] = pd.concat(snapshots, ignore_index=True)
                print(f"  {dataset_name}: {len(datasets[dataset_name]):,} trend-snapshots")
            else:
                datasets[dataset_name] = pd.DataFrame()

        return datasets

    def _build_model3_features(self, events: pd.DataFrame,
                               snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤"""

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        search_events = events[events['event_type'] == 'search']

        if search_events.empty or 'search_query' not in search_events.columns:
            return pd.DataFrame()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
        query_features = []

        for query in search_events['search_query'].unique():
            if not query or query == '':
                continue

            query_events = search_events[search_events['search_query'] == query]

            features = {
                'query': query,
                'total_searches': len(query_events),
                'unique_users': query_events['user_id'].nunique(),
                'first_seen': query_events['ts'].min(),
                'last_seen': query_events['ts'].max(),
                'search_frequency': len(query_events) / 7  # –Ω–∞ –¥–µ–Ω—å
            }

            # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ)
            query_events['date'] = query_events['ts'].dt.date
            daily_counts = query_events.groupby('date').size()

            if len(daily_counts) > 1:
                features['trend_growth_rate'] = (daily_counts.iloc[-1] - daily_counts.iloc[0]) / (
                            daily_counts.iloc[0] + 1)
                features['trend_acceleration'] = self._calculate_acceleration(daily_counts.values)
            else:
                features['trend_growth_rate'] = 0
                features['trend_acceleration'] = 0

            # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
            regions = query_events['region'].unique()
            features['region_spread'] = len(regions)
            features['is_multiregion'] = len(regions) > 1

            query_features.append(features)

        if not query_features:
            return pd.DataFrame()

        df = pd.DataFrame(query_features)
        df.set_index('query', inplace=True)

        # –î–æ–±–∞–≤–ª—è–µ–º Google Trends –µ—Å–ª–∏ –µ—Å—Ç—å
        if not self.google_trends.empty:
            df = self._enrich_with_google_trends(df, snapshot_date)

        return df

    def _calculate_acceleration(self, values):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞"""
        if len(values) < 3:
            return 0

        # –ü—Ä–æ—Å—Ç–∞—è –≤—Ç–æ—Ä–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–∞—è
        return (values[-1] - 2 * values[-2] + values[-3]) / max(values[-3], 1)

    def _enrich_with_google_trends(self, queries_df: pd.DataFrame,
                                   snapshot_date: datetime) -> pd.DataFrame:
        """–û–±–æ–≥–∞—â–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã Google Trends –¥–∞–Ω–Ω—ã–º–∏"""

        start_date = snapshot_date - timedelta(days=7)

        for query in queries_df.index:
            query_trends = self.google_trends[
                (self.google_trends['query'] == query) &
                (self.google_trends['date'] >= start_date.date()) &
                (self.google_trends['date'] <= snapshot_date.date())
                ]

            if not query_trends.empty:
                queries_df.loc[query, 'google_avg_popularity'] = query_trends['popularity'].mean()
                queries_df.loc[query, 'google_trend'] = self._calculate_trend(query_trends['popularity'].values)
            else:
                queries_df.loc[query, 'google_avg_popularity'] = 0
                queries_df.loc[query, 'google_trend'] = 0

        return queries_df

    def _calculate_trend(self, values):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ç—Ä–µ–Ω–¥ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏–π"""
        if len(values) < 2:
            return 0

        x = np.arange(len(values))
        coeffs = np.polyfit(x, values, 1)
        return coeffs[0]  # –ù–∞–∫–ª–æ–Ω –ª–∏–Ω–∏–∏

    def _build_model3_targets(self, target_events: pd.DataFrame,
                              query_index: pd.Index) -> pd.DataFrame:
        """–¢–∞—Ä–≥–µ—Ç—ã –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤"""

        targets = pd.DataFrame(index=query_index)
        targets['target_future_searches'] = 0
        targets['target_trend_continues'] = 0
        targets['target_peak_in_days'] = 999

        if target_events.empty or 'search_query' not in target_events.columns:
            return targets

        search_events = target_events[target_events['event_type'] == 'search']

        for query in query_index:
            query_events = search_events[search_events['search_query'] == query]

            if not query_events.empty:
                targets.loc[query, 'target_future_searches'] = len(query_events)
                targets.loc[query, 'target_trend_continues'] = 1

                # –ö–æ–≥–¥–∞ –ø–∏–∫? (–¥–µ–Ω—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø–æ–∏—Å–∫–æ–≤)
                daily_counts = query_events.groupby(query_events['ts'].dt.date).size()
                if len(daily_counts) > 0:
                    peak_day = daily_counts.idxmax()
                    targets.loc[query, 'target_peak_in_days'] = (peak_day - query_events['ts'].min().date()).days

        return targets

    # ===== –ú–û–î–ï–õ–¨ 4: Adaptive Pricing =====

    def _build_model4_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        print("\nüí∞ Building Model 4: Adaptive Pricing...")

        # –§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –ø–æ–∫—É–ø–∫–∞—Ö –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞—Ö
        price_events = events[events['event_type'].isin(['purchase', 'product_view'])]

        datasets = {}

        for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
            snapshots = []

            for snapshot_date in dates:
                # –û–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ü–µ–Ω–æ–≤–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                feature_start = snapshot_date - timedelta(days=window_back)
                feature_end = snapshot_date

                # –ë—É–¥—É—â–∏–µ –ø–æ–∫—É–ø–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
                target_start = snapshot_date
                target_end = snapshot_date + timedelta(days=window_forward)

                feature_events = price_events[
                    (price_events['ts'] >= feature_start) &
                    (price_events['ts'] < feature_end)
                    ]

                target_events = price_events[
                    (price_events['ts'] >= target_start) &
                    (price_events['ts'] < target_end)
                    ]

                # –§–∏—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–≤–∞—Ä–∞
                features = self._build_model4_features(feature_events, snapshot_date)
                targets = self._build_model4_targets(target_events, features.index)

                if not features.empty:
                    snapshot_df = features.join(targets, how='left').fillna(0)
                    snapshot_df['snapshot_date'] = snapshot_date
                    snapshots.append(snapshot_df.reset_index())

            if snapshots:
                datasets[dataset_name] = pd.concat(snapshots, ignore_index=True)
                print(f"  {dataset_name}: {len(datasets[dataset_name]):,} price-snapshots")
            else:
                datasets[dataset_name] = pd.DataFrame()

        return datasets

    def _build_model4_features(self, events: pd.DataFrame,
                               snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        if events.empty or 'item_id' not in events.columns:
            return pd.DataFrame()

        item_features = []

        for item_id in events['item_id'].unique():
            item_events = events[events['item_id'] == item_id]

            features = {
                'item_id': item_id,
                'total_views': (item_events['event_type'] == 'product_view').sum(),
                'total_purchases': (item_events['event_type'] == 'purchase').sum(),
                'unique_viewers': item_events[item_events['event_type'] == 'product_view']['user_id'].nunique(),
                'unique_buyers': item_events[item_events['event_type'] == 'purchase']['user_id'].nunique(),
            }

            # –ö–æ–Ω–≤–µ—Ä—Å–∏—è
            features['conversion_rate'] = (
                features['total_purchases'] / features['total_views']
                if features['total_views'] > 0 else 0
            )

            # –¶–µ–Ω–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            purchases = item_events[item_events['event_type'] == 'purchase']
            if not purchases.empty:
                features['current_price'] = purchases['price'].iloc[-1] if len(purchases) > 0 else 0
                features['avg_price'] = purchases['price'].mean()
                features['price_std'] = purchases['price'].std()
                features['min_price'] = purchases['price'].min()
                features['max_price'] = purchases['price'].max()
                features['price_range'] = features['max_price'] - features['min_price']
            else:
                # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –ø–æ–∫—É–ø–æ–∫, –±–µ—Ä–µ–º —Ü–µ–Ω—É –∏–∑ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–ª–∏ 0
                views_with_price = item_events[
                    (item_events['event_type'] == 'product_view') &
                    (item_events['price'] > 0)
                    ]
                features['current_price'] = views_with_price['price'].iloc[-1] if len(views_with_price) > 0 else 0
                features['avg_price'] = features['current_price']
                features['price_std'] = 0
                features['min_price'] = features['current_price']
                features['max_price'] = features['current_price']
                features['price_range'] = 0

            # –≠–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å —Å–ø—Ä–æ—Å–∞ (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
            if len(purchases) > 1:
                # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –≤—ã—Å–æ–∫–∏–µ –∏ –Ω–∏–∑–∫–∏–µ —Ü–µ–Ω—ã
                median_price = purchases['price'].median()
                high_price_sales = purchases[purchases['price'] > median_price].shape[0]
                low_price_sales = purchases[purchases['price'] <= median_price].shape[0]

                features['price_elasticity'] = (
                        (high_price_sales - low_price_sales) / (high_price_sales + low_price_sales + 1)
                )
            else:
                features['price_elasticity'] = 0

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ü–µ–Ω
            item_events['date'] = item_events['ts'].dt.date
            price_over_time = purchases.groupby('date')['price'].mean()

            if len(price_over_time) > 1:
                features['price_trend'] = self._calculate_trend(price_over_time.values)
                features['price_volatility'] = price_over_time.std()
            else:
                features['price_trend'] = 0
                features['price_volatility'] = 0

            # –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è —Å—Ä–µ–¥–∞ (–ø–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
            if 'category' in item_events.columns:
                category = item_events['category'].iloc[0] if not item_events['category'].empty else ''
                features['category'] = category

                # –ê–Ω–∞–ª–∏–∑ —Ü–µ–Ω –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                category_events = events[events['category'] == category]
                category_purchases = category_events[category_events['event_type'] == 'purchase']

                if not category_purchases.empty:
                    features['category_avg_price'] = category_purchases['price'].mean()
                    features['price_position'] = (
                        features['current_price'] / features['category_avg_price']
                        if features['category_avg_price'] > 0 else 1
                    )
                else:
                    features['category_avg_price'] = features['current_price']
                    features['price_position'] = 1

            item_features.append(features)

        if not item_features:
            return pd.DataFrame()

        df = pd.DataFrame(item_features)
        df.set_index('item_id', inplace=True)

        return df

    def _build_model4_targets(self, target_events: pd.DataFrame,
                              item_index: pd.Index) -> pd.DataFrame:
        """–¢–∞—Ä–≥–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        targets = pd.DataFrame(index=item_index)
        targets['target_sales_count'] = 0
        targets['target_revenue'] = 0
        targets['target_optimal_price'] = 0
        targets['target_price_change_effect'] = 0

        if target_events.empty:
            return targets

        purchases = target_events[target_events['event_type'] == 'purchase']

        for item_id in item_index:
            item_purchases = purchases[purchases['item_id'] == item_id]

            if not item_purchases.empty:
                targets.loc[item_id, 'target_sales_count'] = len(item_purchases)
                targets.loc[item_id, 'target_revenue'] = item_purchases['price'].sum()

                # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ (—Å—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª–∏ –ø–æ–∫—É–ø–∫–∏)
                targets.loc[item_id, 'target_optimal_price'] = item_purchases['price'].mean()

                # –≠—Ñ—Ñ–µ–∫—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã (–ø–æ–∫–∞ —É–ø—Ä–æ—â–µ–Ω–Ω–æ)
                # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –Ω—É–∂–Ω–æ —Å—Ä–∞–≤–Ω–∏—Ç—å —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ü–µ–Ω–∞–º–∏
                targets.loc[item_id, 'target_price_change_effect'] = 1.0

        return targets

    def save_all_snapshots(self, snapshots_dict: Dict,
                           output_dir: str = "src/analytics/data/innovative_snapshots"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Å–Ω–∞–ø—à–æ—Ç—ã"""

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for model_name, datasets in snapshots_dict.items():
            model_dir = output_path / model_name
            model_dir.mkdir(exist_ok=True)

            for dataset_name, df in datasets.items():
                if not df.empty:
                    file_path = model_dir / f"{dataset_name}.parquet"
                    df.to_parquet(file_path, index=False)

                    print(f"üíæ Saved {model_name}/{dataset_name}: {len(df):,} rows")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    meta = {
                        'model': model_name,
                        'dataset': dataset_name,
                        'rows': len(df),
                        'columns': df.columns.tolist(),  # –ü–µ—Ä–≤—ã–µ 20 –∫–æ–ª–æ–Ω–æ–∫
                        'saved_at': datetime.now().isoformat()
                    }

                    with open(model_dir / f"{dataset_name}_meta.json", 'w') as f:
                        json.dump(meta, f, indent=2)


# ===== –ó–ê–ü–£–°–ö =====

if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ INNOVATIVE SNAPSHOT BUILDER FOR 4 MODELS")
    print("=" * 60)

    builder = InnovativeSnapshotBuilder()

    # –°—Ç—Ä–æ–∏–º –≤—Å–µ —Å–Ω–∞–ø—à–æ—Ç—ã
    snapshots = builder.build_all_snapshots(
        train_end="2024-01-20",
        val_end="2024-01-27",
        test_end="2024-02-03",
        window_back_days=90,
        window_forward_days=14
    )

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    builder.save_all_snapshots(snapshots)

    print("\n" + "=" * 60)
    print("‚úÖ –í–°–ï 4 –ù–ê–ë–û–†–ê –î–ê–ù–ù–´–• –ì–û–¢–û–í–´!")
    print("=" * 60)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    for model_name, datasets in snapshots.items():
        print(f"\nüìä {model_name}:")
        for dataset_name, df in datasets.items():
            if not df.empty:
                print(f"  {dataset_name}: {len(df):,} samples, {len(df.columns)} features")