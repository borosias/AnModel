# innovative_snapshot_builder.py
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Dict, List, Tuple
import warnings
from scipy import stats
from sklearn.impute import SimpleImputer  # –≤–º–µ—Å—Ç–æ KNNImputer
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.feature_selection import VarianceThreshold
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller, grangercausalitytests
from statsmodels.tsa.statespace.sarimax import SARIMAX
from unicodedata import category

warnings.filterwarnings('ignore')


class InnovativeSnapshotBuilder:
    """–°–æ–∑–¥–∞–µ—Ç —Å–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è 4 –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""

    def __init__(self, parquet_dir: str = "../analytics/data/parquet"):
        self.parquet_dir = Path(parquet_dir)
        self.google_trends = self._load_google_trends()

    def quick_test_builders(self, test_date: str = "2025-01-01", n_samples: int = 5):
        """–ë—ã—Å—Ç—Ä–æ —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –≤—Å–µ –±–∏–ª–¥—ã —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –Ω–∞ –º–∞–ª–µ–Ω—å–∫–æ–π –≤—ã–±–æ—Ä–∫–µ"""
        print("\nüß™ QUICK TESTING ALL BUILDERS...")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        events = self._load_and_prepare_events()
        if events.empty:
            print("‚ùå No events loaded!")
            return

        # –ë–µ—Ä–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥–Ω–∞–±–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∞
        events_sample = events.head(10000)
        print(f"‚úÖ Loaded {len(events_sample):,} events for testing")

        # –¢–µ—Å—Ç–æ–≤–∞—è –¥–∞—Ç–∞
        test_dt = pd.to_datetime(test_date)

        # –í quick_test_builders –¥–æ–±–∞–≤—å—Ç–µ:
        if 'region' in events_sample.columns and not events_sample['region'].empty:
            most_common_region = events_sample['region'].mode()[0]
        else:
            most_common_region = 'UA-30'  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä –æ–∫–æ–Ω
        window_back, window_forward = self._optimize_window_sizes(events_sample)
        print(f"üìÖ Test date: {test_dt.date()}")
        print(f"üîß Windows: back={window_back}d, forward={window_forward}d")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–µ–≥–∏–æ–Ω –¥–ª—è —Ç–µ—Å—Ç–∞ Model 2 (–±–µ—Ä–µ–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π)
        most_common_region = events_sample['region'].mode()[0] if 'region' in events_sample.columns else 'UA-30'

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
        models = [
            ('model1',
             lambda ev, dt: self._build_model1_features(ev, dt),
             lambda ev, idx, dt: self._build_model1_targets(ev, idx, dt)),
            ('model2',
             lambda ev, dt: self._build_model2_features(ev, dt, most_common_region),
             lambda ev, dt: self._build_model2_targets(ev, dt)),
            ('model3',
             lambda ev, dt: self._build_model3_features(ev, dt),
             lambda ev, idx: self._build_model3_targets(ev, idx)),
            ('model4',
             lambda ev, dt: self._build_model4_features(ev, dt),
             lambda ev, idx: self._build_model4_targets(ev, idx)),
        ]

        for model_name, feature_builder, target_builder in models:
            print(f"\n{'=' * 40}")
            print(f"Testing {model_name.upper()}...")

            try:
                # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                feature_start = test_dt - timedelta(days=window_back)
                feature_end = test_dt
                target_start = test_dt
                target_end = test_dt + timedelta(days=window_forward)

                feature_events = events_sample[
                    (events_sample['ts'] >= feature_start) &
                    (events_sample['ts'] < feature_end)
                    ]

                target_events = events_sample[
                    (events_sample['ts'] >= target_start) &
                    (events_sample['ts'] < target_end)
                    ]

                print(f"  Feature events: {len(feature_events):,}")
                print(f"  Target events: {len(target_events):,}")

                # –°—Ç—Ä–æ–∏–º —Ñ–∏—á–∏
                features = feature_builder(feature_events, test_dt)
                print(f"  Features shape: {features.shape if not features.empty else 'Empty'}")

                if not features.empty:
                    # –°—Ç—Ä–æ–∏–º —Ç–∞—Ä–≥–µ—Ç—ã
                    if model_name == 'model1':
                        targets = target_builder(target_events, features.index, test_dt)
                    elif model_name == 'model2':
                        targets = target_builder(target_events, test_dt)
                    else:
                        targets = target_builder(target_events, features.index)

                    print(f"  Targets shape: {targets.shape if not targets.empty else 'Empty'}")

                    if not targets.empty:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                        if model_name == 'model2':
                            snapshot = pd.concat([features, targets], axis=1)
                        else:
                            snapshot = features.join(targets, how='left').fillna(0)

                        print(f"  ‚úÖ {model_name} SUCCESS!")
                        print(f"  Snapshot shape: {snapshot.shape}")
                        print(f"  Features: {list(features.columns)[:5]}..." if len(
                            features.columns) > 5 else f"  Features: {list(features.columns)}")
                        print(f"  Targets: {list(targets.columns)}")
                    else:
                        print(f"  ‚ö†Ô∏è {model_name}: No targets generated")
                else:
                    print(f"  ‚ö†Ô∏è {model_name}: No features generated")

            except Exception as e:
                print(f"  ‚ùå {model_name} FAILED: {str(e)}")
                import traceback
                traceback.print_exc()

        print(f"\n{'=' * 40}")
        print("‚úÖ QUICK TEST COMPLETED")
    # –î–æ–±–∞–≤—å—Ç–µ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –≤ –∫–ª–∞—Å—Å InnovativeSnapshotBuilder

    def test_model3_specific(self, test_date: str = "2025-01-01"):
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ç–æ–ª—å–∫–æ Model 3"""
        print("\nüîç TESTING MODEL 3 SPECIFICALLY...")

        events = self._load_and_prepare_events()
        if events.empty:
            print("‚ùå No events loaded!")
            return

        # –ù–µ–±–æ–ª—å—à–∞—è –≤—ã–±–æ—Ä–∫–∞
        events_sample = events.head(20000)
        test_dt = pd.to_datetime(test_date)
        window_back, window_forward = 30, 14

        print(f"Total events: {len(events_sample):,}")
        print(f"Search events: {(events_sample['event_type'] == 'search').sum():,}")
        print(f"Search queries: {events_sample[events_sample['event_type'] == 'search']['search_query'].nunique():,}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–±—ã—Ç–∏–π
        trend_events = events_sample[events_sample['event_type'].isin(['search', 'product_view'])]

        feature_start = test_dt - timedelta(days=7)  # –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤
        feature_end = test_dt

        feature_events = trend_events[
            (trend_events['ts'] >= feature_start) &
            (trend_events['ts'] < feature_end)
            ]

        print(f"\nFeature events: {len(feature_events):,}")
        print(f"Search events in features: {(feature_events['event_type'] == 'search').sum():,}")

        if 'search_query' in feature_events.columns:
            print(
                f"Non-empty search queries: {feature_events[feature_events['event_type'] == 'search']['search_query'].notna().sum():,}")

        # –¢–µ—Å—Ç features
        print("\nBuilding features...")
        features = self._build_model3_features(feature_events, test_dt)

        if features is None:
            print("‚ùå Features returned None!")
        elif features.empty:
            print("‚ö†Ô∏è Features DataFrame is empty")

            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
            search_events = feature_events[feature_events['event_type'] == 'search']
            if not search_events.empty:
                print("\nSearch query samples:")
                print(search_events['search_query'].head(10).tolist())
                print(f"\nUnique search queries: {search_events['search_query'].nunique()}")
        else:
            print(f"‚úÖ Features shape: {features.shape}")
            print(f"Sample queries: {features.index[:5].tolist()}")

    def _load_google_trends(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º Google Trends –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)"""
        trends_file = Path("trends_data/trends_master.parquet")
        if trends_file.exists():
            return pd.read_parquet(trends_file)
        return pd.DataFrame()

    def _optimize_window_sizes(self, events: pd.DataFrame, target_col: str = 'target_purchase_7d'):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã –æ–∫–æ–Ω"""

        if events.empty or target_col not in events.columns:
            return 30, 7  # –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é

        # –ê–Ω–∞–ª–∏–∑ –∞–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è feature window
        try:
            # –ë–µ—Ä–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –ø–æ–∫—É–ø–æ–∫
            purchase_dates = events[events['event_type'] == 'purchase']['ts']
            if len(purchase_dates) >= 30:
                daily_purchases = purchase_dates.groupby(purchase_dates.dt.date).size()

                # –í—ã—á–∏—Å–ª—è–µ–º ACF
                from statsmodels.tsa.stattools import acf
                acf_values = acf(daily_purchases.values, nlags=30, fft=False)

                # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–∫—É –≥–¥–µ ACF –ø–∞–¥–∞–µ—Ç –Ω–∏–∂–µ significance threshold
                threshold = 1.96 / np.sqrt(len(daily_purchases))
                significant_lags = np.where(np.abs(acf_values) > threshold)[0]

                if len(significant_lags) > 1:
                    optimal_window = min(significant_lags[-1], 90)  # –Ω–µ –±–æ–ª–µ–µ 90 –¥–Ω–µ–π
                else:
                    optimal_window = 30
            else:
                optimal_window = 30
        except:
            optimal_window = 30

        # –ê–Ω–∞–ª–∏–∑ –¥–ª—è target window
        try:
            # –°–º–æ—Ç—Ä–∏–º –Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –º–µ–∂–¥—É –ø–æ–∫—É–ø–∫–∞–º–∏
            if len(purchase_dates) >= 10:
                sorted_dates = purchase_dates.sort_values()
                time_diffs = (sorted_dates.shift(-1) - sorted_dates).dt.days.dropna()

                if len(time_diffs) > 0:
                    median_interval = time_diffs.median()
                    # –ë–µ—Ä–µ–º 75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –∏–ª–∏ 14 –¥–Ω–µ–π –º–∞–∫—Å–∏–º—É–º
                    optimal_forward = min(int(time_diffs.quantile(0.75)), 14)
                else:
                    optimal_forward = 7
            else:
                optimal_forward = 7
        except:
            optimal_forward = 7

        return optimal_window, optimal_forward

    # –ò—Å–ø–æ–ª—å–∑—É–π –≤ build_all_snapshots:
    def build_all_snapshots(self, train_end: str, val_end: str, test_end: str,
                            window_back_days: int = None,  # —Å–¥–µ–ª–∞–π –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º
                            window_forward_days: int = None):

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ–±—ã—Ç–∏—è
        events = self._load_and_prepare_events()

        # –ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä –æ–∫–æ–Ω –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω—ã
        if window_back_days is None or window_forward_days is None:
            optimal_back, optimal_forward = self._optimize_window_sizes(events)
            window_back_days = window_back_days or optimal_back
            window_forward_days = window_forward_days or optimal_forward

            print(f"üéØ Auto-optimized windows: back={window_back_days}d, forward={window_forward_days}d")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞—Ç—ã —Å–Ω–∞–ø—à–æ—Ç–æ–≤ (–µ–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤)
        snapshot_dates = self._generate_snapshot_dates(events, window_back_days, window_forward_days)

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test
        train_dates, val_dates, test_dates = self._split_dates(
            snapshot_dates, train_end, val_end, test_end
        )

        print(f"üìä Total snapshots: {len(snapshot_dates)}")
        print(f"  Train: {len(train_dates)}, Val: {len(val_dates)}, Test: {len(test_dates)}")

        # –°—Ç—Ä–æ–∏–º —Å–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        snapshots = {}

        # –ú–æ–¥–µ–ª—å 1: Context-Aware Purchase Prediction
        snapshots['model1'] = self._build_model1_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        # –ú–æ–¥–µ–ª—å 2: Cross-Region Demand Transfer
        snapshots['model2'] = self._build_model2_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        # –ú–æ–¥–µ–ª—å 3: Micro-Trend Anticipation
        snapshots['model3'] = self._build_model3_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        # –ú–æ–¥–µ–ª—å 4: Adaptive Pricing
        snapshots['model4'] = self._build_model4_snapshots(
            events, train_dates, val_dates, test_dates, window_back_days, window_forward_days
        )

        return snapshots

    def _load_and_prepare_events(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –≥–æ—Ç–æ–≤–∏–º —Å–æ–±—ã—Ç–∏—è"""
        parquet_files = list(self.parquet_dir.glob("events_part_*.parquet"))

        if not parquet_files:
            raise ValueError("No parquet files found!")

        print(f"üìÇ Loading {len(parquet_files)} parquet files...")

        # –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–≤—ã—Ö 5 —Ñ–∞–π–ª–æ–≤ (–¥–ª—è –¥–µ–º–æ)
        dfs = []
        for file in parquet_files:
            df = pd.read_parquet(file)
            df['ts'] = pd.to_datetime(df['ts'])

            # –ü–∞—Ä—Å–∏–º properties
            df = self._parse_properties(df)

            dfs.append(df)

        events = pd.concat(dfs, ignore_index=True).sort_values('ts')

        print(f"‚úÖ Loaded {len(events):,} events from {events['ts'].min()} to {events['ts'].max()}")

        return events

    def _parse_properties(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–∞—Ä—Å–∏–º JSON —Å–≤–æ–π—Å—Ç–≤–∞"""
        import json

        def parse_json(x):
            if isinstance(x, str):
                try:
                    return json.loads(x.replace("'", '"'))
                except:
                    return {}
            return x if isinstance(x, dict) else {}

        df['properties_dict'] = df['properties'].apply(parse_json)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ –ø–æ–ª—è
        df['search_query'] = df['properties_dict'].apply(lambda x: x.get('search_query', ''))
        df['category'] = df['properties_dict'].apply(lambda x: x.get('category', ''))
        df['device'] = df['properties_dict'].apply(lambda x: x.get('device', 'desktop'))
        df['price_from_props'] = df['properties_dict'].apply(lambda x: float(x.get('price', 0)))

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Ü–µ–Ω—É
        df['price'] = df.apply(
            lambda row: row['price'] if pd.notna(row['price']) else row['price_from_props'],
            axis=1
        )

        return df

    def _generate_snapshot_dates(self, events: pd.DataFrame,
                                 window_back: int, window_forward: int) -> List[datetime]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞—Ç—ã —Å–Ω–∞–ø—à–æ—Ç–æ–≤ (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤)"""
        min_date = events['ts'].min().floor('D') + timedelta(days=window_back)
        max_date = events['ts'].max().ceil('D') - timedelta(days=window_forward)

        return pd.date_range(start=min_date, end=max_date, freq='1D')

    def _split_dates(self, dates: List[datetime],
                     train_end: str, val_end: str, test_end: str) -> Tuple:
        """–†–∞–∑–¥–µ–ª—è–µ–º –¥–∞—Ç—ã –Ω–∞ train/val/test"""
        train_end_dt = pd.to_datetime(train_end)
        val_end_dt = pd.to_datetime(val_end)
        test_end_dt = pd.to_datetime(test_end)

        train = [d for d in dates if d <= train_end_dt]
        val = [d for d in dates if train_end_dt < d <= val_end_dt]
        test = [d for d in dates if val_end_dt < d <= test_end_dt]

        return train, val, test

    # ===== –ú–û–î–ï–õ–¨ 1: Context-Aware Purchase Prediction =====

    def _build_model1_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ö–û–ù–¢–ï–ö–°–¢–ù–û–ô –ø–æ–∫—É–ø–∫–∏"""

        print("\nüîÆ Building Model 1: Context-Aware Purchase Prediction...")

        datasets = {}

        for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
            snapshots = []

            for snapshot_date in dates:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è –¥–µ–º–æ
                # –û–∫–Ω–æ —Ñ–∏—á
                feature_start = snapshot_date - timedelta(days=window_back)
                feature_end = snapshot_date

                # –û–∫–Ω–æ —Ç–∞—Ä–≥–µ—Ç–æ–≤
                target_start = snapshot_date
                target_end = snapshot_date + timedelta(days=window_forward)

                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è
                feature_events = events[
                    (events['ts'] >= feature_start) &
                    (events['ts'] < feature_end)
                    ]

                target_events = events[
                    (events['ts'] >= target_start) &
                    (events['ts'] < target_end)
                    ]

                # –°—Ç—Ä–æ–∏–º —Ñ–∏—á–∏
                features = self._build_model1_features(feature_events, snapshot_date)

                # –°—Ç—Ä–æ–∏–º –ú–£–õ–¨–¢–ò–ó–ê–î–ê–ß–ù–´–ï —Ç–∞—Ä–≥–µ—Ç—ã
                targets = self._build_model1_targets(target_events, features.index, snapshot_date)

                # –û–±—ä–µ–¥–∏–Ω—è–µ–º
                snapshot_df = features.join(targets, how='left').fillna(0)
                snapshot_df['snapshot_date'] = snapshot_date

                snapshots.append(snapshot_df.reset_index())

            if snapshots:
                datasets[dataset_name] = pd.concat(snapshots, ignore_index=True)
                print(f"  {dataset_name}: {len(datasets[dataset_name]):,} samples")
            else:
                datasets[dataset_name] = pd.DataFrame()

        return datasets

    def _build_model1_features(self, events: pd.DataFrame, snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è Model 1 —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""

        if events.empty:
            return pd.DataFrame()

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_features = events.groupby('user_id').agg({
            'event_id': 'count',
            'ts': ['max', 'min', 'nunique'],
            'event_type': lambda x: (x == 'purchase').sum(),
            'price': 'sum'
        })

        # Flatten columns
        user_features.columns = ['total_events', 'last_event', 'first_event',
                                 'active_days', 'total_purchases', 'total_spent']

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        user_features['days_since_first'] = (snapshot_date - user_features['first_event']).dt.days
        user_features['days_since_last'] = (snapshot_date - user_features['last_event']).dt.days
        user_features['events_per_day'] = user_features['total_events'] / (user_features['days_since_first'] + 1)

        # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –∏ –≤—Ä–µ–º—è
        user_features['snapshot_month'] = snapshot_date.month
        user_features['snapshot_day_of_week'] = snapshot_date.weekday()
        user_features['snapshot_hour'] = snapshot_date.hour

        # –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        # 1. –°–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        sessions = events.groupby(['user_id', 'session_id']).agg({
            'ts': ['min', 'max', 'count'],
            'event_type': lambda x: list(x)
        })

        sessions.columns = ['session_start', 'session_end', 'session_events', 'session_event_types']

        user_sessions = sessions.groupby('user_id').agg({
            'session_events': ['mean', 'std', 'count'],
            'session_start': 'min',
            'session_end': 'max'
        })

        user_sessions.columns = ['avg_session_events', 'std_session_events', 'session_count',
                                 'first_session', 'last_session']

        user_sessions['avg_session_duration'] = (
                                                        user_sessions['last_session'] - user_sessions['first_session']
                                                ).dt.total_seconds() / 3600 / (user_sessions['session_count'] + 1)

        # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
        if 'category' in events.columns:
            category_counts = events.groupby(['user_id', 'category']).size().unstack(fill_value=0)
            category_counts = category_counts.add_prefix('category_')
            user_features = user_features.join(category_counts, how='left')

        # 3. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—É—Ç—Ä–æ/–¥–µ–Ω—å/–≤–µ—á–µ—Ä)
        events['hour'] = events['ts'].dt.hour
        time_patterns = pd.crosstab(events['user_id'], pd.cut(events['hour'],
                                                              bins=[0, 8, 16, 24],
                                                              labels=['night', 'day', 'evening']))
        time_patterns = time_patterns.add_prefix('activity_')

        # 4. –¶–µ–Ω–æ–≤–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å Bayesian –ø–æ–¥—Ö–æ–¥–æ–º
        purchases = events[events['event_type'] == 'purchase']
        if not purchases.empty:
            for user_id, user_purchases in purchases.groupby('user_id'):
                if len(user_purchases) >= 3:
                    # Bayesian –æ—Ü–µ–Ω–∫–∞ —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
                    try:
                        prices = user_purchases['price'].values
                        # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
                        mask = prices > 0
                        if np.sum(mask) >= 2:
                            log_prices = np.log(prices[mask])
                            # –ö–∞–∂–¥–∞—è –ø–æ–∫—É–ø–∫–∞ = 1, –ª–æ–≥–∞—Ä–∏—Ñ–º–∏—Ä—É–µ–º
                            log_quantities = np.log(np.ones(np.sum(mask)))

                            # –ü—Ä–æ—Å—Ç–∞—è OLS –æ—Ü–µ–Ω–∫–∞ —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
                            X = sm.add_constant(log_prices)
                            model = sm.OLS(log_quantities, X)
                            results = model.fit()

                            user_features.loc[user_id, 'price_elasticity'] = results.params[1]
                            user_features.loc[user_id, 'elasticity_se'] = results.bse[1]
                            user_features.loc[user_id, 'elasticity_pval'] = results.pvalues[1]

                            # Bayesian credible interval (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è)
                            n = len(log_prices)
                            t_critical = stats.t.ppf(0.975, n - 2)
                            ci_lower = results.params[1] - t_critical * results.bse[1]
                            ci_upper = results.params[1] + t_critical * results.bse[1]
                            user_features.loc[user_id, 'elasticity_ci_width'] = ci_upper - ci_lower
                    except:
                        pass

        # 5. SARIMA –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–≤–º–µ—Å—Ç–æ Prophet)
        if 'ts' in events.columns and len(events) > 30:
            user_activity = events.groupby(['user_id', pd.Grouper(key='ts', freq='D')]).size()

            for user_id in user_features.index:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                if user_id in user_activity.index:
                    try:
                        user_series = user_activity.loc[user_id]
                        if isinstance(user_series, pd.Series) and len(user_series) > 14:
                            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏
                            adf_result = adfuller(user_series.values, autolag='AIC')
                            user_features.loc[user_id, 'adf_statistic'] = adf_result[0]
                            user_features.loc[user_id, 'adf_pvalue'] = adf_result[1]

                            # –ü—Ä–æ—Å—Ç–∞—è —Å–µ–∑–æ–Ω–Ω–∞—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è
                            if len(user_series) >= 30:
                                decomposition = seasonal_decompose(
                                    user_series.values,
                                    model='additive',
                                    period=7,
                                    extrapolate_trend='freq'
                                )

                                user_features.loc[user_id, 'trend_strength'] = np.maximum(
                                    0,
                                    1 - np.var(decomposition.resid) / np.var(decomposition.trend + decomposition.resid)
                                )
                                user_features.loc[user_id, 'seasonal_strength'] = np.maximum(
                                    0,
                                    1 - np.var(decomposition.resid) / np.var(
                                        decomposition.seasonal + decomposition.resid)
                                )
                    except:
                        pass

        # 6. –†—É—á–Ω–æ–π feature engineering –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ (–≤–º–µ—Å—Ç–æ tsfresh)
        if len(events) > 100:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –∏ –≤—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            time_stats = events.groupby('user_id').agg({
                'ts': lambda x: self._compute_time_series_stats(x, snapshot_date)
            })
            time_stats_df = pd.DataFrame(
                time_stats['ts'].tolist(),
                index=time_stats.index
            )
            user_features = user_features.join(time_stats_df, how='left')

        # 7. –£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –∏ scaling
        if not user_features.empty:
            # –£–¥–∞–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            numeric_cols = user_features.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                # –£–¥–∞–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω—É–ª–µ–≤–æ–π –¥–∏—Å–ø–µ—Ä—Å–∏–µ–π
                variances = user_features[numeric_cols].var()
                cols_to_keep = variances[variances > 1e-10].index
                user_features = user_features[cols_to_keep.tolist() +
                                              [c for c in user_features.columns if c not in numeric_cols]]

                # –ò–º–ø—É—Ç–∞—Ü–∏—è –º–µ–¥–∏–∞–Ω–æ–π
                imputer = SimpleImputer(strategy='median')
                numeric_data = imputer.fit_transform(user_features.select_dtypes(include=[np.number]))
                user_features[user_features.select_dtypes(include=[np.number]).columns] = numeric_data

                # PowerTransformer –≤–º–µ—Å—Ç–æ Quantile (–ª—É—á—à–µ –¥–ª—è 3.10)
                try:
                    transformer = PowerTransformer(method='yeo-johnson')
                    scaled_data = transformer.fit_transform(user_features.select_dtypes(include=[np.number]))
                    user_features[user_features.select_dtypes(include=[np.number]).columns] = scaled_data
                except:
                    # Fallback –Ω–∞ StandardScaler
                    scaler = StandardScaler()
                    scaled_data = scaler.fit_transform(user_features.select_dtypes(include=[np.number]))
                    user_features[user_features.select_dtypes(include=[np.number]).columns] = scaled_data

        return user_features

    def _compute_time_series_stats(self, timestamps, snapshot_date):
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞"""
        if len(timestamps) < 2:
            return {}

        # –ü–†–ê–í–ò–õ–¨–ù–û–ï –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ datetime
        if isinstance(timestamps, pd.Series):
            times = pd.to_datetime(timestamps).sort_values()
        else:
            times = pd.to_datetime(pd.Series(timestamps)).sort_values()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ times –Ω–µ –ø—É—Å—Ç–æ
        if len(times) == 0:
            return {}

        deltas = np.diff(times).astype('timedelta64[s]').astype(float)

        # –ë–ï–ó–û–ü–ê–°–ù–û–ï –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–Ω—Ç—Ä–æ–ø–∏–∏
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∞—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ
            if hasattr(times, 'dt'):
                hours = times.dt.hour
            else:
                # –ï—Å–ª–∏ times —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã
                hours = pd.Series(times).dt.hour if hasattr(pd.Series(times), 'dt') else pd.Series([0] * len(times))

            entropy = self._calculate_entropy(hours) if len(hours) > 0 else 0
        except:
            entropy = 0

        stats_dict = {
            'interarrival_mean': np.mean(deltas) if len(deltas) > 0 else 0,
            'interarrival_std': np.std(deltas) if len(deltas) > 0 else 0,
            'interarrival_cv': np.std(deltas) / np.mean(deltas) if len(deltas) > 0 and np.mean(deltas) > 0 else 0,
            'burstiness': (np.std(deltas) - np.mean(deltas)) / (np.std(deltas) + np.mean(deltas)) if len(
                deltas) > 0 else 0,
            'activity_entropy': entropy,
        }

        # –ê–≤—Ç–æ–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ª–∞–≥–∞ 1 (—Å –∑–∞—â–∏—Ç–æ–π)
        if len(times) >= 5:
            try:
                daily_counts = pd.Series(times).dt.date.value_counts().sort_index()
                if len(daily_counts) >= 3:
                    stats_dict['autocorr_lag1'] = daily_counts.autocorr(lag=1)
            except:
                stats_dict['autocorr_lag1'] = 0

        return stats_dict

    def _calculate_entropy(self, values):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —ç–Ω—Ç—Ä–æ–ø–∏—é –®–µ–Ω–Ω–æ–Ω–∞"""
        from collections import Counter
        counts = Counter(values)
        probs = np.array(list(counts.values())) / len(values)
        return -np.sum(probs * np.log2(probs + 1e-10))

    def _build_model1_targets(self, target_events: pd.DataFrame,
                              user_index: pd.Index, snapshot_date: datetime) -> pd.DataFrame:
        """–ú—É–ª—å—Ç–∏–∑–∞–¥–∞—á–Ω—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è Model 1"""

        targets = pd.DataFrame(index=user_index)

        # 1. –ë–∏–Ω–∞—Ä–Ω—ã–π —Ç–∞—Ä–≥–µ—Ç: –∫—É–ø–∏—Ç –ª–∏ –≤–æ–æ–±—â–µ
        targets['target_will_purchase'] = 0

        # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏—è –ø–æ–∫—É–ø–∫–∏ (–µ—Å–ª–∏ –∫—É–ø–∏—Ç)
        targets['target_category'] = ''

        # 3. –í—Ä–µ–º—è –¥–æ –ø–æ–∫—É–ø–∫–∏ (–≤ –¥–Ω—è—Ö)
        targets['target_days_to_purchase'] = 999

        # 4. –°—É–º–º–∞ –ø–æ–∫—É–ø–∫–∏
        targets['target_purchase_amount'] = 0

        if target_events.empty:
            return targets

        purchases = target_events[target_events['event_type'] == 'purchase']

        if not purchases.empty:
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
            for user_id in purchases['user_id'].unique():
                user_purchases = purchases[purchases['user_id'] == user_id]

                targets.loc[user_id, 'target_will_purchase'] = 1

                # –°–∞–º–∞—è —á–∞—Å—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                if 'category' in user_purchases.columns:
                    top_category = user_purchases['category'].mode()
                    if not top_category.empty:
                        targets.loc[user_id, 'target_category'] = top_category.iloc[0]

                # –í—Ä–µ–º—è –¥–æ –ø–µ—Ä–≤–æ–π –ø–æ–∫—É–ø–∫–∏
                first_purchase_time = user_purchases['ts'].min()
                days_to_purchase = (first_purchase_time - snapshot_date).days
                targets.loc[user_id, 'target_days_to_purchase'] = max(0, days_to_purchase)

                # –°—É–º–º–∞ –ø–æ–∫—É–ø–æ–∫
                targets.loc[user_id, 'target_purchase_amount'] = user_purchases['price'].sum()

        return targets

    def _add_google_trends_features(self, snapshot_date: datetime) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ–º —Ñ–∏—á–∏ –∏–∑ Google Trends"""
        if self.google_trends.empty:
            return pd.DataFrame()

        # –ë–µ—Ä–µ–º —Ç—Ä–µ–Ω–¥—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
        start_date = snapshot_date - timedelta(days=7)

        recent_trends = self.google_trends[
            (self.google_trends['date'] >= start_date.date()) &
            (self.google_trends['date'] <= snapshot_date.date())
            ]

        if recent_trends.empty:
            return pd.DataFrame()

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º
        trends_features = {}

        for query in recent_trends['query'].unique():
            query_trends = recent_trends[recent_trends['query'] == query]

            # –°—Ä–µ–¥–Ω—è—è –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å
            trends_features[f'trend_{query}_avg'] = query_trends['popularity'].mean()

            # –¢—Ä–µ–Ω–¥ (—Ä–æ—Å—Ç/–ø–∞–¥–µ–Ω–∏–µ)
            if len(query_trends) > 1:
                first = query_trends.iloc[0]['popularity']
                last = query_trends.iloc[-1]['popularity']
                trends_features[f'trend_{query}_growth'] = (last - first) / (first + 1)
            else:
                trends_features[f'trend_{query}_growth'] = 0

        return pd.DataFrame([trends_features])

    # ===== –ú–û–î–ï–õ–¨ 2: Cross-Region Demand Transfer =====

    def _build_model2_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞ —Å–ø—Ä–æ—Å–∞ –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–∞–º–∏"""

        print("\nüåç Building Model 2: Cross-Region Demand Transfer...")

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        regions = events['region'].unique()
        print(f"  Regions: {regions}")

        datasets = {'train': [], 'val': [], 'test': []}

        for region in regions:
            region_events = events[events['region'] == region]

            for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
                region_snapshots = []

                for snapshot_date in dates:  # –û–≥—Ä–∞–Ω–∏—á–∏–º
                    feature_start = snapshot_date - timedelta(days=window_back)
                    feature_end = snapshot_date
                    target_start = snapshot_date
                    target_end = snapshot_date + timedelta(days=window_forward)

                    feature_events = region_events[
                        (region_events['ts'] >= feature_start) &
                        (region_events['ts'] < feature_end)
                        ]

                    target_events = region_events[
                        (region_events['ts'] >= target_start) &
                        (region_events['ts'] < target_end)
                        ]

                    # –§–∏—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–≥–∏–æ–Ω–∞
                    features = self._build_model2_features(feature_events, snapshot_date, region)
                    targets = self._build_model2_targets(target_events, snapshot_date)

                    snapshot_df = pd.concat([features, targets], axis=1)
                    snapshot_df['snapshot_date'] = snapshot_date
                    snapshot_df['region'] = region

                    region_snapshots.append(snapshot_df.reset_index(drop=True))

                if region_snapshots:
                    region_df = pd.concat(region_snapshots, ignore_index=True)
                    datasets[dataset_name].append(region_df)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ä–µ–≥–∏–æ–Ω—ã
        result = {}
        for dataset_name in ['train', 'val', 'test']:
            if datasets[dataset_name]:
                result[dataset_name] = pd.concat(datasets[dataset_name], ignore_index=True)
                print(f"  {dataset_name}: {len(result[dataset_name]):,} region-snapshots")
            else:
                result[dataset_name] = pd.DataFrame()

        return result

    def _build_model2_features(self, events: pd.DataFrame,
                               snapshot_date: datetime, region: str) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è Model 2 (—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ)"""

        features = {}

        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–≥–∏–æ–Ω–∞
        features['region_total_events'] = len(events)
        features['region_unique_users'] = events['user_id'].nunique()
        features['region_purchase_count'] = (events['event_type'] == 'purchase').sum()
        features['region_total_spent'] = events[events['event_type'] == 'purchase']['price'].sum()

        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        if len(events) > 0:
            events['hour'] = events['ts'].dt.hour
            morning_events = ((events['hour'] >= 6) & (events['hour'] < 12)).sum()
            evening_events = ((events['hour'] >= 18) & (events['hour'] < 24)).sum()
            features['region_morning_ratio'] = morning_events / len(events)
            features['region_evening_ratio'] = evening_events / len(events)

        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        if 'category' in events.columns:
            top_categories = events['category'].value_counts().head(3)
            for i, (cat, count) in enumerate(top_categories.items()):
                features[f'region_top_category_{i + 1}'] = cat
                features[f'region_top_category_{i + 1}_count'] = count

        # –¶–µ–Ω–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        purchase_events = events[events['event_type'] == 'purchase']
        if len(purchase_events) > 0:
            features['region_avg_price'] = purchase_events['price'].mean()
            features['region_price_std'] = purchase_events['price'].std()

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
        iso_tuple = snapshot_date.isocalendar()  # (year, week, weekday)
        features['snapshot_year'] = iso_tuple[0]
        features['snapshot_week'] = iso_tuple[1]
        features['snapshot_weekday'] = iso_tuple[2]

        # –°–æ—Å–µ–¥–Ω–∏–µ —Ä–µ–≥–∏–æ–Ω—ã (–¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä–∞)
        if region == 'UA-30':
            features['neighbor_UA40_proximity'] = 1.0
            features['neighbor_UA50_proximity'] = 0.7
        elif region == 'UA-40':
            features['neighbor_UA30_proximity'] = 1.0
            features['neighbor_UA50_proximity'] = 0.8
        else:  # UA-50
            features['neighbor_UA30_proximity'] = 0.7
            features['neighbor_UA40_proximity'] = 0.8

        return pd.DataFrame([features])

    def _build_model2_targets(self, target_events: pd.DataFrame,
                              snapshot_date: datetime) -> pd.DataFrame:
        """–¢–∞—Ä–≥–µ—Ç—ã –¥–ª—è Model 2 (—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–ø—Ä–æ—Å)"""

        targets = {}

        # –û–±—â–∏–π —Å–ø—Ä–æ—Å
        purchases = target_events[target_events['event_type'] == 'purchase']
        targets['target_purchase_count'] = len(purchases)
        targets['target_total_spent'] = purchases['price'].sum() if not purchases.empty else 0

        # –°–ø—Ä–æ—Å –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (top 3)
        if 'category' in purchases.columns and not purchases.empty:
            top_categories = purchases['category'].value_counts().head(3)
            for i, (cat, count) in enumerate(top_categories.items()):
                targets[f'target_category_{cat}_demand'] = count

        # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞
        if not purchases.empty:
            purchases['day_of_week'] = purchases['ts'].dt.dayofweek
            weekday_demand = purchases[purchases['day_of_week'] < 5]['price'].sum()
            weekend_demand = purchases[purchases['day_of_week'] >= 5]['price'].sum()
            targets['target_weekday_demand'] = weekday_demand
            targets['target_weekend_demand'] = weekend_demand

        return pd.DataFrame([targets])

    # ===== –ú–û–î–ï–õ–¨ 3: Micro-Trend Anticipation =====

    def _build_model3_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""

        print("\nüìà Building Model 3: Micro-Trend Anticipation...")

        # –§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞—Ö
        trend_events = events[events['event_type'].isin(['search', 'product_view'])]

        if trend_events.empty:
            print("  ‚ö†Ô∏è No search or view events for trend analysis")
            return {'train': pd.DataFrame(), 'val': pd.DataFrame(), 'test': pd.DataFrame()}

        print(f"  Trend events: {len(trend_events):,}")
        print(f"  Search events: {(trend_events['event_type'] == 'search').sum():,}")
        print(f"  View events: {(trend_events['event_type'] == 'product_view').sum():,}")

        datasets = {}

        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞—Ç
        def process_dates(dataset_name, dates):
            snapshots = []
            total_processed = 0
            total_skipped = 0

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞—Ç –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if len(dates) > 50:
                print(f"  {dataset_name}: Too many dates ({len(dates)}), sampling 50...")
                dates = dates[:50]

            for i, snapshot_date in enumerate(dates):
                if i % 10 == 0:
                    print(f"    Processing {dataset_name} date {i + 1}/{len(dates)}: {snapshot_date.date()}")

                # –ö–æ—Ä–æ—Ç–∫–æ–µ –æ–∫–Ω–æ –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤
                feature_start = snapshot_date - timedelta(days=7)
                feature_end = snapshot_date
                target_start = snapshot_date
                target_end = snapshot_date + timedelta(days=7)

                feature_events = trend_events[
                    (trend_events['ts'] >= feature_start) &
                    (trend_events['ts'] < feature_end)
                    ]

                target_events = trend_events[
                    (trend_events['ts'] >= target_start) &
                    (trend_events['ts'] < target_end)
                    ]

                # –§–∏—á–∏ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤/—Ç–æ–≤–∞—Ä–æ–≤
                features = self._build_model3_features(feature_events, snapshot_date)

                if features.empty:
                    total_skipped += 1
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ features.index –Ω–µ –ø—É—Å—Ç–æ–π
                if len(features.index) == 0:
                    total_skipped += 1
                    continue

                targets = self._build_model3_targets(target_events, features.index)

                # –ï—Å–ª–∏ targets –ø—É—Å—Ç—ã–µ, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º –∏–Ω–¥–µ–∫—Å–æ–º
                if targets.empty:
                    targets = pd.DataFrame(index=features.index)
                    for col in ['target_future_searches', 'target_trend_continues', 'target_peak_in_days']:
                        targets[col] = 0

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤
                if not features.index.equals(targets.index):
                    print(f"    Warning: Index mismatch for {snapshot_date.date()}")
                    # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                    common_idx = features.index.intersection(targets.index)
                    if len(common_idx) == 0:
                        total_skipped += 1
                        continue
                    features = features.loc[common_idx]
                    targets = targets.loc[common_idx]

                try:
                    snapshot_df = features.join(targets, how='left').fillna(0)
                    snapshot_df['snapshot_date'] = snapshot_date
                    snapshot_df['dataset'] = dataset_name
                    snapshots.append(snapshot_df.reset_index())
                    total_processed += 1
                except Exception as e:
                    print(f"    Error joining features and targets: {str(e)}")
                    total_skipped += 1
                    continue

            print(f"    {dataset_name}: Processed {total_processed}, skipped {total_skipped}")

            if snapshots:
                return pd.concat(snapshots, ignore_index=True)
            else:
                return pd.DataFrame()

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–∞—Ç–∞—Å–µ—Ç—ã
        for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
            print(f"\n  Processing {dataset_name} ({len(dates)} dates)...")
            result_df = process_dates(dataset_name, dates)
            datasets[dataset_name] = result_df

            if not result_df.empty:
                print(f"  ‚úÖ {dataset_name}: {len(result_df):,} trend-snapshots, {len(result_df.columns)} features")
            else:
                print(f"  ‚ö†Ô∏è {dataset_name}: No snapshots generated")

        return datasets

    def _build_model3_features(self, events: pd.DataFrame,
                               snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤ (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""

        print(f"    Building Model 3 features for {snapshot_date.date()}...")

        # –†–∞–∑–¥–µ–ª—è–µ–º —Å–æ–±—ã—Ç–∏—è –ø–æ —Ç–∏–ø–∞–º
        search_events = events[events['event_type'] == 'search']
        view_events = events[events['event_type'] == 'product_view']

        print(f"      Total events: {len(events):,}")
        print(f"      Search events: {len(search_events):,}")
        print(f"      View events: {len(view_events):,}")

        # –°–ø–∏—Å–æ–∫ –¥–ª—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö —Ñ–∏—á
        all_features = []

        # 1. –¢—Ä–µ–Ω–¥—ã –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if not search_events.empty and 'search_query' in search_events.columns:
            query_features = self._build_search_query_features(search_events, snapshot_date)
            if not query_features.empty:
                all_features.append(query_features)
                print(f"      Query features: {len(query_features)} queries")

        # 2. –¢—Ä–µ–Ω–¥—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ—Å–º–æ—Ç—Ä—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
        if not view_events.empty and 'category' in view_events.columns:
            category_features = self._build_category_features(view_events, snapshot_date)
            if not category_features.empty:
                all_features.append(category_features)
                print(f"      Category features: {len(category_features)} categories")

        # 3. –¢—Ä–µ–Ω–¥—ã –ø–æ —Ç–æ–≤–∞—Ä–∞–º (–µ—Å–ª–∏ –Ω–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–π)
        elif not view_events.empty and 'item_id' in view_events.columns:
            item_features = self._build_item_features(view_events, snapshot_date)
            if not item_features.empty:
                all_features.append(item_features)
                print(f"      Item features: {len(item_features)} items")

        # 4. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        temporal_features = self._build_temporal_features(events, snapshot_date)
        if not temporal_features.empty:
            all_features.append(temporal_features)
            print(f"      Temporal features: {len(temporal_features)} time periods")

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∏—á–∏
        if all_features:
            result = pd.concat(all_features, axis=0)
            print(f"      Total features generated: {len(result)} trend entities")
            return result
        else:
            print(f"      No features generated")
            return pd.DataFrame()

    def _build_search_query_features(self, search_events: pd.DataFrame,
                                     snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""

        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        valid_searches = search_events[
            search_events['search_query'].notna() &
            (search_events['search_query'].astype(str).str.strip() != '')
            ]

        if valid_searches.empty:
            return pd.DataFrame()

        # –ë–µ—Ä–µ–º —Ç–æ–ø-100 –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ
        query_counts = valid_searches['search_query'].value_counts()
        top_queries = query_counts.head(100).index

        query_features_list = []

        for query in top_queries:
            try:
                query_data = valid_searches[valid_searches['search_query'] == query]

                # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
                features = {
                    'trend_id': f"query_{query}",
                    'trend_type': 'search_query',
                    'entity': query,
                    'total_count': len(query_data),
                    'unique_users': query_data['user_id'].nunique(),
                    'first_seen': query_data['ts'].min(),
                    'last_seen': query_data['ts'].max(),
                }

                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
                query_data['date'] = query_data['ts'].dt.date
                date_range = pd.date_range(start=features['first_seen'].date(),
                                           end=snapshot_date.date(), freq='D')
                daily_counts = query_data.groupby('date').size().reindex(date_range.date, fill_value=0)

                # –ê–∫—Ç–∏–≤–Ω—ã—Ö –¥–Ω–µ–π
                active_days = max((features['last_seen'] - features['first_seen']).days + 1, 1)
                features['frequency'] = features['total_count'] / active_days
                features['active_days'] = (daily_counts > 0).sum()

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–Ω—è–º
                if len(daily_counts) > 1:
                    features['mean_daily'] = daily_counts.mean()
                    features['std_daily'] = daily_counts.std()
                    features['cv_daily'] = features['std_daily'] / features['mean_daily'] if features[
                                                                                                 'mean_daily'] > 0 else 0

                    # –¢—Ä–µ–Ω–¥ –∏ —É—Å–∫–æ—Ä–µ–Ω–∏–µ
                    features['trend_slope'] = self._calculate_linear_trend(daily_counts.values)
                    features['trend_acceleration'] = self._calculate_acceleration(daily_counts.values)

                    # –†–æ—Å—Ç (CAGR)
                    if daily_counts.iloc[0] > 0 and daily_counts.iloc[-1] > 0:
                        periods = len(daily_counts) - 1
                        features['cagr'] = (daily_counts.iloc[-1] / daily_counts.iloc[0]) ** (1 / periods) - 1
                    else:
                        features['cagr'] = 0

                    # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è vs –ø–µ—Ä–≤—ã–µ 3 –¥–Ω—è
                    if len(daily_counts) >= 6:
                        first_avg = daily_counts.iloc[:3].mean()
                        last_avg = daily_counts.iloc[-3:].mean()
                        features['recent_growth'] = (last_avg - first_avg) / (first_avg + 1)
                    else:
                        features['recent_growth'] = 0
                else:
                    features.update({
                        'mean_daily': features['total_count'],
                        'std_daily': 0,
                        'cv_daily': 0,
                        'trend_slope': 0,
                        'trend_acceleration': 0,
                        'cagr': 0,
                        'recent_growth': 0
                    })

                # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏
                if 'region' in query_data.columns:
                    regions = query_data['region'].unique()
                    features['region_count'] = len(regions)
                    features['is_multi_region'] = len(regions) > 1

                    # –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π —Ä–µ–≥–∏–æ–Ω
                    if len(regions) > 0:
                        top_region = query_data['region'].mode()
                        if not top_region.empty:
                            features['top_region'] = top_region.iloc[0]
                else:
                    features['region_count'] = 1
                    features['is_multi_region'] = False
                    features['top_region'] = 'unknown'

                # –î–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if 'device' in query_data.columns:
                    devices = query_data['device'].unique()
                    features['device_count'] = len(devices)

                # –í—Ä–µ–º—è —Å—É—Ç–æ–∫
                if 'ts' in query_data.columns:
                    query_data['hour'] = query_data['ts'].dt.hour
                    morning = ((query_data['hour'] >= 6) & (query_data['hour'] < 12)).sum()
                    evening = ((query_data['hour'] >= 18) & (query_data['hour'] < 24)).sum()
                    features['morning_ratio'] = morning / len(query_data) if len(query_data) > 0 else 0
                    features['evening_ratio'] = evening / len(query_data) if len(query_data) > 0 else 0

                query_features_list.append(features)

            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                continue

        if query_features_list:
            df = pd.DataFrame(query_features_list)
            df.set_index('trend_id', inplace=True)
            return df
        else:
            return pd.DataFrame()

    def _build_category_features(self, view_events: pd.DataFrame,
                                 snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º"""

        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∞–ª–∏–¥–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        valid_views = view_events[view_events['category'].notna()]

        if valid_views.empty:
            return pd.DataFrame()

        # –ë–µ—Ä–µ–º —Ç–æ–ø-50 –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        category_counts = valid_views['category'].value_counts()
        top_categories = category_counts.head(50).index

        category_features_list = []

        for category in top_categories:
            try:
                category_data = valid_views[valid_views['category'] == category]

                # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
                features = {
                    'trend_id': f"category_{category}",
                    'trend_type': 'category',
                    'entity': category,
                    'total_views': len(category_data),
                    'unique_viewers': category_data['user_id'].nunique(),
                    'first_view': category_data['ts'].min(),
                    'last_view': category_data['ts'].max(),
                }

                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
                category_data['date'] = category_data['ts'].dt.date
                date_range = pd.date_range(start=features['first_view'].date(),
                                           end=snapshot_date.date(), freq='D')
                daily_counts = category_data.groupby('date').size().reindex(date_range.date, fill_value=0)

                # –ê–∫—Ç–∏–≤–Ω—ã—Ö –¥–Ω–µ–π
                active_days = max((features['last_view'] - features['first_view']).days + 1, 1)
                features['frequency'] = features['total_views'] / active_days
                features['active_days'] = (daily_counts > 0).sum()

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–Ω—è–º
                if len(daily_counts) > 1:
                    features['mean_daily'] = daily_counts.mean()
                    features['std_daily'] = daily_counts.std()
                    features['cv_daily'] = features['std_daily'] / features['mean_daily'] if features[
                                                                                                 'mean_daily'] > 0 else 0

                    # –¢—Ä–µ–Ω–¥
                    features['trend_slope'] = self._calculate_linear_trend(daily_counts.values)
                    features['trend_acceleration'] = self._calculate_acceleration(daily_counts.values)

                    # –†–æ—Å—Ç
                    if daily_counts.iloc[0] > 0 and daily_counts.iloc[-1] > 0:
                        periods = len(daily_counts) - 1
                        features['cagr'] = (daily_counts.iloc[-1] / daily_counts.iloc[0]) ** (1 / periods) - 1
                    else:
                        features['cagr'] = 0
                else:
                    features.update({
                        'mean_daily': features['total_views'],
                        'std_daily': 0,
                        'cv_daily': 0,
                        'trend_slope': 0,
                        'trend_acceleration': 0,
                        'cagr': 0
                    })

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π
                # –ö–æ–Ω–≤–µ—Ä—Å–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø–æ–∫—É–ø–∫–∞—Ö)
                if 'event_type' in category_data.columns:
                    purchases_in_cat = category_data[category_data['event_type'] == 'purchase']
                    features['purchase_count'] = len(purchases_in_cat)
                    features['conversion_rate'] = len(purchases_in_cat) / len(category_data) if len(
                        category_data) > 0 else 0

                # –¶–µ–Ω–æ–≤—ã–µ —Ñ–∏—á–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å —Ü–µ–Ω–∞)
                if 'price' in category_data.columns:
                    price_data = category_data[category_data['price'] > 0]['price']
                    if len(price_data) > 0:
                        features['avg_price'] = price_data.mean()
                        features['price_std'] = price_data.std()
                        features['min_price'] = price_data.min()
                        features['max_price'] = price_data.max()

                # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏
                if 'region' in category_data.columns:
                    regions = category_data['region'].unique()
                    features['region_count'] = len(regions)
                    features['is_multi_region'] = len(regions) > 1

                category_features_list.append(features)

            except Exception as e:
                continue

        if category_features_list:
            df = pd.DataFrame(category_features_list)
            df.set_index('trend_id', inplace=True)
            return df
        else:
            return pd.DataFrame()

    def _build_item_features(self, view_events: pd.DataFrame,
                             snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ —Ç–æ–≤–∞—Ä–∞–º"""

        # –ë–µ—Ä–µ–º —Ç–æ–ø-50 —Ç–æ–≤–∞—Ä–æ–≤
        item_counts = view_events['item_id'].value_counts()
        top_items = item_counts.head(50).index

        item_features_list = []

        for item_id in top_items:
            try:
                item_data = view_events[view_events['item_id'] == item_id]

                # –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏
                features = {
                    'trend_id': f"item_{item_id}",
                    'trend_type': 'item',
                    'entity': str(item_id),
                    'total_views': len(item_data),
                    'unique_viewers': item_data['user_id'].nunique(),
                    'first_view': item_data['ts'].min(),
                    'last_view': item_data['ts'].max(),
                }

                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∏—á–∏
                item_data['date'] = item_data['ts'].dt.date
                date_range = pd.date_range(start=features['first_view'].date(),
                                           end=snapshot_date.date(), freq='D')
                daily_counts = item_data.groupby('date').size().reindex(date_range.date, fill_value=0)

                # –ê–∫—Ç–∏–≤–Ω—ã—Ö –¥–Ω–µ–π
                active_days = max((features['last_view'] - features['first_view']).days + 1, 1)
                features['frequency'] = features['total_views'] / active_days
                features['active_days'] = (daily_counts > 0).sum()

                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –¥–Ω—è–º
                if len(daily_counts) > 1:
                    features['mean_daily'] = daily_counts.mean()
                    features['std_daily'] = daily_counts.std()
                    features['cv_daily'] = features['std_daily'] / features['mean_daily'] if features[
                                                                                                 'mean_daily'] > 0 else 0

                    # –¢—Ä–µ–Ω–¥
                    features['trend_slope'] = self._calculate_linear_trend(daily_counts.values)

                    # –†–æ—Å—Ç –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è
                    if len(daily_counts) >= 4:
                        recent_avg = daily_counts.iloc[-3:].mean()
                        prev_avg = daily_counts.iloc[-6:-3].mean() if len(daily_counts) >= 6 else daily_counts.iloc[0]
                        features['recent_growth'] = (recent_avg - prev_avg) / (prev_avg + 1)
                    else:
                        features['recent_growth'] = 0
                else:
                    features.update({
                        'mean_daily': features['total_views'],
                        'std_daily': 0,
                        'cv_daily': 0,
                        'trend_slope': 0,
                        'recent_growth': 0
                    })

                # –ö–æ–Ω–≤–µ—Ä—Å–∏—è
                if 'event_type' in item_data.columns:
                    purchases = item_data[item_data['event_type'] == 'purchase']
                    features['purchase_count'] = len(purchases)
                    features['conversion_rate'] = len(purchases) / len(item_data) if len(item_data) > 0 else 0

                # –¶–µ–Ω–∞
                if 'price' in item_data.columns:
                    price_data = item_data[item_data['price'] > 0]['price']
                    if len(price_data) > 0:
                        features['avg_price'] = price_data.mean()

                item_features_list.append(features)

            except Exception as e:
                continue

        if item_features_list:
            df = pd.DataFrame(item_features_list)
            df.set_index('trend_id', inplace=True)
            return df
        else:
            return pd.DataFrame()

    def _build_temporal_features(self, events: pd.DataFrame,
                                 snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤ (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ)"""

        if events.empty:
            return pd.DataFrame()

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —á–∞—Å–∞–º
        events['hour'] = events['ts'].dt.hour
        hourly_counts = events.groupby('hour').size()

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        events['day_of_week'] = events['ts'].dt.dayofweek
        dow_counts = events.groupby('day_of_week').size()

        features = {
            'trend_id': 'global_temporal',
            'trend_type': 'temporal',
            'entity': 'global',
            'total_events': len(events),
            'unique_users': events['user_id'].nunique(),

            # –ß–∞—Å–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            'peak_hour': hourly_counts.idxmax() if not hourly_counts.empty else 0,
            'peak_hour_count': hourly_counts.max() if not hourly_counts.empty else 0,
            'morning_events': ((events['hour'] >= 6) & (events['hour'] < 12)).sum(),
            'afternoon_events': ((events['hour'] >= 12) & (events['hour'] < 18)).sum(),
            'evening_events': ((events['hour'] >= 18) & (events['hour'] < 24)).sum(),
            'night_events': ((events['hour'] >= 0) & (events['hour'] < 6)).sum(),

            # –î–Ω–µ–≤–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            'peak_dow': dow_counts.idxmax() if not dow_counts.empty else 0,
            'weekend_events': (events['day_of_week'] >= 5).sum(),
            'weekday_events': (events['day_of_week'] < 5).sum(),

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π
            'search_ratio': (events['event_type'] == 'search').sum() / len(events) if len(events) > 0 else 0,
            'view_ratio': (events['event_type'] == 'product_view').sum() / len(events) if len(events) > 0 else 0,
            'purchase_ratio': (events['event_type'] == 'purchase').sum() / len(events) if len(events) > 0 else 0,
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å –ø–æ —á–∞—Å–∞–º
        if not hourly_counts.empty:
            features['hourly_cv'] = hourly_counts.std() / hourly_counts.mean() if hourly_counts.mean() > 0 else 0

        df = pd.DataFrame([features])
        df.set_index('trend_id', inplace=True)
        return df

    def _calculate_linear_trend(self, values):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ª–∏–Ω–µ–π–Ω—ã–π —Ç—Ä–µ–Ω–¥ (–Ω–∞–∫–ª–æ–Ω)"""
        if len(values) < 2:
            return 0

        x = np.arange(len(values))
        slope, _ = np.polyfit(x, values, 1)
        return float(slope)

    def _calculate_acceleration(self, values):
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Å–∫–æ—Ä–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞"""
        if len(values) < 3:
            return 0

        try:
            x = np.arange(len(values))
            coeffs = np.polyfit(x, values, 2)
            return float(2 * coeffs[0])  # –£—Å–∫–æ—Ä–µ–Ω–∏–µ = 2 * –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø—Ä–∏ x¬≤
        except:
            return float(values[-1] - 2 * values[-2] + values[-3])

    def _build_model3_targets(self, target_events: pd.DataFrame,
                              trend_index: pd.Index) -> pd.DataFrame:
        """–¢–∞—Ä–≥–µ—Ç—ã –¥–ª—è –º–∏–∫—Ä–æ-—Ç—Ä–µ–Ω–¥–æ–≤ (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""

        targets = pd.DataFrame(index=trend_index)

        # –ë–∞–∑–æ–≤—ã–µ —Ç–∞—Ä–≥–µ—Ç—ã –¥–ª—è –≤—Å–µ—Ö —Ç—Ä–µ–Ω–¥–æ–≤
        for col in ['target_future_count', 'target_growth', 'target_peak',
                    'target_continues', 'target_cross_region']:
            targets[col] = 0

        if target_events.empty:
            return targets

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ç—Ä–µ–Ω–¥
        for trend_id in trend_index:
            try:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ç—Ä–µ–Ω–¥–∞ –∏ —Å—É—â–Ω–æ—Å—Ç—å
                if trend_id.startswith('query_'):
                    entity_type = 'search_query'
                    entity = trend_id.split('query_', 1)[1]
                    filter_col = 'search_query'
                elif trend_id.startswith('category_'):
                    entity_type = 'category'
                    entity = trend_id.split('category_', 1)[1]
                    filter_col = 'category'
                elif trend_id.startswith('item_'):
                    entity_type = 'item'
                    entity = trend_id.split('item_', 1)[1]
                    filter_col = 'item_id'
                else:  # global_temporal
                    # –ì–ª–æ–±–∞–ª—å–Ω—ã–π —Ç—Ä–µ–Ω–¥
                    targets.loc[trend_id, 'target_future_count'] = len(target_events)
                    targets.loc[trend_id, 'target_continues'] = 1
                    continue

                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –¥–ª—è —ç—Ç–æ–π —Å—É—â–Ω–æ—Å—Ç–∏
                if entity_type == 'search_query':
                    entity_events = target_events[
                        (target_events['event_type'] == 'search') &
                        (target_events[filter_col] == entity)
                        ]
                else:
                    entity_events = target_events[
                        (target_events[filter_col] == entity)
                    ]

                if not entity_events.empty:
                    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                    targets.loc[trend_id, 'target_future_count'] = len(entity_events)
                    targets.loc[trend_id, 'target_continues'] = 1

                    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
                    targets.loc[trend_id, 'target_unique_users'] = entity_events['user_id'].nunique()

                    # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
                    if 'ts' in entity_events.columns:
                        entity_events['date'] = entity_events['ts'].dt.date
                        daily_counts = entity_events.groupby('date').size()

                        if len(daily_counts) > 0:
                            targets.loc[trend_id, 'target_peak'] = daily_counts.max()

                            # –†–æ—Å—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å vs –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å)
                            if len(daily_counts) >= 2:
                                first_day = daily_counts.iloc[0]
                                last_day = daily_counts.iloc[-1]
                                if first_day > 0:
                                    targets.loc[trend_id, 'target_growth'] = (last_day - first_day) / first_day

                    # –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ
                    if 'region' in entity_events.columns:
                        regions = entity_events['region'].unique()
                        targets.loc[trend_id, 'target_region_count'] = len(regions)
                        targets.loc[trend_id, 'target_cross_region'] = 1 if len(regions) > 1 else 0

                    # –ö–æ–Ω–≤–µ—Ä—Å–∏—è –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤/–∫–∞—Ç–µ–≥–æ—Ä–∏–π
                    if entity_type in ['category', 'item'] and 'event_type' in entity_events.columns:
                        purchases = entity_events[entity_events['event_type'] == 'purchase']
                        targets.loc[trend_id, 'target_purchases'] = len(purchases)
                        if len(entity_events) > 0:
                            targets.loc[trend_id, 'target_conversion'] = len(purchases) / len(entity_events)

            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—à–∏–±–∫–∏
                continue

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
        targets = targets.fillna(0)

        return targets

    # ===== –ú–û–î–ï–õ–¨ 4: Adaptive Pricing =====

    def _build_model4_snapshots(self, events, train_dates, val_dates, test_dates,
                                window_back, window_forward):
        """–°–Ω–∞–ø—à–æ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        print("\nüí∞ Building Model 4: Adaptive Pricing...")

        # –§–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –ø–æ–∫—É–ø–∫–∞—Ö –∏ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞—Ö
        price_events = events[events['event_type'].isin(['purchase', 'product_view'])]

        datasets = {}

        for dataset_name, dates in [('train', train_dates), ('val', val_dates), ('test', test_dates)]:
            snapshots = []

            for snapshot_date in dates:
                # –û–∫–Ω–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ü–µ–Ω–æ–≤–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                feature_start = snapshot_date - timedelta(days=window_back)
                feature_end = snapshot_date

                # –ë—É–¥—É—â–∏–µ –ø–æ–∫—É–ø–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
                target_start = snapshot_date
                target_end = snapshot_date + timedelta(days=window_forward)

                feature_events = price_events[
                    (price_events['ts'] >= feature_start) &
                    (price_events['ts'] < feature_end)
                    ]

                target_events = price_events[
                    (price_events['ts'] >= target_start) &
                    (price_events['ts'] < target_end)
                    ]

                # –§–∏—á–∏ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ç–æ–≤–∞—Ä–∞ - –° –ü–†–û–í–ï–†–ö–û–ô!
                features = self._build_model4_features(feature_events, snapshot_date)

                if features is None or features.empty:
                    print(f"    {snapshot_date.date()}: No features generated")
                    continue

                targets = self._build_model4_targets(target_events, features.index)

                if not features.empty:
                    snapshot_df = features.join(targets, how='left').fillna(0)
                    snapshot_df['snapshot_date'] = snapshot_date
                    snapshots.append(snapshot_df.reset_index())

            if snapshots:
                datasets[dataset_name] = pd.concat(snapshots, ignore_index=True)
                print(f"  {dataset_name}: {len(datasets[dataset_name]):,} price-snapshots")
            else:
                datasets[dataset_name] = pd.DataFrame()
                print(f"  {dataset_name}: No snapshots generated")

        return datasets

    def _build_model4_features(self, events: pd.DataFrame,
                               snapshot_date: datetime) -> pd.DataFrame:
        """–§–∏—á–∏ –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        if events.empty or 'item_id' not in events.columns:
            return pd.DataFrame()

        item_features = []

        # –û–≥—Ä–∞–Ω–∏—á–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        unique_items = events['item_id'].unique()
        if len(unique_items) > 100:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-100 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–≤–∞—Ä—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–±—ã—Ç–∏–π
            item_counts = events['item_id'].value_counts().head(100)
            unique_items = item_counts.index.tolist()

        for item_id in unique_items:
            try:
                item_events = events[events['item_id'] == item_id]

                features = {
                    'item_id': item_id,
                    'total_views': (item_events['event_type'] == 'product_view').sum(),
                    'total_purchases': (item_events['event_type'] == 'purchase').sum(),
                    'unique_viewers': item_events[item_events['event_type'] == 'product_view']['user_id'].nunique(),
                    'unique_buyers': item_events[item_events['event_type'] == 'purchase']['user_id'].nunique(),
                }

                # –ö–æ–Ω–≤–µ—Ä—Å–∏—è
                features['conversion_rate'] = (
                    features['total_purchases'] / features['total_views']
                    if features['total_views'] > 0 else 0
                )

                # –¶–µ–Ω–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                purchases = item_events[item_events['event_type'] == 'purchase']
                if not purchases.empty:
                    features['current_price'] = purchases['price'].iloc[-1] if len(purchases) > 0 else 0
                    features['avg_price'] = purchases['price'].mean()
                    features['price_std'] = purchases['price'].std()
                    features['min_price'] = purchases['price'].min()
                    features['max_price'] = purchases['price'].max()
                    features['price_range'] = features['max_price'] - features['min_price']
                else:
                    # –ï—Å–ª–∏ –Ω–µ –±—ã–ª–æ –ø–æ–∫—É–ø–æ–∫, –±–µ—Ä–µ–º —Ü–µ–Ω—É –∏–∑ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–ª–∏ 0
                    views_with_price = item_events[
                        (item_events['event_type'] == 'product_view') &
                        (item_events['price'] > 0)
                        ]
                    features['current_price'] = views_with_price['price'].iloc[-1] if len(views_with_price) > 0 else 0
                    features['avg_price'] = features['current_price']
                    features['price_std'] = 0
                    features['min_price'] = features['current_price']
                    features['max_price'] = features['current_price']
                    features['price_range'] = 0

                # Bayesian Beta-Binomial –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Å–∏–∏
                if features['total_views'] > 0:
                    # Prior: Beta(Œ±=2, Œ≤=8) - –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Å–∏—é ~20%
                    alpha_prior = 2
                    beta_prior = 8

                    alpha_post = alpha_prior + features['total_purchases']
                    beta_post = beta_prior + features['total_views'] - features['total_purchases']

                    # MAP –æ—Ü–µ–Ω–∫–∞ (mode of Beta distribution)
                    features['conversion_rate_map'] = (alpha_post - 1) / (alpha_post + beta_post - 2) if (
                                                                                                                 alpha_post + beta_post) > 2 else 0

                    # Mean
                    features['conversion_rate_mean'] = alpha_post / (alpha_post + beta_post)

                    # Standard deviation
                    var = (alpha_post * beta_post) / ((alpha_post + beta_post) ** 2 * (alpha_post + beta_post + 1))
                    features['conversion_rate_std'] = np.sqrt(var) if var > 0 else 0

                    # 90% credible interval (–∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ)
                    if features['conversion_rate_std'] > 0:
                        z_score = stats.norm.ppf(0.95)
                        margin = z_score * features['conversion_rate_std']
                        features['conversion_rate_lower'] = max(0, features['conversion_rate_mean'] - margin)
                        features['conversion_rate_upper'] = min(1, features['conversion_rate_mean'] + margin)

                # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è —Ü–µ–Ω–æ–≤–∞—è —ç–ª–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å
                if not purchases.empty and len(purchases) >= 10:
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –Ω–µ–¥–µ–ª—è–º –¥–ª—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏
                    purchases['week'] = purchases['ts'].dt.isocalendar().week
                    weekly_data = purchases.groupby('week').agg({
                        'price': ['mean', 'std'],
                        'event_id': 'count'
                    })
                    weekly_data.columns = ['price_mean', 'price_std', 'quantity']

                    if len(weekly_data) >= 4:
                        # –õ–æ–≥-–ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å
                        valid_mask = (weekly_data['price_mean'] > 0) & (weekly_data['quantity'] > 0)
                        if valid_mask.sum() >= 3:
                            log_price = np.log(weekly_data.loc[valid_mask, 'price_mean'].values)
                            log_quantity = np.log(weekly_data.loc[valid_mask, 'quantity'].values)

                            # OLS —Å –≥–µ—Ç–µ—Ä–æ—Å–∫–µ–¥–∞—Å—Ç–∏—á–Ω–æ-—É—Å—Ç–æ–π—á–∏–≤—ã–º–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–º–∏ –æ—à–∏–±–∫–∞–º–∏
                            X = sm.add_constant(log_price)
                            model = sm.OLS(log_quantity, X)
                            results = model.fit(cov_type='HC3')

                            features['price_elasticity_ols'] = results.params[1]
                            features['elasticity_pvalue'] = results.pvalues[1]
                            features['elasticity_r2'] = results.rsquared

                item_features.append(features)

            except Exception as e:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã
                continue

        # –í–û–ó–í–†–ê–©–ê–ï–ú DATAFRAME - –ò–°–ü–†–ê–í–õ–ï–ù–û!
        if item_features:
            df = pd.DataFrame(item_features)
            df.set_index('item_id', inplace=True)
            return df
        else:
            return pd.DataFrame()  # –í—Å–µ–≥–¥–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º DataFrame, –¥–∞–∂–µ –ø—É—Å—Ç–æ–π

    def _build_model4_targets(self, target_events: pd.DataFrame,
                              item_index: pd.Index) -> pd.DataFrame:
        """–¢–∞—Ä–≥–µ—Ç—ã –¥–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è"""

        targets = pd.DataFrame(index=item_index)
        targets['target_sales_count'] = 0
        targets['target_revenue'] = 0
        targets['target_optimal_price'] = 0
        targets['target_price_change_effect'] = 0.0

        if target_events.empty:
            return targets

        purchases = target_events[target_events['event_type'] == 'purchase']

        for item_id in item_index:
            item_purchases = purchases[purchases['item_id'] == item_id]

            if not item_purchases.empty:
                targets.loc[item_id, 'target_sales_count'] = len(item_purchases)
                targets.loc[item_id, 'target_revenue'] = item_purchases['price'].sum()

                # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è —Ü–µ–Ω–∞ (—Å—Ä–µ–¥–Ω—è—è —Ü–µ–Ω–∞ –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –±—ã–ª–∏ –ø–æ–∫—É–ø–∫–∏)
                targets.loc[item_id, 'target_optimal_price'] = item_purchases['price'].mean()

                # –≠—Ñ—Ñ–µ–∫—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω—ã: –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –º–µ–∂–¥—É –¥–Ω–µ–≤–Ω–æ–π —Ü–µ–Ω–æ–π –∏ –¥–Ω–µ–≤–Ω—ã–º–∏ –ø—Ä–æ–¥–∞–∂–∞–º–∏ –≤ –æ–∫–Ω–µ target
                item_purchases['date'] = item_purchases['ts'].dt.date
                daily_qty = item_purchases.groupby('date').size()
                daily_price = item_purchases.groupby('date')['price'].mean()
                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                common_idx = daily_qty.index.intersection(daily_price.index)
                if len(common_idx) >= 3:
                    qty_series = daily_qty.loc[common_idx].astype(float)
                    price_series = daily_price.loc[common_idx].astype(float)
                    if price_series.std() > 0 and qty_series.std() > 0:
                        corr = float(np.corrcoef(price_series.values, qty_series.values)[0, 1])
                        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –∑–Ω–∞–∫, —á—Ç–æ–±—ã –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ–∑–Ω–∞—á–∞–ª–æ —É–ª—É—á—à–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞ –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ —Ü–µ–Ω—ã
                        targets.loc[item_id, 'target_price_change_effect'] = -corr
                    else:
                        targets.loc[item_id, 'target_price_change_effect'] = 0.0
                else:
                    targets.loc[item_id, 'target_price_change_effect'] = 0.0

        return targets

    def save_all_snapshots(self, snapshots_dict: Dict,
                           output_dir: str = "../analytics/data/innovative_snapshots"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤—Å–µ —Å–Ω–∞–ø—à–æ—Ç—ã"""

        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)

        for model_name, datasets in snapshots_dict.items():
            model_dir = output_path / model_name
            model_dir.mkdir(exist_ok=True)

            for dataset_name, df in datasets.items():
                if not df.empty:
                    file_path = model_dir / f"{dataset_name}.parquet"
                    df.to_parquet(file_path, index=False)

                    print(f"üíæ Saved {model_name}/{dataset_name}: {len(df):,} rows")

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    meta = {
                        'model': model_name,
                        'dataset': dataset_name,
                        'rows': len(df),
                        'columns': df.columns.tolist(),  # –ü–µ—Ä–≤—ã–µ 20 –∫–æ–ª–æ–Ω–æ–∫
                        'saved_at': datetime.now().isoformat()
                    }

                    with open(model_dir / f"{dataset_name}_meta.json", 'w') as f:
                        json.dump(meta, f, indent=2)


# ===== –ó–ê–ü–£–°–ö =====

if __name__ == "__main__":
    print("=" * 60)
    print("üöÄ INNOVATIVE SNAPSHOT BUILDER FOR 4 MODELS")
    print("=" * 60)

    builder = InnovativeSnapshotBuilder()

    # 1. –¢–µ—Å—Ç–∏—Ä—É–µ–º Model 3 –æ—Ç–¥–µ–ª—å–Ω–æ
    builder.test_model3_specific(test_date="2025-01-01")

    # 2. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
    builder.quick_test_builders(test_date="2025-01-01")

    # 3. –ï—Å–ª–∏ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—ã–π –±–∏–ª–¥
    print("\n" + "=" * 60)
    print("üèóÔ∏è STARTING FULL BUILD...")
    print("=" * 60)

    try:
        snapshots = builder.build_all_snapshots(
            train_end="2025-06-01",
            val_end="2025-08-01",
            test_end="2025-09-01",
            window_back_days=30,
            window_forward_days=14
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        builder.save_all_snapshots(snapshots)

        print("\n" + "=" * 60)
        print("‚úÖ –í–°–ï 4 –ù–ê–ë–û–†–ê –î–ê–ù–ù–´–• –ì–û–¢–û–í–´!")
        print("=" * 60)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        for model_name, datasets in snapshots.items():
            print(f"\nüìä {model_name}:")
            for dataset_name, df in datasets.items():
                if not df.empty:
                    print(f"  {dataset_name}: {len(df):,} samples, {len(df.columns)} features")

    except Exception as e:
        print(f"\n‚ùå BUILD FAILED: {str(e)}")
        import traceback

        traceback.print_exc()