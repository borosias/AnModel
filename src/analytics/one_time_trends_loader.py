# one_time_trends_loader.py
import os
import pandas as pd
from datetime import datetime, timedelta
import json
from serpapi import GoogleSearch
from pathlib import Path
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def _parse_results(results: dict, queries: list, region: str) -> list:
    """–ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ SerpAPI —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —Ñ–æ—Ä–º–∞—Ç–æ–º –¥–∞—Ç"""
    trends = []

    def _parse_date(date_str_raw: str):
        """
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤:
          - 'Jun 4, 2025'
          - 'Dec 1‚Äì7, 2024'
          - 'Dec 29, 2024‚ÄìJan 4, 2025'
        –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –¥–∞—Ç—É –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞.
        """
        if not date_str_raw:
            return None

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ç—Ä–∞–Ω–Ω—ã–µ —é–Ω–∏–∫–æ–¥‚Äë—Å–∏–º–≤–æ–ª—ã (—É–∑–∫–∏–π –ø—Ä–æ–±–µ–ª, –¥–ª–∏–Ω–Ω–æ–µ —Ç–∏—Ä–µ –∏ —Ç.–ø.)
        s = (
            date_str_raw
            .replace('\u2009', ' ')  # narrow no‚Äëbreak space
            .replace('\u2011', '-')  # non‚Äëbreaking hyphen
            .replace('\u2012', '-')  # figure dash
            .replace('\u2013', '-')  # en dash
            .replace('\u2014', '-')  # em dash
            .strip()
        )

        # 1) –ü—ã—Ç–∞–µ–º—Å—è –∫–∞–∫ –æ–±—ã—á–Ω—É—é –¥–∞—Ç—É 'Jun 4, 2025'
        try:
            return datetime.strptime(s, '%b %d, %Y').date()
        except ValueError:
            pass

        # 2) –ü–æ–ø—É–ª—è—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç 'Dec 1-7, 2024'
        #    –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é —á–∞—Å—Ç—å –¥–æ –¥–µ—Ñ–∏—Å–∞, –ø–ª—é—Å –≥–æ–¥ —Å–ø—Ä–∞–≤–∞.
        try:
            # s –º–æ–∂–µ—Ç –±—ã—Ç—å 'Dec 1-7, 2024' –∏–ª–∏ 'Dec 29, 2024-Jan 4, 2025'
            # –ë–µ—Ä—ë–º –≥–æ–¥ –∏–∑ –∫–æ–Ω—Ü–∞ —Å—Ç—Ä–æ–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 4 —Ü–∏—Ñ—Ä—ã)
            year = s[-4:]
            if not year.isdigit():
                raise ValueError

            # –ß–∞—Å—Ç—å –¥–æ –∑–∞–ø—è—Ç–æ–π
            if ',' in s:
                left_part = s.split(',', 1)[0]  # 'Dec 1-7' –∏–ª–∏ 'Dec 29, 2024-Jan 4'
            else:
                left_part = s

            # –ï—Å–ª–∏ –µ—Å—Ç—å –¥–µ—Ñ–∏—Å, –±–µ—Ä—ë–º –≤—Å—ë —Å–ª–µ–≤–∞ –æ—Ç –Ω–µ–≥–æ ('Dec 1')
            if '-' in left_part:
                left_part = left_part.split('-', 1)[0].strip()

            candidate = f"{left_part}, {year}"  # 'Dec 1, 2024'
            return datetime.strptime(candidate, '%b %d, %Y').date()
        except Exception:
            logger.warning(f"‚ö†Ô∏è Cannot parse date: {date_str_raw}")
            return None

    try:
        timeline_data = results.get("interest_over_time", {}).get("timeline_data", [])

        print(f"üìÖ Found {len(timeline_data)} timeline entries")

        for day_data in timeline_data:
            date_str = day_data.get("date")
            if not date_str:
                continue

            date = _parse_date(date_str)
            if date is None:
                continue

            # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ –±–∞—Ç—á–µ
            values = day_data.get("values", [])
            print(f"üìà Date: {date}, Values count: {len(values)}")

            for i, query in enumerate(queries):
                if i < len(values):
                    popularity = values[i].get("value", 0)
                    formatted_value = values[i].get("formattedValue", "0")
                else:
                    popularity = 0
                    formatted_value = "0"

                trends.append({
                    'query': query,
                    'region': region,
                    'date': date,
                    'popularity': int(popularity) if popularity else 0,
                    'formatted_value': formatted_value,
                    'loaded_at': datetime.now()
                })

    except Exception as e:
        logger.error(f"‚ùå Parse error for {queries} in {region}: {e}")
        import traceback
        traceback.print_exc()

    return trends


class OneTimeTrendsLoader:
    def __init__(self, storage_path: str = "./trends_data"):
        self.api_key = os.getenv("SERPAPI_KEY", "e0905b78baa8db75444b707477b51b353782a44bd35d7fde188817b55fb89d45")
        if not self.api_key:
            raise ValueError("SERPAPI_KEY not found!")

        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)

        self.queries = ["shoes", "blanket", "phone", "charger", "jacket", "backpack", "watch"]
        self.regions = ["UA-30", "UA-40", "UA-50"]

    def load_all_trends_once(self, days_back: int = 365):
        """–û–¥–∏–Ω —Ä–∞–∑ –∑–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ —Ç—Ä–µ–Ω–¥—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet"""

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –ª–∏ —É–∂–µ
        master_file = self.storage_path / "trends_master.parquet"
        if master_file.exists():
            logger.info("Trends already loaded! Reading from existing file...")
            return pd.read_parquet(master_file)

        logger.info(f"Loading trends for {days_back} days...")

        all_trends = []
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        date_range = f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}"

        for region in self.regions:
            for i in range(0, len(self.queries), 5):  # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ 5 –∑–∞–ø—Ä–æ—Å–æ–≤
                batch_queries = self.queries[i:i + 5]
                queries_str = ",".join(batch_queries)

                logger.info(f"Loading {queries_str} for {region}...")

                try:
                    params = {
                        "engine": "google_trends",
                        "q": queries_str,
                        "geo": region,
                        "data_type": "TIMESERIES",
                        "date": date_range,
                        "api_key": self.api_key
                    }

                    search = GoogleSearch(params)
                    results = search.get_dict()

                    print(f"üìä Raw API response keys: {results.keys()}")

                    trends_data = _parse_results(results, batch_queries, region)
                    all_trends.extend(trends_data)

                    logger.info(f"‚úÖ Got {len(trends_data)} records for this batch")

                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                    time.sleep(2)

                except Exception as e:
                    logger.error(f"Failed for {queries_str} in {region}: {e}")
                    continue

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ parquet
        if all_trends:
            df = pd.DataFrame(all_trends)
            df = self._add_features(df)

            # –ù–û–†–ú–ê–õ–ò–ó–£–ï–ú –°–•–ï–ú–£ –î–õ–Ø parquet
            df['date'] = pd.to_datetime(df['date']).dt.normalize()
            df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').fillna(0).astype(int)

            # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –±–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç
            base_cols = ['date', 'query', 'region', 'popularity']
            for col in base_cols:
                if col not in df.columns:
                    logger.warning(f"Missing column '{col}' in trends data ‚Äî adding empty")
                    df[col] = None

            master_file = self.storage_path / "trends_master.parquet"
            df.to_parquet(master_file, index=False)
            logger.info(f"‚úÖ Saved {len(df)} records to {master_file}")

            return df
        else:
            logger.error("‚ùå No data loaded!")
            return pd.DataFrame()

    def _add_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª—è–µ–º ML —Ñ–∏—á–∏"""
        if df.empty:
            return df

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –æ–∫–æ–Ω–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π
        df = df.sort_values(['query', 'region', 'date'])

        result_dfs = []

        for (query, region), group in df.groupby(['query', 'region']):
            group = group.copy().sort_values('date')

            # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
            group['popularity_7d_avg'] = group['popularity'].rolling(7, min_periods=1).mean()
            group['popularity_30d_avg'] = group['popularity'].rolling(30, min_periods=1).mean()

            # –ò–∑–º–µ–Ω–µ–Ω–∏—è
            group['popularity_change_1d'] = group['popularity'].pct_change().fillna(0)
            group['popularity_change_7d'] = group['popularity'].pct_change(7).fillna(0)

            # –ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–¥ (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ø–µ—Ä–≤—ã–º –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–º)
            if len(group) > 1:
                group['trend_slope'] = (group['popularity'].iloc[-1] - group['popularity'].iloc[0]) / len(group)
            else:
                group['trend_slope'] = 0

            result_dfs.append(group)

        return pd.concat(result_dfs, ignore_index=True)

    def get_trends_data(self) -> pd.DataFrame:
        """–ü—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ parquet"""
        master_file = self.storage_path / "trends_master.parquet"

        if not master_file.exists():
            logger.warning("No trends data found! Run load_all_trends_once() first.")
            return pd.DataFrame()

        return pd.read_parquet(master_file)


# –ü—Ä–æ—Å—Ç–æ–π usage
if __name__ == "__main__":
    print("üöÄ Starting trends loader...")

    # –ó–∞—Ç–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É:
    loader = OneTimeTrendsLoader()
    df = loader.load_all_trends_once(days_back=365)

    if not df.empty:
        print(f"‚úÖ Successfully loaded {len(df)} trend records")
        print(f"üìä Data shape: {df.shape}")
        print(f"üìÖ Date range: {df['date'].min()} to {df['date'].max()}")
        print(f"üîç Queries: {df['query'].unique().tolist()}")
        print(f"üåç Regions: {df['region'].unique().tolist()}")

        # –ü–æ–∫–∞–∂–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
        print("\nüìã Sample data:")
        print(df.head(10))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_file = Path("./trends_data/loading_stats.json")
        stats = {
            'loaded_at': datetime.now().isoformat(),
            'total_records': len(df),
            'date_range': {
                'start': df['date'].min().isoformat(),
                'end': df['date'].max().isoformat()
            },
            'queries': df['query'].unique().tolist(),
            'regions': df['region'].unique().tolist()
        }

        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)

        print("üìä Stats saved to loading_stats.json")
    else:
        print("‚ùå Failed to load trends data")
        print("üí° Try running debug_load() first to see what's happening")
