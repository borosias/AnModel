# one_time_trends_loader.py
import os
import pandas as pd
from datetime import datetime, timedelta
import json
from serpapi import GoogleSearch
from pathlib import Path
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class OneTimeTrendsLoader:
    def __init__(self, storage_path: str = "./trends_data"):
        self.api_key = os.getenv("SERPAPI_KEY","e0905b78baa8db75444b707477b51b353782a44bd35d7fde188817b55fb89d45")
        if not self.api_key:
            raise ValueError("SERPAPI_KEY not found!")

        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)

        self.queries = ["shoes", "blanket", "phone", "charger", "jacket", "backpack", "watch"]
        self.regions = ["UA-30", "UA-40", "UA-50"]

    def load_all_trends_once(self, days_back: int = 365):
        """ĞĞ´Ğ¸Ğ½ Ñ€Ğ°Ğ· Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ²ÑĞµ Ñ‚Ñ€ĞµĞ½Ğ´Ñ‹ Ğ¸ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² parquet"""

        # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ½Ğµ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ Ğ»Ğ¸ ÑƒĞ¶Ğµ
        master_file = self.storage_path / "trends_master.parquet"
        if master_file.exists():
            logger.info("Trends already loaded! Reading from existing file...")
            return pd.read_parquet(master_file)

        logger.info(f"Loading trends for {days_back} days...")

        all_trends = []
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)

        date_range = f"{start_date.strftime('%Y-%m-%d')} {end_date.strftime('%Y-%m-%d')}"

        for region in self.regions:
            for i in range(0, len(self.queries), 5):  # Ğ“Ñ€ÑƒĞ¿Ğ¿Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿Ğ¾ 5 Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ¾Ğ²
                batch_queries = self.queries[i:i + 5]
                queries_str = ",".join(batch_queries)

                logger.info(f"Loading {queries_str} for {region}...")

                try:
                    params = {
                        "engine": "google_trends",
                        "q": queries_str,
                        "geo": region,
                        "data_type": "TIMESERIES",
                        "date": date_range,
                        "api_key": self.api_key
                    }

                    search = GoogleSearch(params)
                    results = search.get_dict()

                    print(f"ğŸ“Š Raw API response keys: {results.keys()}")

                    trends_data = self._parse_results(results, batch_queries, region)
                    all_trends.extend(trends_data)

                    logger.info(f"âœ… Got {len(trends_data)} records for this batch")

                    # ĞŸĞ°ÑƒĞ·Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸
                    time.sleep(2)

                except Exception as e:
                    logger.error(f"Failed for {queries_str} in {region}: {e}")
                    continue

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² parquet
        if all_trends:
            df = pd.DataFrame(all_trends)
            df = self._add_features(df)

            # ĞĞĞ ĞœĞĞ›Ğ˜Ğ—Ğ£Ğ•Ğœ Ğ¡Ğ¥Ğ•ĞœĞ£ Ğ”Ğ›Ğ¯ parquet
            df['date'] = pd.to_datetime(df['date']).dt.normalize()
            df['popularity'] = pd.to_numeric(df['popularity'], errors='coerce').fillna(0).astype(int)

            # Ğ£Ğ±ĞµĞ´Ğ¸Ğ¼ÑÑ, Ñ‡Ñ‚Ğ¾ Ğ±Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğµ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸ Ğ¿Ñ€Ğ¸ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚
            base_cols = ['date', 'query', 'region', 'popularity']
            for col in base_cols:
                if col not in df.columns:
                    logger.warning(f"Missing column '{col}' in trends data â€” adding empty")
                    df[col] = None

            master_file = self.storage_path / "trends_master.parquet"
            df.to_parquet(master_file, index=False)
            logger.info(f"âœ… Saved {len(df)} records to {master_file}")

            return df
        else:
            logger.error("âŒ No data loaded!")
            return pd.DataFrame()

    def _parse_results(self, results: dict, queries: list, region: str) -> list:
        """ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¸Ğ· SerpAPI Ñ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ¼ Ğ´Ğ°Ñ‚"""
        trends = []

        try:
            timeline_data = results.get("interest_over_time", {}).get("timeline_data", [])

            print(f"ğŸ“… Found {len(timeline_data)} timeline entries")

            for day_data in timeline_data:
                date_str = day_data.get("date")
                if not date_str:
                    continue

                # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ´Ğ°Ñ‚Ñƒ Ğ² Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğµ "Jun 4, 2025"
                try:
                    date = datetime.strptime(date_str, '%b %d, %Y').date()
                except ValueError:
                    logger.warning(f"âš ï¸ Cannot parse date: {date_str}")
                    continue

                # Ğ”Ğ»Ñ ĞºĞ°Ğ¶Ğ´Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ° Ğ² Ğ±Ğ°Ñ‚Ñ‡Ğµ
                values = day_data.get("values", [])
                print(f"ğŸ“ˆ Date: {date}, Values count: {len(values)}")

                for i, query in enumerate(queries):
                    if i < len(values):
                        popularity = values[i].get("value", 0)
                        formatted_value = values[i].get("formattedValue", "0")
                    else:
                        popularity = 0
                        formatted_value = "0"

                    trends.append({
                        'query': query,
                        'region': region,
                        'date': date,
                        'popularity': int(popularity) if popularity else 0,
                        'formatted_value': formatted_value,
                        'loaded_at': datetime.now()
                    })

        except Exception as e:
            logger.error(f"âŒ Parse error for {queries} in {region}: {e}")
            import traceback
            traceback.print_exc()

        return trends

    def _add_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ML Ñ„Ğ¸Ñ‡Ğ¸"""
        if df.empty:
            return df

        # Ğ¡Ğ¾Ñ€Ñ‚Ğ¸Ñ€ÑƒĞµĞ¼ Ğ´Ğ»Ñ Ğ¾ĞºĞ¾Ğ½Ğ½Ñ‹Ñ… Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ğ¹
        df = df.sort_values(['query', 'region', 'date'])

        result_dfs = []

        for (query, region), group in df.groupby(['query', 'region']):
            group = group.copy().sort_values('date')

            # Ğ¡ĞºĞ¾Ğ»ÑŒĞ·ÑÑ‰Ğ¸Ğµ ÑÑ€ĞµĞ´Ğ½Ğ¸Ğµ
            group['popularity_7d_avg'] = group['popularity'].rolling(7, min_periods=1).mean()
            group['popularity_30d_avg'] = group['popularity'].rolling(30, min_periods=1).mean()

            # Ğ˜Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ
            group['popularity_change_1d'] = group['popularity'].pct_change().fillna(0)
            group['popularity_change_7d'] = group['popularity'].pct_change(7).fillna(0)

            # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ‚Ñ€ĞµĞ½Ğ´ (Ñ€Ğ°Ğ·Ğ½Ğ¸Ñ†Ğ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¼ Ğ¸ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¼)
            if len(group) > 1:
                group['trend_slope'] = (group['popularity'].iloc[-1] - group['popularity'].iloc[0]) / len(group)
            else:
                group['trend_slope'] = 0

            result_dfs.append(group)

        return pd.concat(result_dfs, ignore_index=True)

    def get_trends_data(self) -> pd.DataFrame:
        """ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· parquet"""
        master_file = self.storage_path / "trends_master.parquet"

        if not master_file.exists():
            logger.warning("No trends data found! Run load_all_trends_once() first.")
            return pd.DataFrame()

        return pd.read_parquet(master_file)


# ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ usage
if __name__ == "__main__":
    print("ğŸš€ Starting trends loader...")

    # Ğ—Ğ°Ñ‚ĞµĞ¼ Ğ¾ÑĞ½Ğ¾Ğ²Ğ½ÑƒÑ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºÑƒ:
    loader = OneTimeTrendsLoader()
    df = loader.load_all_trends_once(days_back=180)

    if not df.empty:
        print(f"âœ… Successfully loaded {len(df)} trend records")
        print(f"ğŸ“Š Data shape: {df.shape}")
        print(f"ğŸ“… Date range: {df['date'].min()} to {df['date'].max()}")
        print(f"ğŸ” Queries: {df['query'].unique().tolist()}")
        print(f"ğŸŒ Regions: {df['region'].unique().tolist()}")

        # ĞŸĞ¾ĞºĞ°Ğ¶ĞµĞ¼ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…
        print("\nğŸ“‹ Sample data:")
        print(df.head(10))

        # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºÑƒ
        stats_file = Path("./trends_data/loading_stats.json")
        stats = {
            'loaded_at': datetime.now().isoformat(),
            'total_records': len(df),
            'date_range': {
                'start': df['date'].min().isoformat(),
                'end': df['date'].max().isoformat()
            },
            'queries': df['query'].unique().tolist(),
            'regions': df['region'].unique().tolist()
        }

        with open(stats_file, 'w') as f:
            json.dump(stats, f, indent=2)

        print("ğŸ“Š Stats saved to loading_stats.json")
    else:
        print("âŒ Failed to load trends data")
        print("ğŸ’¡ Try running debug_load() first to see what's happening")