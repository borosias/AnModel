import os
import glob
import logging
from bisect import bisect_right
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Import the micro‚Äëtrend and sequence modules.  These imports are placed at
# module level to avoid repeated initialisation inside tight loops.
from src.models.models.micro_trend import MicroTrend
from src.models.models.sequence_model import SequenceModel

EVENTS_DIR = "../data/parquet"
OUT_DIR = "../data/snapshots/model1"
HORIZON_DAYS = 7
TRAIN_RATIO = 0.7
VAL_RATIO = 0.15
TEST_RATIO = 0.15
TRENDS_PATH = "../trends_data/trends_master.parquet"  # –ø—É—Ç—å –∫ —Ç—Ä–µ–Ω–¥–∞–º

os.makedirs(OUT_DIR, exist_ok=True)


def _print_progress(current, total, prefix=""):  # –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä –±–µ–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    """–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –≤ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–µ. current –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 0 –∏–ª–∏ 1.

    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è total == 0 (–Ω–∏—á–µ–≥–æ –Ω–µ –≤—ã–≤–æ–¥–∏—Ç).
    """
    if not total:
        return
    # –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º current –≤ –¥–∏–∞–ø–∞–∑–æ–Ω [0..total]
    current = max(0, min(current, total))
    bar_len = 30
    filled = int(bar_len * current / total)
    bar = "‚ñà" * filled + "-" * (bar_len - filled)
    percent = (current / total) * 100
    print(f"\r{prefix} [{bar}] {current}/{total} ({percent:.0f}%)", end="", flush=True)
    if current >= total:
        print()  # –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏


def _get_logger():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–æ–≥–≥–µ—Ä –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å–Ω–∞–ø—à–æ—Ç–æ–≤.

    –õ–æ–≥–∏ –ø–∏—à—É—Ç—Å—è –∫–∞–∫ –≤ –∫–æ–Ω—Å–æ–ª—å, —Ç–∞–∫ –∏ –≤ —Ñ–∞–π–ª OUT_DIR/snapshot_builder1.log
    """
    logger = logging.getLogger("snapshot_builder1")
    if logger.handlers:
        return logger

    logger.setLevel(logging.INFO)

    fmt = logging.Formatter(
        fmt="%(asctime)s | %(levelname)s | %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )

    # –ö–æ–Ω—Å–æ–ª—å
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch.setFormatter(fmt)
    logger.addHandler(ch)

    # –§–∞–π–ª
    try:
        log_path = os.path.join(OUT_DIR, "snapshot_builder1.log")
        fh = logging.FileHandler(log_path, encoding="utf-8")
        fh.setLevel(logging.INFO)
        fh.setFormatter(fmt)
        logger.addHandler(fh)
    except Exception as e:
        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å —Ñ–∞–π–ª, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Å –∫–æ–Ω—Å–æ–ª—å—é
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä: {e}")

    return logger


def load_events(events_dir):
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ–±—ã—Ç–∏–π"""
    logger = _get_logger()
    files = sorted(glob.glob(os.path.join(events_dir, "events_part_*.parquet")))
    if not files:
        raise FileNotFoundError(f"No parquet files in {events_dir}")

    # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
    parts = []
    total_files = len(files)
    for i, fpath in enumerate(files, start=1):
        try:
            parts.append(pd.read_parquet(fpath))
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {fpath}: {e}")
            raise
        _print_progress(i, total_files, prefix="–ó–∞–≥—Ä—É–∑–∫–∞ parquet —Ñ–∞–π–ª–æ–≤")
    df = pd.concat(parts, ignore_index=True)

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
    df["ts"] = pd.to_datetime(df["ts"], utc=True)
    df = df.sort_values("ts")

    df["date"] = df["ts"].dt.date
    if 'price' in df.columns:
        df["price"] = pd.to_numeric(df["price"], errors="coerce").fillna(0.0)

    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å–æ–±—ã—Ç–∏–π")
    logger.info(f"–ü–µ—Ä–∏–æ–¥: {df['date'].min()} - {df['date'].max()}")

    return df

def load_trends(trends_path: str = TRENDS_PATH) -> pd.DataFrame:
    logger = _get_logger()

    if not os.path.exists(trends_path):
        logger.info(f"–§–∞–π–ª —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {trends_path} ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ç—Ä–µ–Ω–¥‚Äë—Ñ–∏—á")
        return pd.DataFrame()

    try:
        df = pd.read_parquet(trends_path)
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ç—Ä–µ–Ω–¥—ã –∏–∑ {trends_path}: {e}")
        return pd.DataFrame()

    if df.empty:
        logger.info(f"–§–∞–π–ª —Ç—Ä–µ–Ω–¥–æ–≤ –ø—É—Å—Ç–æ–π: {trends_path} ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ç—Ä–µ–Ω–¥‚Äë—Ñ–∏—á")
        return pd.DataFrame()

    if "date" not in df.columns or "popularity" not in df.columns:
        logger.warning("–í trends_master.parquet –Ω–µ—Ç –∫–æ–ª–æ–Ω–æ–∫ 'date' –∏ 'popularity' ‚Äî —Ç—Ä–µ–Ω–¥—ã –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è")
        return pd.DataFrame()

    # 1. –ü—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã
    df["date"] = pd.to_datetime(df["date"]).dt.normalize()
    df["popularity"] = pd.to_numeric(df["popularity"], errors="coerce")

    # 2. –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –ø–æ —Ç–µ–º –¥–∞—Ç–∞–º, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å (–Ω–µ–¥–µ–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏)
    daily = (
        df.dropna(subset=["popularity"])
        .groupby("date")["popularity"]
        .agg(
            trend_popularity_mean="mean",
            trend_popularity_max="max",
        )
        .sort_index()
    )

    if daily.empty:
        logger.info("–¢—Ä–µ–Ω–¥—ã –ø–æ—Å–ª–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø—É—Å—Ç—ã–µ ‚Äî –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Ç—Ä–µ–Ω–¥‚Äë—Ñ–∏—á")
        return pd.DataFrame()

    # 3. –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ–º –¥–æ —Å–ø–ª–æ—à–Ω–æ–≥–æ –¥–Ω–µ–≤–Ω–æ–≥–æ —Ä—è–¥–∞ –∏ —Ç—è–Ω–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤–ø–µ—Ä—ë–¥
    full_index = pd.date_range(daily.index.min(), daily.index.max(), freq="D")
    daily = daily.reindex(full_index).ffill()

    # –ò–Ω–¥–µ–∫—Å –¥–µ–ª–∞–µ–º —Ç–∏–ø–æ–º date, —á—Ç–æ–±—ã —Å–æ–≤–ø–∞–¥–∞–ª —Å–æ snapshot_date
    daily.index = daily.index.date

    logger.info(
        f"–ó–∞–≥—Ä—É–∂–µ–Ω—ã —Ç—Ä–µ–Ω–¥—ã: {len(daily)} –¥–Ω–µ–π "
        f"({daily.index.min()} - {daily.index.max()})"
    )

    return daily


def build_snapshots_simple(df, horizon_days=HORIZON_DAYS, trends_daily: pd.DataFrame | None = None):
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ snapshots.

    –í–ê–ñ–ù–û: –î–ª—è –æ–±—É—á–µ–Ω–∏—è –ù–ï –≤–∫–ª—é—á–∞–µ–º snapshots –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ HORIZON_DAYS –¥–Ω–µ–π,
    —Ç.–∫. –¥–ª—è –Ω–∏—Ö –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –±—É–¥—É—â–∏—Ö –ø–æ–∫—É–ø–∫–∞—Ö (—Ç–∞—Ä–≥–µ—Ç –±—É–¥–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ = 0).
    """
    logger = _get_logger()

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤
    trends_dict: dict = {}
    if trends_daily is not None and not trends_daily.empty:
        trends_dict = trends_daily.to_dict(orient="index")
        logger.info(f"–¢—Ä–µ–Ω–¥‚Äë—Ñ–∏—á–∏ –±—É–¥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –¥–ª—è {len(trends_dict)} –¥–Ω–µ–π")

    # Initialise micro‚Äëtrend calculator and sequence model once per build.  The
    # sequence model needs to learn the mapping of event types before it can
    # encode histories.  We derive the mapping from the full set of event
    # types present in the dataset.
    micro_calc = MicroTrend()
    seq_model = SequenceModel()
    # Fit the sequence model on all unique event types.  The stable order of
    # unique types ensures consistent column ordering across snapshots.
    if 'event_type' in df.columns:
        unique_types = df['event_type'].dropna().astype(str).unique().tolist()
        seq_model.fit(unique_types)
    else:
        # If no event types, leave model unfitted; encoding will return zeros.
        unique_types = []

    all_dates = sorted(df['date'].unique())
    max_date = all_dates[-1]

    # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—Ç—Å–µ–∫–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ HORIZON_DAYS –¥–Ω–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ---
    # –î–ª—è —ç—Ç–∏—Ö –¥–Ω–µ–π —É –Ω–∞—Å –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –±—É–¥—É—â–∏—Ö –ø–æ–∫—É–ø–∫–∞—Ö
    cutoff_date = max_date - timedelta(days=horizon_days)
    train_dates = [d for d in all_dates if d <= cutoff_date]

    logger.info(f"–í—Å–µ–≥–æ –¥–Ω–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö: {len(all_dates)} ({all_dates[0]} - {max_date})")
    logger.info(f"–î–Ω–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (—Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º –±—É–¥—É—â–∏–º): {len(train_dates)} (–¥–æ {cutoff_date})")
    logger.info(f"–û—Ç—Å–µ—á–µ–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö {horizon_days} –¥–Ω–µ–π ‚Äî –¥–ª—è –Ω–∏—Ö —Ç–∞—Ä–≥–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–µ–Ω")

    if not train_dates:
        logger.warning("–ù–µ—Ç –¥–∞—Ç —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º –±—É–¥—É—â–∏–º! –£–≤–µ–ª–∏—á—å—Ç–µ –ø–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö.")
        return pd.DataFrame()

    has_item = 'item_id' in df.columns
    has_region = 'region' in df.columns

    users = df['user_id'].unique()
    n_users = len(users)
    logger.info(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {n_users}")

    snapshots = []

    for u_idx, user_id in enumerate(users, start=1):
        # –ë–µ—Ä–µ–º –≤—Å–µ —Å–æ–±—ã—Ç–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_events = df[df['user_id'] == user_id].sort_values('ts')
        if user_events.empty:
            continue

        first_ts = user_events['ts'].iloc[0]

        # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –¥–∞—Ç—ã —Å–æ–±—ã—Ç–∏–π –≤ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –≤ –æ–∫–Ω–µ
        event_dates = user_events['date'].values
        event_prices = user_events['price'].fillna(0.0).values
        event_types = user_events['event_type'].values

        # –ö—ç—à–∏—Ä—É–µ–º –ø–æ–∫—É–ø–∫–∏ –¥–ª—è —Ç–∞—Ä–≥–µ—Ç–∞
        purchases_mask = (event_types == 'purchase')
        purchases_df = user_events[purchases_mask]

        purchases_ord = []
        purchase_amounts = []
        if not purchases_df.empty:
            purchases_ord = [d.toordinal() for d in purchases_df['date'].tolist()]
            purchase_amounts = purchases_df['price'].fillna(0.0).astype(float).tolist()

        # –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –¥–Ω—è–º (–¥–ª—è –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã—Ö —Å—á–µ—Ç—á–∏–∫–æ–≤)
        per_day_counts = (
            user_events.groupby('date')
            .agg(
                total_events=('event_type', 'size'),
                total_clicks=('event_type', lambda s: (s == 'click').sum()),
                total_purchases=('event_type', lambda s: (s == 'purchase').sum()),
                total_spent=('price', lambda s: s.fillna(0.0).sum()),
            )
            .sort_index()
        )

        # Last values helper
        last_rows_per_day = user_events.groupby('date').tail(1).set_index('date')

        # Set of items helper
        items_per_day = pd.Series(dtype=object)
        if has_item:
            items_per_day = user_events.dropna(subset=['item_id']).groupby('date')['item_id'].apply(set)

        user_active_days = list(per_day_counts.index)

        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–≤—ã–π –¥–µ–Ω—å —é–∑–µ—Ä–∞ –≤ train_dates
        first_user_day = user_active_days[0]
        start_idx = np.searchsorted(train_dates, first_user_day, side='left')

        # –ö—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        cum = {
            "events": 0, "clicks": 0, "purchases": 0, "spent": 0.0, "days": 0
        }
        seen_items = set()
        last_vals = {
            "type": None, "region": None, "item": None, "ts": None
        }

        pd_ptr = 0
        n_user_days = len(user_active_days)

        # --- –ò–ó–ú–ï–ù–ï–ù–ò–ï: –∏—Ç–µ—Ä–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–æ train_dates (–±–µ–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö HORIZON_DAYS) ---
        for d in train_dates[start_idx:]:
            # 1. –û–±–Ω–æ–≤–ª—è–µ–º –∫—É–º—É–ª—è—Ç–∏–≤–Ω—ã–µ (–∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ) –¥–∞–Ω–Ω—ã–µ
            while pd_ptr < n_user_days and user_active_days[pd_ptr] <= d:
                day = user_active_days[pd_ptr]
                agg = per_day_counts.loc[day]

                cum["events"] += int(agg['total_events'])
                cum["clicks"] += int(agg['total_clicks'])
                cum["purchases"] += int(agg['total_purchases'])
                cum["spent"] += float(agg['total_spent'])
                cum["days"] += 1

                if day in last_rows_per_day.index:
                    lr = last_rows_per_day.loc[day]
                    last_vals["type"] = lr.get('event_type')
                    last_vals["ts"] = lr.get('ts')
                    if has_region: last_vals["region"] = lr.get('region')
                    if has_item: last_vals["item"] = lr.get('item_id')

                if has_item and day in items_per_day.index:
                    seen_items.update(items_per_day.loc[day])

                pd_ptr += 1

            if cum["events"] == 0:
                continue

            # 2. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º ROLLING WINDOWS
            mask_7d = (event_dates > d - timedelta(days=7)) & (event_dates <= d)
            mask_30d = (event_dates > d - timedelta(days=30)) & (event_dates <= d)

            events_last_7d = np.sum(mask_7d)
            events_last_30d = np.sum(mask_30d)

            purchases_last_30d = np.sum((event_types[mask_30d] == 'purchase'))
            spent_last_30d = np.sum(event_prices[mask_30d])

            # 3. –î–∞—Ç—ã
            snapshot_datetime = pd.Timestamp(datetime.combine(d, datetime.max.time())).tz_localize('UTC')
            days_since_first = (snapshot_datetime - first_ts).days if first_ts else 999
            days_since_last = (snapshot_datetime - last_vals["ts"]).days if last_vals["ts"] else 0

            # 4. –¢–∞—Ä–≥–µ—Ç (Will Purchase) ‚Äî —Ç–µ–ø–µ—Ä—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π, —Ç.–∫. d <= cutoff_date
            will_purchase = 0
            days_to_next = 999
            next_amount = 0.0
            if purchases_ord:
                s_ord = d.toordinal()
                idx = bisect_right(purchases_ord, s_ord)
                if idx < len(purchases_ord):
                    delta = purchases_ord[idx] - s_ord
                    if 0 < delta <= horizon_days:
                        will_purchase = 1
                        days_to_next = delta
                        next_amount = float(purchase_amounts[idx])

            row = {
                "snapshot_date": d,
                "user_id": user_id,
                # Cumulative
                "total_events": int(cum["events"]),
                "unique_days": int(cum["days"]),
                "total_clicks": int(cum["clicks"]),
                "total_purchases": int(cum["purchases"]),
                "total_spent": float(cum["spent"]),
                "distinct_items": int(len(seen_items)),
                # Rolling
                "events_last_7d": int(events_last_7d),
                "events_last_30d": int(events_last_30d),
                "purchases_last_30d": int(purchases_last_30d),
                "spent_last_30d": float(spent_last_30d),
                # Derived
                "conversion_rate_30d": float(purchases_last_30d / max(1, events_last_30d)),
                "avg_order_value_30d": float(spent_last_30d / max(1, purchases_last_30d)),
                "purchase_frequency": float(cum["purchases"] / max(1, cum["days"])),
                "avg_spend_per_event": float(cum["spent"] / max(1, cum["events"])),
                # Recency
                "days_since_first": int(days_since_first),
                "days_since_last": int(days_since_last),
                "events_per_day": float(cum["events"] / max(1, cum["days"])),
                "recency_score": float(1.0 / (1.0 + days_since_last)),
                # Last Context
                "last_event_type": last_vals["type"],
                "last_region": last_vals["region"],
                "last_item": last_vals["item"],
                # Targets
                "will_purchase_next_7d": int(will_purchase),
                "days_to_next_purchase": int(days_to_next),
                "next_purchase_amount": float(next_amount),
            }

            # 5. Micro‚Äëtrend features
            try:
                # Use the user's per‚Äëevent arrays for micro‚Äëtrend calculations.  Note
                # that ``event_dates`` comes from the date column (not ts) so that
                # day‚Äëlevel windows match snapshot boundaries.  ``event_prices`` is
                # optional and may be ``None`` if the price column is absent.
                event_dates_arr = event_dates  # np.ndarray of ``date`` values
                event_prices_arr = event_prices if 'price' in user_events.columns else None
                micro_feats = micro_calc.compute(
                    event_dates_arr, event_types, event_prices_arr, d
                )
                row.update(micro_feats)
            except Exception as ex:
                # In case of any unexpected error during micro feature calculation
                logger.warning(f"Micro‚Äëtrend calculation failed for user {user_id} on {d}: {ex}")

            # 6. Sequence embedding features
            try:
                # Encode the user‚Äôs history up to the snapshot as a distribution over
                # event types.  Convert the timestamp series to numpy datetime64 for
                # comparison.  ``snapshot_datetime`` is converted to datetime64
                # automatically by numpy.
                ts_np = user_events['ts'].values.astype('datetime64[ns]')
                snapshot_np = np.datetime64(snapshot_datetime.to_pydatetime())
                seq_emb = seq_model.encode_history(event_types.astype(str), ts_np, snapshot_np)
                # Expand the embedding into named columns.  Use a prefix that makes
                # it clear these are sequential features.
                for idx, val in enumerate(seq_emb):
                    row[f"seq_emb_{idx}"] = float(val)
            except Exception as ex:
                logger.warning(f"Sequence embedding failed for user {user_id} on {d}: {ex}")

            if trends_dict:
                t = trends_dict.get(d)
                if t:
                    row.update(t)

            snapshots.append(row)

        _print_progress(u_idx, n_users, prefix="–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ snapshots")

    if snapshots:
        result = pd.DataFrame(snapshots)
        logger.info(f"\n–°–æ–∑–¥–∞–Ω–æ {len(result)} snapshots")
        pos = result['will_purchase_next_7d'].sum()
        logger.info(f"–ü–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö: {pos} ({pos / len(result):.2%})")
        return result
    else:
        return pd.DataFrame()

def split_and_save_simple(snaps_df, out_dir):
    """–ü—Ä–æ—Å—Ç–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ"""
    logger = _get_logger()
    if snaps_df.empty:
        logger.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
        return

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∞—Ç–µ
    snaps_df = snaps_df.sort_values('snapshot_date')

    # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã
    unique_dates = snaps_df['snapshot_date'].unique()
    n_dates = len(unique_dates)

    # –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è
    train_end_idx = int(n_dates * TRAIN_RATIO)
    val_end_idx = train_end_idx + int(n_dates * VAL_RATIO)

    # –ì—Ä–∞–Ω–∏—Ü—ã –¥–∞—Ç
    train_dates = unique_dates[:train_end_idx]
    val_dates = unique_dates[train_end_idx:val_end_idx]
    test_dates = unique_dates[val_end_idx:]

    # –†–∞–∑–¥–µ–ª—è–µ–º
    train_df = snaps_df[snaps_df['snapshot_date'].isin(train_dates)].copy()
    val_df = snaps_df[snaps_df['snapshot_date'].isin(val_dates)].copy()
    test_df = snaps_df[snaps_df['snapshot_date'].isin(test_dates)].copy()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    train_df.to_parquet(os.path.join(out_dir, 'train.parquet'), index=False)
    val_df.to_parquet(os.path.join(out_dir, 'val.parquet'), index=False)
    test_df.to_parquet(os.path.join(out_dir, 'test.parquet'), index=False)

    logger.info(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ:")
    logger.info(f"  Train: {len(train_df)} —Å—Ç—Ä–æ–∫")
    logger.info(f"  Val:   {len(val_df)} —Å—Ç—Ä–æ–∫")
    logger.info(f"  Test:  {len(test_df)} —Å—Ç—Ä–æ–∫")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    for name, df in [("Train", train_df), ("Val", val_df), ("Test", test_df)]:
        pos = df['will_purchase_next_7d'].sum()
        total = len(df)
        logger.info(f"  {name}: {pos}/{total} ({pos / max(1, total):.2%}) –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    logger = _get_logger()
    logger.info("–ü—Ä–æ—Å—Ç–æ–π –±–∏–ª–¥–µ—Ä snapshots (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)")
    logger.info("=" * 50)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    logger.info("\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    df = load_events(EVENTS_DIR)

    # 1.1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—Ä–µ–Ω–¥—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
    logger.info("\n1.1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–¥–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)...")
    trends_daily = load_trends(TRENDS_PATH)

    # 2. –°—Ç—Ä–æ–∏–º snapshots
    logger.info("\n2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ snapshots...")
    snaps = build_snapshots_simple(df, horizon_days=HORIZON_DAYS, trends_daily=trends_daily)

    # --- –ù–û–í–û–ï: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö ---
    if not snaps.empty:
        logger.info("\nüìä –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
        pos_rate = snaps['will_purchase_next_7d'].mean()
        logger.info(f"  Positive rate: {pos_rate:.2%}")

        pos_mask = snaps['will_purchase_next_7d'] == 1
        neg_mask = snaps['will_purchase_next_7d'] == 0

        for col in ['events_last_7d', 'events_last_30d', 'conversion_rate_30d', 'recency_score']:
            if col in snaps.columns:
                pos_mean = snaps.loc[pos_mask, col].mean()
                neg_mean = snaps.loc[neg_mask, col].mean()
                logger.info(f"  {col}: pos={pos_mean:.3f}, neg={neg_mean:.3f}, diff={pos_mean - neg_mean:.3f}")

        if pos_rate < 0.01:
            logger.warning("‚ö†Ô∏è Positive rate < 1%! –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω—É–ª–∏.")
            logger.warning("   –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —É–≤–µ–ª–∏—á—å—Ç–µ purchase probability –≤ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–µ –∏–ª–∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç–µ span_days.")
        elif pos_rate < 0.03:
            logger.warning("‚ö†Ô∏è Positive rate < 3%. –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–º.")

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º
    logger.info("\n3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ...")
    if not snaps.empty:
        split_and_save_simple(snaps, OUT_DIR)
        logger.info(f"\n–ì–æ—Ç–æ–≤–æ! –î–∞–Ω–Ω—ã–µ –≤ {OUT_DIR}")
    else:
        print("–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å snapshots")


if __name__ == "__main__":
    main()