import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeClassifier, export_text
from src.models.models.context_aware import ContextAwareModel
import shap

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
pd.set_option("display.max_columns", None)
pd.set_option("display.width", 200)
pd.set_option("display.float_format", '{:.2f}'.format)
plt.style.use('seaborn-v0_8')

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model = ContextAwareModel.load("./src/models/production_models/context_aware_model1.pkl")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–ø–∫—É —Å–æ —Å–Ω–∞–ø—à–æ—Ç–∞–º–∏ (–∞–≤—Ç–æ–ø–æ–∏—Å–∫)
SNAPSHOT_BASE = os.path.join(BASE_DIR, "src", "analytics", "data", "daily_features")
snapshots = sorted([d for d in os.listdir(SNAPSHOT_BASE) if d.startswith("snapshot_")])
if not snapshots:
    raise FileNotFoundError("–ù–µ—Ç —Å–Ω–∞–ø—à–æ—Ç–æ–≤ –≤ data/daily_features")
SNAPSHOT_DIR = os.path.join(SNAPSHOT_BASE, snapshots[-1])

print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {SNAPSHOT_DIR}")


def load_dataset(name: str) -> pd.DataFrame:
    path = os.path.join(SNAPSHOT_DIR, f"{name}.parquet")
    if not os.path.exists(path):
        raise FileNotFoundError(path)
    return pd.read_parquet(path)


df = load_dataset("daily_snapshot1")
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")

# ==================== 1. –ü–û–õ–£–ß–ï–ù–ò–ï –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô ====================

result = model.predict(df)

# –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–µ –¥–ª—è UI
UI_FEATURES = [
    'events_last_7d',
    'days_since_last',
    'purchase_frequency',
    'avg_spend_per_event',
    'conversion_rate_30d',
    'total_purchases'
]

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
out = pd.concat([df, result], axis=1)

print("\n" + "=" * 80)
print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô")
print("=" * 80)
print(f"–ü–æ—Ä–æ–≥ –º–æ–¥–µ–ª–∏ (threshold): {model.optimal_threshold_:.3f}")
print(f"–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å (avg_proba): {out['purchase_proba'].mean():.3f}")
print(f"Will Purchase = 1: {out['will_purchase_pred'].sum()} ({out['will_purchase_pred'].mean():.1%})")

# ==================== 2. –ê–ù–ê–õ–ò–ó –ü–û–†–û–ì–û–í –î–õ–Ø UI ====================

print("\n" + "=" * 80)
print("–ê–ù–ê–õ–ò–ó –ü–û–†–û–ì–û–í–´–• –ó–ù–ê–ß–ï–ù–ò–ô (–î–õ–Ø –§–†–û–ù–¢–ï–ù–î–ê)")
print("=" * 80)

# –°–µ–≥–º–µ–Ω—Ç–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
# High: > 70%, Medium: 30-70%, Low: < 30%
out['segment'] = pd.cut(
    out['purchase_proba'],
    bins=[-0.1, 0.3, 0.7, 1.1],
    labels=['Low (Red)', 'Medium (Blue)', 'High (Green)']
)

print("\n–°—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ–∏—á –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏:")
print("-" * 60)
segment_stats = out.groupby('segment',observed=True)[UI_FEATURES].median()
print(segment_stats)

print("\n" + "=" * 80)
print("–î–ï–¢–ê–õ–¨–ù–´–ï –î–ò–ê–ü–ê–ó–û–ù–´ (–ö–≤–∞–Ω—Ç–∏–ª–∏ 25% - 75%)")
print("–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ —á–∏—Å–ª–∞ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Ü–≤–µ—Ç–æ–≤ –Ω–∞ —Ñ—Ä–æ–Ω—Ç–µ")
print("=" * 80)

for feature in UI_FEATURES:
    print(f"\nüîπ {feature}:")

    # –ë–µ—Ä–µ–º "–ó–µ–ª–µ–Ω—ã–π" —Å–µ–≥–º–µ–Ω—Ç (–≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)
    high_segment = out[out['purchase_proba'] >= 0.6][feature]
    # –ë–µ—Ä–µ–º "–ö—Ä–∞—Å–Ω—ã–π" —Å–µ–≥–º–µ–Ω—Ç (–Ω–∏–∑–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å)
    low_segment = out[out['purchase_proba'] <= 0.1][feature]

    if len(high_segment) > 0:
        q25_high = high_segment.quantile(0.25)
        median_high = high_segment.median()
        print(f"   –î–ª—è –¢–û–ü-–∫–ª–∏–µ–Ω—Ç–æ–≤ –æ–±—ã—á–Ω–æ: > {q25_high:.1f} (–º–µ–¥–∏–∞–Ω–∞ {median_high:.1f})")

    if len(low_segment) > 0:
        q75_low = low_segment.quantile(0.75)
        median_low = low_segment.median()
        print(f"   –î–ª—è –ê–£–¢–°–ê–ô–î–ï–†–û–í –æ–±—ã—á–Ω–æ: < {q75_low:.1f} (–º–µ–¥–∏–∞–Ω–∞ {median_low:.1f})")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –ø–æ—Ä–æ–≥–∞
    if len(high_segment) > 0 and len(low_segment) > 0:
        # –ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–∞–≤–∏–ª–æ: —Å—Ä–µ–¥–Ω–µ–µ –º–µ–∂–¥—É "–ø–ª–æ—Ö–∏–º" –º–∞–∫—Å–∏–º—É–º–æ–º –∏ "—Ö–æ—Ä–æ—à–∏–º" –º–∏–Ω–∏–º—É–º–æ–º
        suggested_threshold = (q75_low + q25_high) / 2
        print(f"   üëâ –†–ï–ö–û–ú–ï–ù–î–£–ï–ú–´–ô –ü–û–†–û–ì (Good): {suggested_threshold:.1f}")

# ==================== 3. SURROGATE TREE (–ü–æ–Ω—è—Ç–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞) ====================

print("\n" + "=" * 80)
print("–ì–ï–ù–ï–†–ê–¶–ò–Ø –ß–ï–õ–û–í–ï–ö–û–ß–ò–¢–ê–ï–ú–´–• –ü–†–ê–í–ò–õ (SURROGATE TREE)")
print("–ú–æ–¥–µ–ª—å —Å–ª–æ–∂–Ω–∞—è, –Ω–æ –º—ã —É–ø—Ä–æ—Å—Ç–∏–º –µ—ë –ª–æ–≥–∏–∫—É –¥–æ 3-—Ö –≥–ª–∞–≤–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π")
print("=" * 80)

# –û–±—É—á–∞–µ–º –º–∞–ª–µ–Ω—å–∫–æ–µ –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π, —á—Ç–æ–±—ã –ø–æ–Ω—è—Ç—å –ª–æ–≥–∏–∫—É "–ë–æ–ª—å—à–æ–≥–æ –±—Ä–∞—Ç–∞"
tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=20)
X_tree = df[UI_FEATURES].fillna(0)
y_tree = out['will_purchase_pred']  # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –º–æ–¥–µ–ª–∏

tree.fit(X_tree, y_tree)

rules = export_text(tree, feature_names=UI_FEATURES)
print(rules)

# –í—ã—Ç–∞—Å–∫–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å —Ñ–∏—á –¥–ª—è —ç—Ç–æ–≥–æ –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–µ—Ä–µ–≤–∞
tree_importance = pd.DataFrame({
    'feature': UI_FEATURES,
    'importance': tree.feature_importances_
}).sort_values('importance', ascending=False)

print("\n–¢–û–ü-3 –§–∏—á–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –¥–µ–ª—è—Ç –ª—é–¥–µ–π –Ω–∞ –î–∞/–ù–µ—Ç (–ø–æ —Å—É—Ä—Ä–æ–≥–∞—Ç—É):")
print(tree_importance.head(3))

# ==================== 4. –ì–û–¢–û–í–´–ô –ö–û–ù–§–ò–ì –î–õ–Ø –§–†–û–ù–¢–ê ====================

print("\n" + "=" * 80)
print("üöÄ –ì–û–¢–û–í–´–ô JSON-–ö–û–ù–§–ò–ì –î–õ–Ø UserSearch.tsx")
print("–°–∫–æ–ø–∏—Ä—É–π—Ç–µ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ñ—É–Ω–∫—Ü–∏—é classifyUserForUI")
print("=" * 80)


# –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è –∫–æ–Ω—Ñ–∏–≥–∞
def get_safe_threshold(series_high, series_low, default):
    if len(series_high) == 0 or len(series_low) == 0:
        return default
    # –ü–æ—Ä–æ–≥ "–•–æ—Ä–æ—à–æ" = –Ω–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–ø-25% –ª—É—á—à–∏—Ö
    good = series_high.quantile(0.25)
    # –ü–æ—Ä–æ–≥ "–ü–ª–æ—Ö–æ" = –≤–µ—Ä—Ö–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ —Ç–æ–ø-75% —Ö—É–¥—à–∏—Ö
    bad = series_low.quantile(0.75)
    return good, bad


high_mask = out['purchase_proba'] >= 0.6
low_mask = out['purchase_proba'] <= 0.1

ev7_good, ev7_bad = get_safe_threshold(out[high_mask]['events_last_7d'], out[low_mask]['events_last_7d'], (10, 0))
recency_good, recency_bad = get_safe_threshold(out[high_mask]['days_since_last'], out[low_mask]['days_since_last'],
                                               (7, 30))
freq_good, freq_bad = get_safe_threshold(out[high_mask]['purchase_frequency'], out[low_mask]['purchase_frequency'],
                                         (0.1, 0.01))
money_good, money_bad = get_safe_threshold(out[high_mask]['avg_spend_per_event'], out[low_mask]['avg_spend_per_event'],
                                           (150, 50))

print("const THRESHOLDS = {")
print(f"  // –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∑–∞ 7 –¥–Ω–µ–π (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)")
print(f"  events7: {{ good: {int(ev7_good)}, bad: {int(ev7_bad)} }},")
print(f"  // –î–Ω–µ–π —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∏–∑–∏—Ç–∞ (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)")
print(f"  recency: {{ good: {int(recency_good)}, bad: {int(recency_bad)} }},")
print(f"  // –ß–∞—Å—Ç–æ—Ç–∞ –ø–æ–∫—É–ø–æ–∫ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)")
print(f"  freq: {{ good: {freq_good:.2f}, bad: {freq_bad:.2f} }},")
print(f"  // –°—Ä–µ–¥–Ω–∏–π —á–µ–∫ –Ω–∞ —Å–æ–±—ã—Ç–∏–µ (—á–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)")
print(f"  avgSpend: {{ good: {int(money_good)}, bad: {int(money_bad)} }}")
print("};")

print("\n" + "=" * 80)
print("SHAP SUMMARY (–¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è)")
print("=" * 80)

# –ë–∞–∑–æ–≤—ã–π SHAP (–∫–∞–∫ —Ä–∞–Ω—å—à–µ) –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
try:
    import lightgbm as lgb

    if hasattr(model, 'clf') and isinstance(model.clf, lgb.LGBMClassifier):
        explainer = shap.TreeExplainer(model.clf)
        # –ë–µ—Ä–µ–º —Å—ç–º–ø–ª –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        sample_df = model._prepare_features_infer(df).iloc[:500]
        shap_values = explainer.shap_values(sample_df)
        if isinstance(shap_values, list): shap_values = shap_values[1]

        shap_sum = pd.DataFrame({
            'feature': sample_df.columns,
            'importance': np.abs(shap_values).mean(axis=0)
        }).sort_values('importance', ascending=False).head(5)
        print("–¢–æ–ø-5 —Ñ–∏—á –ø–æ SHAP (—Ä–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å):")
        print(shap_sum)
except Exception as e:
    print(f"SHAP –ø—Ä–æ–ø—É—â–µ–Ω: {e}")

print("\n–ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–µ–∫—Ü–∏–∏ 4 –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞.")